{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "V2 Version of Research Machine\n",
    "0. Starts with the key question that needs to be answered\n",
    "- In memory, 1) context from one level up is provided\n",
    "\n",
    "1. Searchers (\n",
    "    INPUT: [key question, context, verifier feedback, running list of clues & relevant info]\n",
    "    PROCESS: \n",
    "    OUTPUT: [key question, context, verifier feedback, updated running list of clues & relevant info, best answer, history of actions]\n",
    ") -- they figure out if they need to 1.1) search more or 1.2) decompose to smaller key questions that need to be answered\n",
    "\n",
    "1.1. Seekers\n",
    "2.1) Key question decompositions are created and sent to verifiers to see if there's a reasonable answer\n",
    "2.2) Searchers look to find answer online -- need to figure out how to do this well. And when to stop searching.\n",
    "- In memory, 1) context from directly one level up is provided, 2) biggest clues and top answers are tracked, and 3) feedback from verifiers\n",
    "\n",
    "2. Verifiers (\n",
    "    INPUT: [key question, context, verifier feedback, updated running list of clues & relevant info, best answer, updated history]\n",
    "    PROCESS: \n",
    "        1. Evaluate best answer, updated running list of clues & relevant info to see if there's a reasonable answer to the key question\n",
    "        2. If yes, return answer to previous verifier with this reasonable answer as the answer (learned tactic)\n",
    "        3. If no, give feedback as to what's wrong to analyzers\n",
    "    OUTPUT: [updated key question, updated context, updated verifier feedback, updated running list of clues & relevant info, updated history]\n",
    ") -- they check based on the top answers whether there is a reasonable answer to the question yet. If yes, then return the answer to the previous verifier so they can evaluate if they have a reasonable answer with this new information.\n",
    "- Memory = full history of decomposition. They are responsible for backtracking and posing the key question for analyzers\n",
    "- Checks for logical validity\n",
    "- Checks for reasonable answer or not\n",
    "- Give feedback to Analyzers\n",
    "\n",
    "3. Analyzers (\n",
    "    PROCESS: Given feedback and history, figure out what key question we need to answer. \n",
    ")\n",
    "\n",
    "Risks:\n",
    "1. Analyzers probably need more frameworks to backtrack on their approach, because decomposition might not work\n",
    "2. This process is not MECE, I need to spend more time thinking about the right approach later.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT template\n",
    "prompt = f'''\n",
    "\n",
    "'''\n",
    "res = chat_openai(prompt, model=\"gpt-3.5-turbo\")[0]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming up with context and key question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"I'm supporting a biocement research project. I have to run a carbon neutrality analysis, so I'd like to know what efficiencies we need to achieve with ECR enzymes for the experimental process to be carbon negative, neutral, or positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key question\n",
    "search_query = \"How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogpt.commands.web_selenium import browse_website, scrape_text_with_selenium_no_agent\n",
    "import json\n",
    "from util import sanitize_filename\n",
    "import os\n",
    "from prompts import get_predicted_usefulness_of_text_prompt\n",
    "from collections import defaultdict\n",
    "from llm import chat_openai\n",
    "from autogpt.commands.web_search import web_search_ddg\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_file_safe = sanitize_filename(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How_efficiently_do_the_ECR_enzymes_work_in_Kitsatospor_setae_bacteria_'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_query_file_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = \"academic\"\n",
    "# search_engine = \"general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f'autoscious_logs/{search_query_file_safe}'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Implementing a \"write down the context, key question, deliverable, and any clarifying questions before you start searching for the answer to the question\".\n",
    "# prompt = f'''\n",
    "\n",
    "# '''\n",
    "# res = chat_openai(prompt, model=\"gpt-3.5-turbo\")[0]\n",
    "# print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implementing a verifier that responds to the clarifying questions as best as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifier check (personal check for now):\n",
    "- The key question seems pretty clear to me. I added answers to FAQs / clarifying questions, but I'd like to be able to support this later. Clarifying questions is key in this step to get feedback from the human. If not, then I suppose the verifierLM can take their best guess at what the deliverable is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Skip] Getting key questions and the decomposition and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key questions decomposition list:  [{'project_question': 'How efficiently do the ECR enzymes work, especially in Kitsatospor setae bacteria?', 'project_objective': 'To determine the efficiency of ECR enzymes in Kitsatospor setae bacteria', 'key_drivers': {'1': {'driver': 'ECR enzyme activity', 'hypotheses': {'1': {'hypothesis': 'ECR enzyme activity is high in Kitsatospor setae bacteria', 'key_questions': {'1': 'What is the level of ECR enzyme activity in Kitsatospor setae bacteria?'}}}}}}, {'project_question': 'How efficiently do the ECR enzymes work, especially in Kitsatospor setae bacteria?', 'project_objective': 'To determine the efficiency of ECR enzymes in Kitsatospor setae bacteria', 'key_drivers': {'1': {'driver': 'ECR enzyme activity', 'hypotheses': {'1': {'hypothesis': 'ECR enzyme activity is high in Kitsatospor setae bacteria', 'key_questions': {'2': 'How does ECR enzyme activity compare to other bacteria?'}}}}}}, {'project_question': 'How efficiently do the ECR enzymes work, especially in Kitsatospor setae bacteria?', 'project_objective': 'To determine the efficiency of ECR enzymes in Kitsatospor setae bacteria', 'key_drivers': {'1': {'driver': 'ECR enzyme activity', 'hypotheses': {'1': {'hypothesis': 'ECR enzyme activity is high in Kitsatospor setae bacteria', 'key_questions': {'3': 'What factors influence ECR enzyme activity in Kitsatospor setae bacteria?'}}}}}}, {'project_question': 'How efficiently do the ECR enzymes work, especially in Kitsatospor setae bacteria?', 'project_objective': 'To determine the efficiency of ECR enzymes in Kitsatospor setae bacteria', 'key_drivers': {'1': {'driver': 'ECR enzyme activity', 'hypotheses': {'1': {'hypothesis': 'ECR enzyme activity is high in Kitsatospor setae bacteria', 'key_questions': {'4': 'What are the potential applications or implications of high ECR enzyme activity in Kitsatospor setae bacteria?'}}}}}}]\n"
     ]
    }
   ],
   "source": [
    "# # Create a decomposition for each key question only\n",
    "# context = \"Enoyl-CoA carboxylase/reductase enzymes (ECRs)\"\n",
    "# key_question_decomposition_list = []\n",
    "# for driver_key, driver_value in decomposition['key_drivers'].items():\n",
    "#     for hypothesis_key, hypothesis_value in driver_value['hypotheses'].items():\n",
    "#         for question_key, question_value in hypothesis_value['key_questions'].items():\n",
    "#             new_decomposition = decomposition.copy()\n",
    "#             new_decomposition['key_drivers'] = {\n",
    "#                 driver_key: {\n",
    "#                     'driver': driver_value['driver'],\n",
    "#                     'hypotheses': {\n",
    "#                         hypothesis_key: {\n",
    "#                             'hypothesis': hypothesis_value['hypothesis'],\n",
    "#                             'key_questions': {\n",
    "#                                 question_key: question_value\n",
    "#                             }\n",
    "#                         }\n",
    "#                     }\n",
    "#                 }\n",
    "#             }\n",
    "#             key_question_decomposition_list.append(new_decomposition)\n",
    "# print(\"Key questions decomposition list: \", key_question_decomposition_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming up with many good search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f'autoscious_logs/{search_query_file_safe}/sources'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_search_queries_prompt(key_question, search_engine):\n",
    "  return f'''\n",
    "Key question:\n",
    "{key_question}\n",
    "\n",
    "Task:\n",
    "For the key question, write a clear and comprehensive but short (1 query) list of search queries optimized for best search engine results, so that you can confidently and quickly surface the most relevant information to determine the best answer to the question. Extract a string of search keywords query from the key question.\n",
    "\n",
    "The output should be in JSON format: \n",
    "```json\n",
    "{{\n",
    "  \"1\": \"<insert query>\",\n",
    "  \"keywords_query\": \"<insert keywords>\"\n",
    "}}\n",
    "\n",
    "Respond only with the output, with no explanation or conversation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key question:\n",
      "How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\n",
      "\n",
      "Task:\n",
      "For the key question, write a clear and comprehensive but short (1 query) list of search queries optimized for best search engine results, so that you can confidently and quickly surface the most relevant information to determine the best answer to the question. Extract a string of search keywords query from the key question.\n",
      "\n",
      "The output should be in JSON format: \n",
      "```json\n",
      "{\n",
      "  \"1\": \"<insert query>\",\n",
      "  \"keywords_query\": \"<insert keywords>\"\n",
      "}\n",
      "\n",
      "Respond only with the output, with no explanation or conversation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_initial_search_queries_prompt(search_query, search_engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  \n",
      "Key question:\n",
      "How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\n",
      "\n",
      "Task:\n",
      "For the key question, write a clear and comprehensive but short (1 query) list of search queries optimized for best search engine results, so that you can confidently and quickly surface the most relevant information to determine the best answer to the question. Extract a string of search keywords query from the key question.\n",
      "\n",
      "The output should be in JSON format: \n",
      "```json\n",
      "{\n",
      "  \"1\": \"<insert query>\",\n",
      "  \"keywords_query\": \"<insert keywords>\"\n",
      "}\n",
      "\n",
      "Respond only with the output, with no explanation or conversation.\n",
      "\n",
      "Completion info:  {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"{\\n  \\\"1\\\": \\\"Efficiency of ECR enzymes in Kitsatospor setae bacteria\\\",\\n  \\\"keywords_query\\\": \\\"ECR enzymes efficiency Kitsatospor setae bacteria\\\"\\n}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1691419510,\n",
      "  \"id\": \"chatcmpl-7kvlm90gatYAgyRKogdRmh1r8An8N\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 38,\n",
      "    \"prompt_tokens\": 168,\n",
      "    \"total_tokens\": 206\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "context = \"Enoyl-CoA carboxylase/reductase enzymes (ECRs)\"\n",
    "\n",
    "# for decomposition_idx, key_question_decomposition in enumerate(key_question_decomposition_list):\n",
    "key_question_initial_search_queries = json.loads(chat_openai(get_initial_search_queries_prompt(search_query, search_engine), model=\"gpt-3.5-turbo\")[0])\n",
    "\n",
    "keywords_query = key_question_initial_search_queries.pop('keywords_query')\n",
    "\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'w') as f:\n",
    "    json.dump(key_question_initial_search_queries, f, indent=2)\n",
    "\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/keywords_query.txt', 'w') as f:\n",
    "    json.dump(keywords_query, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Debug testing) Google Scholar search given search keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from scholarly import scholarly\n",
    "from scholarly import ProxyGenerator\n",
    "\n",
    "# Set up a ProxyGenerator object to use free proxies\n",
    "# This needs to be done only once per session\n",
    "pg = ProxyGenerator()\n",
    "pg.FreeProxies()\n",
    "scholarly.use_proxy(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "scholar_res_gen = scholarly.search_pubs('Enoyl-CoA carboxylase/reductase (ECR) enzyme activity Kitasatospora setae bacteria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scholarly.publication_parser._SearchScholarIterator at 0x1f0d8958160>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scholar_res_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_res = next(scholar_res_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/126447/1/ETH23842.pdf\n"
     ]
    }
   ],
   "source": [
    "print(first_res['eprint_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader \n",
    "from io import BytesIO\n",
    "\n",
    "def try_getting_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    f = BytesIO(response.content)\n",
    "    try:\n",
    "        pdf = PdfReader(f)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Get the PDF content\n",
    "def try_getting_pdf_content():\n",
    "    response = requests.get('https://pubs.acs.org/doi/pdf/10.1021/acs.chemrev.2c00581')\n",
    "    f = BytesIO(response.content)\n",
    "    try:\n",
    "        pdf = PdfReader(f)\n",
    "        content = \"\"\n",
    "\n",
    "        for i in range(len(pdf.pages)):\n",
    "            page = pdf.pages[i]\n",
    "            text = page.extract_text()\n",
    "            content += text\n",
    "        return content\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "MaxTriesExceededException",
     "evalue": "Cannot Fetch from Google Scholar.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMaxTriesExceededException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scholar_res_gen \u001b[39m=\u001b[39m scholarly\u001b[39m.\u001b[39;49msearch_pubs(\u001b[39m'\u001b[39;49m\u001b[39mEnoyl-CoA carboxylase/reductase (ECR) enzyme activity Kitasatospora setae bacteria\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\venv\\lib\\site-packages\\scholarly\\_scholarly.py:160\u001b[0m, in \u001b[0;36m_Scholarly.search_pubs\u001b[1;34m(self, query, patents, citations, year_low, year_high, sort_by, include_last_year, start_index)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Searches by query and returns a generator of Publication objects\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[39m:param query: terms to be searched\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_url(_PUBSEARCH\u001b[39m.\u001b[39mformat(requests\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mquote(query)), patents\u001b[39m=\u001b[39mpatents,\n\u001b[0;32m    158\u001b[0m                           citations\u001b[39m=\u001b[39mcitations, year_low\u001b[39m=\u001b[39myear_low, year_high\u001b[39m=\u001b[39myear_high,\n\u001b[0;32m    159\u001b[0m                           sort_by\u001b[39m=\u001b[39msort_by, include_last_year\u001b[39m=\u001b[39minclude_last_year, start_index\u001b[39m=\u001b[39mstart_index)\n\u001b[1;32m--> 160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__nav\u001b[39m.\u001b[39;49msearch_publications(url)\n",
      "File \u001b[1;32mc:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\venv\\lib\\site-packages\\scholarly\\_navigator.py:296\u001b[0m, in \u001b[0;36mNavigator.search_publications\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch_publications\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _SearchScholarIterator:\n\u001b[0;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a Publication Generator given a url\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \n\u001b[0;32m    291\u001b[0m \u001b[39m    :param url: the url where publications can be found.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[39m    :rtype: {_SearchScholarIterator}\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m     \u001b[39mreturn\u001b[39;00m _SearchScholarIterator(\u001b[39mself\u001b[39;49m, url)\n",
      "File \u001b[1;32mc:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\venv\\lib\\site-packages\\scholarly\\publication_parser.py:53\u001b[0m, in \u001b[0;36m_SearchScholarIterator.__init__\u001b[1;34m(self, nav, url)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pubtype \u001b[39m=\u001b[39m PublicationSource\u001b[39m.\u001b[39mPUBLICATION_SEARCH_SNIPPET \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m/scholar?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m url \u001b[39melse\u001b[39;00m PublicationSource\u001b[39m.\u001b[39mJOURNAL_CITATION_LIST\n\u001b[0;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nav \u001b[39m=\u001b[39m nav\n\u001b[1;32m---> 53\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_url(url)\n\u001b[0;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_total_results()\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_parser \u001b[39m=\u001b[39m PublicationParser(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nav)\n",
      "File \u001b[1;32mc:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\venv\\lib\\site-packages\\scholarly\\publication_parser.py:59\u001b[0m, in \u001b[0;36m_SearchScholarIterator._load_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_url\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m):\n\u001b[0;32m     58\u001b[0m     \u001b[39m# this is temporary until setup json file\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_soup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nav\u001b[39m.\u001b[39;49m_get_soup(url)\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgs_r gs_or gs_scl\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgsc_mpat_ttl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\venv\\lib\\site-packages\\scholarly\\_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_soup\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BeautifulSoup:\n\u001b[0;32m    238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     html \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_page(\u001b[39m'\u001b[39;49m\u001b[39mhttps://scholar.google.com\u001b[39;49m\u001b[39m{0}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(url))\n\u001b[0;32m    240\u001b[0m     html \u001b[39m=\u001b[39m html\u001b[39m.\u001b[39mreplace(\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\xa0\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    241\u001b[0m     res \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\venv\\lib\\site-packages\\scholarly\\_navigator.py:190\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[1;34m(self, pagerequest, premium)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_page(pagerequest, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mraise\u001b[39;00m MaxTriesExceededException(\u001b[39m\"\u001b[39m\u001b[39mCannot Fetch from Google Scholar.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mMaxTriesExceededException\u001b[0m: Cannot Fetch from Google Scholar."
     ]
    }
   ],
   "source": [
    "scholar_res_gen = scholarly.search_pubs('Enoyl-CoA carboxylase/reductase (ECR) enzyme activity Kitasatospora setae bacteria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_res = []\n",
    "for res in scholar_res_gen:\n",
    "    item = {}\n",
    "    item['title'] = res['bib']['title']\n",
    "    if try_getting_pdf(res['eprint_url']):\n",
    "        item['href'] = res['eprint_url']\n",
    "    else:\n",
    "        item['href'] = res['pub_url']\n",
    "    item['body'] = res['bib']['abstract']\n",
    "    web_search_res += [item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(web_search_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogpt.commands.web_selenium import scrape_text_with_selenium_no_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through url:  https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.2c00581\n",
      "select firefox options!\n",
      "Driver is getting url\n",
      "set timeout!\n",
      "Page loaded within 15 seconds\n",
      "Driver got url\n",
      "Driver has found page source\n",
      "Handing off to Beautiful Soup!\n",
      "done extractin\n",
      "Text:  Download Hi-Res ImageDownload to MS-PowerPointCite This:Chem. Rev. 2023, 123, 9, 5702-5754\n",
      "ADVERTISEMENT\n",
      "RETURN TO ISSUEPREVReviewNEXTEnzymatic Conversion of CO2: From Natural to Artificial UtilizationSarah BierbaumerSarah BierbaumerInstitute of Chemistry, University of Graz, NAWI Graz, Heinrichstraße 28, 8010 Graz, AustriaMore by Sarah BierbaumerView Biographyhttps://orcid.org/0000-0003-3883-7108, Maren NattermannMaren NattermannDepartment of Biochemistry and Synthetic Metabolism, Max Planck In\n"
     ]
    }
   ],
   "source": [
    "text = scrape_text_with_selenium_no_agent('https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.2c00581', None, search_engine='firefox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web search given search keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from scholarly import scholarly\n",
    "from scholarly import ProxyGenerator\n",
    "\n",
    "# Set up a ProxyGenerator object to use free proxies\n",
    "# This needs to be done only once per session\n",
    "pg = ProxyGenerator()\n",
    "pg.FreeProxies()\n",
    "scholarly.use_proxy(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# def download_pdf(url, target_path):\n",
    "#     response = requests.get(url)\n",
    "    \n",
    "#     # Ensure we got a valid response\n",
    "#     if response.status_code == 200:\n",
    "#         with open(target_path, 'wb') as f:\n",
    "#             f.write(response.content)\n",
    "#     else:\n",
    "#         print(f\"Unable to get URL: {url}\")\n",
    "#         print(f\"Response Code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_pdf(\"https://www.africau.edu/images/default/sample.pdf\", r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\question_answering\\test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to support pdfs! https://towardsdatascience.com/how-to-extract-text-from-any-pdf-and-image-for-large-language-model-2d17f02875e6\n",
    "# # Looks like with requests, we can't always download pdfs unfortunately, they'll probably need to be added as sources to check through by users manually\n",
    "from pytesseract import image_to_string\n",
    "import pypdfium2 as pdfium\n",
    "\n",
    "def convert_pdf_to_images(file_path, scale=300/72):\n",
    "\n",
    "    pdf_file = pdfium.PdfDocument(file_path)\n",
    "\n",
    "    page_indices = [i for i in range(len(pdf_file))]\n",
    "\n",
    "    renderer = pdf_file.render(\n",
    "        pdfium.PdfBitmap.to_pil,\n",
    "        page_indices=page_indices,\n",
    "        scale=scale,\n",
    "    )\n",
    "\n",
    "    final_images = []\n",
    "\n",
    "    for i, image in zip(page_indices, renderer):\n",
    "\n",
    "        image_byte_array = BytesIO()\n",
    "        image.save(image_byte_array, format='jpeg', optimize=True)\n",
    "        image_byte_array = image_byte_array.getvalue()\n",
    "        final_images.append(dict({i: image_byte_array}))\n",
    "\n",
    "    return final_images\n",
    "\n",
    "# 2. Extract text from images via pytesseract\n",
    "\n",
    "\n",
    "def extract_text_from_img(list_dict_final_images):\n",
    "\n",
    "    image_list = [list(data.values())[0] for data in list_dict_final_images]\n",
    "    image_content = []\n",
    "\n",
    "    for index, image_bytes in enumerate(image_list):\n",
    "\n",
    "        image = Image.open(BytesIO(image_bytes))\n",
    "        raw_text = str(image_to_string(image))\n",
    "        image_content.append(raw_text)\n",
    "\n",
    "    return \"\\n\".join(image_content)\n",
    "\n",
    "\n",
    "def extract_content_from_url(url: str):\n",
    "    images_list = convert_pdf_to_images(url)\n",
    "    text_with_pytesseract = extract_text_from_img(images_list)\n",
    "\n",
    "    return text_with_pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = extract_content_from_url(\"https://phys.org/news/2022-04-soil-microbe-rev-artificial-photosynthesis.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(try_getting_pdf_content(\"https://phys.org/news/2022-04-soil-microbe-rev-artificial-photosynthesis.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from PyPDF2 import PdfReader \n",
    "# from io import BytesIO\n",
    "\n",
    "# def is_pdf_encrypted(url):\n",
    "#     response = requests.get(url)\n",
    "#     f = BytesIO(response.content)\n",
    "\n",
    "#     try:\n",
    "#         pdf = PdfReader(f)\n",
    "#         return pdf.isEncrypted\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred: {e}\")\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: EOF marker not found\n",
      "The PDF is not encrypted.\n"
     ]
    }
   ],
   "source": [
    "# url = \"https://phys.org/news/2022-04-soil-microbe-rev-artificial-photosynthesis.pdf\"\n",
    "# if is_pdf_encrypted(url):\n",
    "#     print(\"The PDF is encrypted.\")\n",
    "# else:\n",
    "#     print(\"The PDF is not encrypted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader \n",
    "from io import BytesIO\n",
    "\n",
    "def try_getting_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    f = BytesIO(response.content)\n",
    "    try:\n",
    "        pdf = PdfReader(f)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Could not get pdf\")\n",
    "        return False\n",
    "\n",
    "# Get the PDF content\n",
    "def try_getting_pdf_content(url):\n",
    "    response = requests.get(url)\n",
    "    f = BytesIO(response.content)\n",
    "    try:\n",
    "        pdf = PdfReader(f)\n",
    "        content = \"\"\n",
    "\n",
    "        for i in range(len(pdf.pages)):\n",
    "            page = pdf.pages[i]\n",
    "            text = page.extract_text()\n",
    "            content += text\n",
    "        return content\n",
    "    except:\n",
    "        print(\"Error getting PDF content\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def google_search_raw(search_term, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=os.getenv('DEV_KEY'))\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "\n",
    "    search_results = res.get(\"items\", [])\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Create a list of only the URLs from the search results\n",
    "    search_results_links = [item[\"link\"] for item in search_results]\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google(search_query):\n",
    "    num_google_searches = 8\n",
    "    results = google_search_raw(search_query, os.getenv('MY_CSE_ID'), num=num_google_searches, lr=\"lang_en\", cr=\"countryUS\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  Efficiency of ECR enzymes in Kitsatospor setae bacteria\n",
      "trying academic search\n",
      "trying normal search\n"
     ]
    }
   ],
   "source": [
    "MAX_RETRIES = 3\n",
    "\n",
    "# for decomposition_idx, key_question_decomposition in enumerate(key_question_decomposition_list):\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'r') as f:\n",
    "    key_question_initial_search_queries = json.load(f)\n",
    "\n",
    "for idx, query in key_question_initial_search_queries.items():\n",
    "    print(\"query: \", query)\n",
    "    # query = \"ECR enzyme efficiency in k setae\" # Hard coded to get the results I want\n",
    "\n",
    "    web_search_res = []\n",
    "    if search_engine == \"academic\":\n",
    "        print(\"trying academic search\")\n",
    "        try:\n",
    "            scholar_res_gen = scholarly.search_pubs(query)\n",
    "\n",
    "            for res in scholar_res_gen:\n",
    "                item = {}\n",
    "                item['title'] = res['bib']['title']\n",
    "                if try_getting_pdf(res['eprint_url']):\n",
    "                    item['href'] = res['eprint_url']\n",
    "                    item['pdf'] = True\n",
    "                else:\n",
    "                    item['href'] = res['pub_url']\n",
    "                    item['pdf'] = False\n",
    "                item['body'] = res['bib']['abstract']\n",
    "                web_search_res += [item]\n",
    "        except: \n",
    "            print(\"Exception, trying normal search\")\n",
    "    if web_search_res == []:\n",
    "        # DDG\n",
    "        print(\"trying normal search\")\n",
    "        web_search_res = json.loads(web_search_ddg(query))\n",
    "        if len(web_search_res) == 0:\n",
    "            print(\"trying google search!\")\n",
    "            # Google\n",
    "            web_search_res_raw = search_google(query) # google uses 'link' instead of 'href'\n",
    "            web_search_res = [{\n",
    "                'title': web_search_res_raw[i]['title'], \n",
    "                'href': web_search_res_raw[i]['link'], \n",
    "                'body': web_search_res_raw[i]['snippet'],\n",
    "                'pdf': False\n",
    "                } for i in range(len(web_search_res_raw))\n",
    "            ]\n",
    "\n",
    "    # save web search results\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_results_query_{idx}.json', 'w') as f:\n",
    "        json.dump(web_search_res, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading type 1: filtering unlikely relevant sources based on title and body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtering_web_results_ratings(key_question, web_search_res):\n",
    "    return f'''\n",
    "Key question:\n",
    "{key_question}\n",
    "\n",
    "Task:\n",
    "Based on the key question and each search result's title and body content, reason and assign a predicted usefulness score of the search result's content and potential useful references to answering the key question using a 5-point Likert scale, with 1 being very not useful, 2 being not useful, 3 being somewhat useful, 4 being useful, 5 being very useful.\n",
    "\n",
    "Search results:\n",
    "{web_search_res}\n",
    "\n",
    "The output should be in JSON format: \n",
    "```json\n",
    "{{\n",
    "  'href': 'relevance score',\n",
    "  etc.\n",
    "}}\n",
    "```\n",
    "\n",
    "Respond only with the output, with no explanation or conversation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  \n",
      "Key question:\n",
      "How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\n",
      "\n",
      "Task:\n",
      "Based on the key question and each search result's title and body content, reason and assign a predicted usefulness score of the search result's content and potential useful references to answering the key question using a 5-point Likert scale, with 1 being very not useful, 2 being not useful, 3 being somewhat useful, 4 being useful, 5 being very useful.\n",
      "\n",
      "Search results:\n",
      "[{'title': 'Awakening the Sleeping Carboxylase Function of Enzymes: Engineering the ...', 'href': 'https://pubs.acs.org/doi/10.1021/jacs.9b03431', 'body': 'However, for enoyl-CoA carboxylase/reductase from Kitasatospora setae (ECR Ks), four conserved amino acids that form a CO 2-binding pocket at the active site were described recently (Figure 1a). These four amino acids anchor and position the CO 2 molecule during catalysis, in which a reactive enolate is formed that attacks the CO 2. Figure 1'}, {'title': 'Four amino acids define the CO2 binding pocket of enoyl-CoA ...', 'href': 'https://pubmed.ncbi.nlm.nih.gov/31243147/', 'body': 'One of the most efficient classes of carboxylating enzymes are enoyl-CoA carboxylases/reductases (Ecrs), which outcompete the plant enzyme RuBisCO in catalytic efficiency and fidelity by more than an order of magnitude.'}, {'title': 'How a soil microbe could rev up artificial photosynthesis - Phys.org', 'href': 'https://phys.org/news/2022-04-soil-microbe-rev-artificial-photosynthesis.html', 'body': 'This depiction of ECR, an enzyme found in soil bacteria, shows each of its four identical molecules in a different color. These molecules work together in pairs—blue with white and green with ...'}, {'title': 'Reaction scheme and structural organization of the K. setae ECR ...', 'href': 'https://www.researchgate.net/figure/Reaction-scheme-and-structural-organization-of-the-K-setae-ECR-complex-a_fig1_360182999', 'body': 'The tetrameric oligomer state of the ECR apo enzyme is further supported by size-exclusion chromatography, which showed that the 51.2 kDa protein eluted as a single peak at 205 kDa ( Figure S3 ...'}, {'title': 'Key extracellular enzymes triggered high-efficiency composting ... - PubMed', 'href': 'https://pubmed.ncbi.nlm.nih.gov/31176934/', 'body': 'A consortium of key bacterial taxa plays critical roles in the composting process. In order to elucidate the identity and mechanisms by which specific bacterial species drive high-efficiency composting, the succession of key bacterial consortia and extracellular enzymes produced during the composting process were monitored in composting piles with varying initial C/N ratios.'}, {'title': 'Bacterial Enzymes and Antibiotic Resistance - PMC', 'href': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351036/', 'body': 'Classification of these enzymes is based on their participation in various biochemical mechanisms: modification of the enzymes that act as antibiotic targets, enzymatic modification of intracellular targets, enzymatic transformation of antibiotics, and the implementation of cellular metabolism reactions.'}, {'title': 'Comparison of bacteria disintegration methods and their ... - Nature', 'href': 'https://www.nature.com/articles/s41598-021-99873-x', 'body': 'To compare the efficiency of different cell disintegration methods, sonication, sand mill, and tissue lyser were used. For bacterial extract metabolite analysis, 1H NMR together with univariate ...'}, {'title': 'Engineering new catalytic activities in enzymes - Nature', 'href': 'https://www.nature.com/articles/s41929-019-0385-5', 'body': 'Abstract. The efficiency, selectivity and sustainability benefits offered by enzymes are enticing chemists to consider biocatalytic transformations to complement or even supplant more traditional ...'}]\n",
      "\n",
      "The output should be in JSON format: \n",
      "```json\n",
      "{\n",
      "  'href': 'relevance score',\n",
      "  etc.\n",
      "}\n",
      "```\n",
      "\n",
      "Respond only with the output, with no explanation or conversation.\n",
      "\n",
      "Completion info:  {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"{\\n  \\\"https://pubs.acs.org/doi/10.1021/jacs.9b03431\\\": 4,\\n  \\\"https://pubmed.ncbi.nlm.nih.gov/31243147/\\\": 5,\\n  \\\"https://phys.org/news/2022-04-soil-microbe-rev-artificial-photosynthesis.html\\\": 3,\\n  \\\"https://www.researchgate.net/figure/Reaction-scheme-and-structural-organization-of-the-K-setae-ECR-complex-a_fig1_360182999\\\": 4,\\n  \\\"https://pubmed.ncbi.nlm.nih.gov/31176934/\\\": 2,\\n  \\\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351036/\\\": 1,\\n  \\\"https://www.nature.com/articles/s41598-021-99873-x\\\": 1,\\n  \\\"https://www.nature.com/articles/s41929-019-0385-5\\\": 3\\n}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1691418792,\n",
      "  \"id\": \"chatcmpl-7kvaCN05rlwD9LxGcGn3KKDzRJi8w\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 204,\n",
      "    \"prompt_tokens\": 981,\n",
      "    \"total_tokens\": 1185\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'r') as f:\n",
    "    key_question_initial_search_queries = json.load(f)\n",
    "\n",
    "for query_idx, query in key_question_initial_search_queries.items():\n",
    "    # load web search results\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_results_query_{query_idx}.json', 'r') as f:\n",
    "        web_search_res = json.loads(f.read())\n",
    "    \n",
    "    filtered_web_results = {}\n",
    "    if web_search_res != []:\n",
    "        # filter web results based on title and body\n",
    "        filtered_web_results = json.loads(chat_openai(get_filtering_web_results_ratings(search_query, web_search_res), model=\"gpt-3.5-turbo\")[0])\n",
    "\n",
    "    ratings_url_dict = defaultdict(list)\n",
    "    for url, rating in filtered_web_results.items():\n",
    "        ratings_url_dict[str(rating)].append(url)\n",
    "\n",
    "    # save filtered search results\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/rated_web_results_query_{int(query_idx)}.json', 'w') as f:\n",
    "        json.dump(ratings_url_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading type 2 & 3: filtering based on skimming and sampling from each source, and only saving most relevant sources for fact extraction and quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skimming and rating is irrelevant for now. Only useful for prioritizing which articles to read given limited tokens. Still need to run to get the full text though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE code for predicting usefulness of very relevant (5) and relevant (4) results.\n",
    "from autogpt.commands.web_selenium import scrape_text_with_selenium_no_agent\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "SAMPLING_FACTOR = 0.1 # Also cap it so it falls under the max token limit\n",
    "MAX_TOKENS = 2500 * 4 # 1 token = 4 chars, 2500 + 500 (prompt) tokens is high for GPT3.5\n",
    "MAX_CHUNKS = int(MAX_TOKENS / CHUNK_SIZE)\n",
    "# context = \"Enoyl-CoA carboxylase/reductase enzymes (ECRs)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_chunks(text, CHUNK_SIZE, num_chunk_samples):\n",
    "    step_size = len(text) // num_chunk_samples\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), step_size):\n",
    "        chunk = text[i:i+CHUNK_SIZE]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Break after getting the required number of chunks\n",
    "        if len(chunks) >= num_chunk_samples:\n",
    "            break\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to determine how useful the text is likely to be for answering the key questions\n",
    "def get_predicted_usefulness_of_text_prompt(key_question, sample_text_chunks):\n",
    "    return f'''\n",
    "Key question:\n",
    "{key_question}\n",
    "\n",
    "Task: \n",
    "Based on the key question and the sample text chunks of the source text, the goal is to identify how useful reading the full source text would be to extract direct quoted facts or references to determine the best answer to the key question. \n",
    "\n",
    "Deliverable:\n",
    "Assign a predicted usefulness score of the full source text using a 5-point Likert scale, with 1 being very unlikely to be usefulness, 2 being unlikely to be useful, 3 being somewhat likely to be useful, 4 being likely to be useful, and 5 being very likely useful and containing facts or references that answer the key question.\n",
    "\n",
    "Sample text chunks from the source text:\n",
    "{sample_text_chunks}\n",
    "\n",
    "The output should be of the following JSON format\n",
    "{{\n",
    "    \"\"predicted_usefulness: <insert predicted usefulness rating>,\n",
    "   etc.\n",
    "}}\n",
    "\n",
    "\n",
    "Respond only with the output, with no explanation or conversation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Function to return sample chunks based on relevance to key_question using cosine similarity\n",
    "# TODO: have chatgpt come up w what the form of the answeer might look like and match on that!\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # or 'all-mpnet-base-v2'\n",
    "\n",
    "def get_most_relevant_chunks(key_question, text, CHUNK_SIZE, num_chunk_samples):\n",
    "    # 1. Split text into chunks\n",
    "    chunks = [text[i:i+CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n",
    "\n",
    "    # 2. For each chunk, compute the cosine similarity with key_question\n",
    "    key_embedding = model.encode([key_question])\n",
    "    chunk_embeddings = model.encode(chunks)\n",
    "    \n",
    "    similarities = [cosine_similarity(key_embedding.reshape(1, -1), chunk_embedding.reshape(1, -1))[0][0] for chunk_embedding in chunk_embeddings]\n",
    "    \n",
    "    # 3. Sort chunks by similarity\n",
    "    sorted_chunks = [chunk for _, chunk in sorted(zip(similarities, chunks), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    # 4. Return top num_chunk_samples chunks\n",
    "    return sorted_chunks[:num_chunk_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "def get_most_relevant_chunks_with_bm25(key_question, text, CHUNK_SIZE, num_chunk_samples):\n",
    "    # 1. Split text into chunks\n",
    "    chunks = [text[i:i+CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n",
    "\n",
    "    # 2. Tokenize the chunks\n",
    "    tokenized_chunks = [re.findall(r\"\\w+\", chunk) for chunk in chunks]\n",
    "\n",
    "    # 3. Initialize BM25\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "    # 4. Query BM25 with the key question\n",
    "    tokenized_question = re.findall(r\"\\w+\", key_question)\n",
    "    scores = bm25.get_scores(tokenized_question)\n",
    "\n",
    "    # 5. Sort chunks by BM25 scores\n",
    "    sorted_chunks = [chunk for _, chunk in sorted(zip(scores, chunks), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    # 6. Return top num_chunk_samples chunks\n",
    "    return sorted_chunks[:num_chunk_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sample_chunks(key_question, text, CHUNK_SIZE, num_chunk_samples):\n",
    "#     step_size = len(text) // num_chunk_samples\n",
    "\n",
    "#     chunks = []\n",
    "#     for i in range(0, len(text), step_size):\n",
    "#         chunk = text[i:i+CHUNK_SIZE]\n",
    "#         chunks.append(chunk)\n",
    "\n",
    "#         # Break after getting the required number of chunks\n",
    "#         if len(chunks) >= num_chunk_samples:\n",
    "#             break\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "def find_title(url, web_search_info):\n",
    "    for item in web_search_info:\n",
    "        if item[\"href\"] == url:\n",
    "            return item[\"title\"]\n",
    "    return None\n",
    "\n",
    "def check_pdf(url, web_search_info):\n",
    "    for item in web_search_info:\n",
    "        if \"pdf\" in item.keys() and item[\"pdf\"]:\n",
    "            return True\n",
    "    print(\"Not pdf\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  1 rating_source_idx 0 Skimming url: https://pubs.acs.org/doi/10.1021/jacs.9b03431\n",
      "URL or text already visited!\n",
      "query  1 rating_source_idx 1 Skimming url: https://www.researchgate.net/figure/Reaction-scheme-and-structural-organization-of-the-K-setae-ECR-complex-a_fig1_360182999\n",
      "Could not get pdf\n",
      "Going through url:  https://www.researchgate.net/figure/Reaction-scheme-and-structural-organization-of-the-K-setae-ECR-complex-a_fig1_360182999\n",
      "select firefox options!\n",
      "Driver is getting url\n",
      "set timeout!\n",
      "Page loaded within 15 seconds\n",
      "Driver got url\n",
      "Driver has found page source\n",
      "Handing off to Beautiful Soup!\n",
      "done extractin\n",
      "Text:  Figure 1 - available via license: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 InternationalContent may be subject to copyright.DownloadView publicationCopy referenceCopy captionEmbed figureReaction scheme and structural organization of the K. setae ECR complex. (a) Carboxylation reaction scheme of ECR. (b) Anisotropic B-factors of the tetramer of the different ECR complexes solved in this study are shown color coded according to the B factors (blue for low values and red for hig\n",
      "len(sample_chunks) 1\n",
      "Prompt:  \n",
      "Key question:\n",
      "How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\n",
      "\n",
      "Task: \n",
      "Based on the key question and the sample text chunks of the source text, the goal is to identify how useful reading the full source text would be to extract direct quoted facts or references to determine the best answer to the key question. \n",
      "\n",
      "Deliverable:\n",
      "Assign a predicted usefulness score of the full source text using a 5-point Likert scale, with 1 being very unlikely to be usefulness, 2 being unlikely to be useful, 3 being somewhat likely to be useful, 4 being likely to be useful, and 5 being very likely useful and containing facts or references that answer the key question.\n",
      "\n",
      "Sample text chunks from the source text:\n",
      "['Figure 1 - available via license: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 InternationalContent may be subject to copyright.DownloadView publicationCopy referenceCopy captionEmbed figureReaction scheme and structural organization of the K. setae ECR complex. (a) Carboxylation reaction scheme of ECR. (b) Anisotropic B-factors of the tetramer of the different ECR complexes solved in this study are shown color coded according to the B factors (blue for low values and red for high values).Source publication\\nIntersubunit Coupling Enables Fast CO 2 -Fixation by Reductive CarboxylasesArticleFull-text availableApr 2022 Hasan Demirci Yashas Rao Gabriele Stoffel[...] Soichi WakatsukiEnoyl-CoA carboxylases/reductases (ECRs) are some of the most efficient CO2-fixing enzymes described to date. However, the molecular mechanisms underlying the extraordinary catalytic activity of ECRs on the level of the protein assembly remain elusive. Here we used a combination of ambient-tempera']\n",
      "\n",
      "The output should be of the following JSON format\n",
      "{\n",
      "    \"\"predicted_usefulness: <insert predicted usefulness rating>,\n",
      "   etc.\n",
      "}\n",
      "\n",
      "\n",
      "Respond only with the output, with no explanation or conversation.\n",
      "\n",
      "Completion info:  {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"{\\n    \\\"predicted_usefulness\\\": 4\\n}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1691418907,\n",
      "  \"id\": \"chatcmpl-7kvc3vf8TCXSFBNy6Kh8NtW9EL4PV\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 11,\n",
      "    \"prompt_tokens\": 466,\n",
      "    \"total_tokens\": 477\n",
      "  }\n",
      "}\n",
      "query  1 rating_source_idx 0 Skimming url: https://pubmed.ncbi.nlm.nih.gov/31243147/\n",
      "Could not get pdf\n",
      "Going through url:  https://pubmed.ncbi.nlm.nih.gov/31243147/\n",
      "select firefox options!\n",
      "Driver is getting url\n",
      "set timeout!\n",
      "Page loaded within 15 seconds\n",
      "Driver got url\n",
      "Driver has found page source\n",
      "Handing off to Beautiful Soup!\n",
      "done extractin\n",
      "Text:  This site needs JavaScript to work properly. Please enable it to take advantage of the complete set of features!\n",
      "Clipboard, Search History, and several other advanced features are temporarily unavailable.\n",
      "Skip to main page content\n",
      "The .gov means it’s official.\n",
      "Federal government websites often end in .gov or .mil. Before\n",
      "sharing sensitive information, make sure you’re on a federal\n",
      "government site.\n",
      "The site is secure.\n",
      "The https:// ensures that you are connecting to the\n",
      "official website and that a\n",
      "len(sample_chunks) 1\n",
      "Prompt:  \n",
      "Key question:\n",
      "How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\n",
      "\n",
      "Task: \n",
      "Based on the key question and the sample text chunks of the source text, the goal is to identify how useful reading the full source text would be to extract direct quoted facts or references to determine the best answer to the key question. \n",
      "\n",
      "Deliverable:\n",
      "Assign a predicted usefulness score of the full source text using a 5-point Likert scale, with 1 being very unlikely to be usefulness, 2 being unlikely to be useful, 3 being somewhat likely to be useful, 4 being likely to be useful, and 5 being very likely useful and containing facts or references that answer the key question.\n",
      "\n",
      "Sample text chunks from the source text:\n",
      "['Publication types\\nMeSH terms\\nSubstances\\nSupplementary concepts\\nAssociated data\\nRelated information\\nLinkOut - more resources\\nFull text links\\nCiteDisplay options\\nDisplay options\\nFormat\\nAbstractPubMedPMID\\nAbstract\\nCarboxylases are biocatalysts that capture and convert carbon dioxide (CO2) under mild conditions and atmospheric concentrations at a scale of more than 400 Gt annually. However, how these enzymes bind and control the gaseous CO2 molecule during catalysis is only poorly understood. One of the most efficient classes of carboxylating enzymes are enoyl-CoA carboxylases/reductases (Ecrs), which outcompete the plant enzyme RuBisCO in catalytic efficiency and fidelity by more than an order of magnitude. Here we investigated the interactions of CO2 within the active site of Ecr from Kitasatospora setae Combining experimental biochemistry, protein crystallography, and advanced computer simulations we show that 4 amino acids, N81, F170, E171, and H365, are required to create a highly eff']\n",
      "\n",
      "The output should be of the following JSON format\n",
      "{\n",
      "    \"\"predicted_usefulness: <insert predicted usefulness rating>,\n",
      "   etc.\n",
      "}\n",
      "\n",
      "\n",
      "Respond only with the output, with no explanation or conversation.\n",
      "\n",
      "Completion info:  {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"{\\n    \\\"predicted_usefulness\\\": 4\\n}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1691418925,\n",
      "  \"id\": \"chatcmpl-7kvcLfSk4cTwgR56OmmQdeEQn3kjK\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 11,\n",
      "    \"prompt_tokens\": 466,\n",
      "    \"total_tokens\": 477\n",
      "  }\n",
      "}\n",
      "query  1 rating_source_idx 0 Skimming url: https://phys.org/news/2022-04-soil-microbe-rev-artificial-photosynthesis.html\n",
      "Could not get pdf\n",
      "Going through url:  https://phys.org/news/2022-04-soil-microbe-rev-artificial-photosynthesis.html\n",
      "select firefox options!\n",
      "Driver is getting url\n",
      "set timeout!\n",
      "Page loaded within 15 seconds\n",
      "Driver got url\n",
      "Driver has found page source\n",
      "Handing off to Beautiful Soup!\n",
      "done extractin\n",
      "Text:  share this!\n",
      "203\n",
      "43\n",
      "Share\n",
      "Email\n",
      "Home\n",
      "Biology\n",
      "Biotechnology\n",
      "Home\n",
      "Biology\n",
      "Molecular & Computational biology\n",
      "April 29, 2022\n",
      "How a soil microbe could rev up artificial photosynthesis\n",
      "by Glennda Chui, \t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t SLAC National Accelerator Laboratory\n",
      "A close-up look at Kitasatospora setae, a bacterium isolated from soil in Japan. These bacteria fix carbon—turn carbon dioxide from their environment into biomolecules they need to survive—thanks to enzymes called ECRs. Researchers are looking fo\n",
      "len(sample_chunks) 1\n",
      "Prompt:  \n",
      "Key question:\n",
      "How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\n",
      "\n",
      "Task: \n",
      "Based on the key question and the sample text chunks of the source text, the goal is to identify how useful reading the full source text would be to extract direct quoted facts or references to determine the best answer to the key question. \n",
      "\n",
      "Deliverable:\n",
      "Assign a predicted usefulness score of the full source text using a 5-point Likert scale, with 1 being very unlikely to be usefulness, 2 being unlikely to be useful, 3 being somewhat likely to be useful, 4 being likely to be useful, and 5 being very likely useful and containing facts or references that answer the key question.\n",
      "\n",
      "Sample text chunks from the source text:\n",
      "['lding on previous developments but never inventing something entirely new from scratch.\\nWhat\\'s more, he said, the step in natural photosynthesis that fixes CO2 from the air, which relies on an enzyme called Rubisco, is a bottleneck that bogs the whole chain of photosynthetic reactions down. So using speedy ECR enzymes to carry out this step, and engineering them to go even faster, could bring a big boost in efficiency.\\n\"We aren\\'t trying to make a carbon copy of photosynthesis,\" Erb explained. \"We want to design a process that\\'s much more efficient by using our understanding of engineering to rebuild the concepts of nature. This \\'photosynthesis 2.0\\' could take place in living or synthetic systems such as artificial chloroplasts—droplets of water suspended in oil.\"\\nPortraits of an enzyme\\nWakatsuki and his group had been investigating a related system, nitrogen fixation, which converts nitrogen gas from the atmosphere into compounds that living things need. Intrigued by the question of wh']\n",
      "\n",
      "The output should be of the following JSON format\n",
      "{\n",
      "    \"\"predicted_usefulness: <insert predicted usefulness rating>,\n",
      "   etc.\n",
      "}\n",
      "\n",
      "\n",
      "Respond only with the output, with no explanation or conversation.\n",
      "\n",
      "Completion info:  {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"{\\n    \\\"predicted_usefulness\\\": 4\\n}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1691418963,\n",
      "  \"id\": \"chatcmpl-7kvcxGzz6iElDCFpPG93wtM3ZYGRB\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 11,\n",
      "    \"prompt_tokens\": 452,\n",
      "    \"total_tokens\": 463\n",
      "  }\n",
      "}\n",
      "query  1 rating_source_idx 1 Skimming url: https://www.nature.com/articles/s41929-019-0385-5\n",
      "Could not get pdf\n",
      "Going through url:  https://www.nature.com/articles/s41929-019-0385-5\n",
      "select firefox options!\n",
      "Driver is getting url\n",
      "set timeout!\n",
      "Page loaded within 15 seconds\n",
      "Driver got url\n",
      "Driver has found page source\n",
      "Handing off to Beautiful Soup!\n",
      "done extractin\n",
      "Text:  We use cookies to make sure that our website works properly, as well as some optional cookies to personalise content and advertising, provide social media features and analyse how people use our site. By accepting some or all optional cookies you give consent to the processing of your personal data, including transfer to third parties, some in countries outside of the European Economic Area that do not offer the same data protection standards as the country where you live. You can decide which o\n",
      "len(sample_chunks) 5\n",
      "Prompt:  \n",
      "Key question:\n",
      "How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\n",
      "\n",
      "Task: \n",
      "Based on the key question and the sample text chunks of the source text, the goal is to identify how useful reading the full source text would be to extract direct quoted facts or references to determine the best answer to the key question. \n",
      "\n",
      "Deliverable:\n",
      "Assign a predicted usefulness score of the full source text using a 5-point Likert scale, with 1 being very unlikely to be usefulness, 2 being unlikely to be useful, 3 being somewhat likely to be useful, 4 being likely to be useful, and 5 being very likely useful and containing facts or references that answer the key question.\n",
      "\n",
      "Sample text chunks from the source text:\n",
      "['eantime, to ensure continued support, we are displaying the site without styles\\nand JavaScript.\\nAdvertisement\\nnature\\nnature catalysis\\nreview articles\\narticle\\nAbstractThe efficiency, selectivity and sustainability benefits offered by enzymes are enticing chemists to consider biocatalytic transformations to complement or even supplant more traditional synthetic routes. Increasing demands for efficient and versatile synthetic methods, combined with powerful new discovery and engineering tools, has prompted innovations in biocatalysis, especially the development of new enzymes for precise transformations or ‘molecular editing’. As a result, the past decade has witnessed an impressive expansion of the catalytic repertoire of enzymes to include new and useful transformations not known (or relevant) in the biological world. In this Review we illustrate various ways in which researchers have approached using the catalytic machineries of enzymes for new-to-nature transformations. These efforts ', ' S. Enzyme promiscuity: a mechanistic and evolutionary perspective. Annu. Rev. Biochem. 79, 471–505 (2010). An in-depth Review of how to recognize and understand enzyme promiscuity.\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nTawfik, D. S. Messy biology and the origins of evolutionary innovations. Nat. Chem. Bio. 6, 692–696 (2010).Article\\nCAS\\nGoogle Scholar\\nBusto, E., Gotor-Fernández, V. & Gotor, V. Hydrolases: catalytically promiscuous enzymes for non-conventional reactions in organic synthesis. Chem. Soc. Rev. 39, 4504–4523 (2010).Article\\nCAS\\nPubMed\\nGoogle Scholar\\nCampos, K. R. et al. The importance of synthetic chemistry in the pharmaceutical industry. Science 363, eaat0805 (2019).Article\\nCAS\\nPubMed\\nGoogle Scholar\\nBornscheuer, U. T. & Kazlauskas, R. J. Catalytic promiscuity in biocatalysis: using old enzymes to form new bonds and follow new pathways. Angew. Chem. Int. Ed. 43, 6032–6040 (2004).Article\\nCAS\\nGoogle Scholar\\nBranneby, C. et al. Carbon–carbon bonds by hydrolytic enzymes. J. Am. Chem', 'g. 3: New chemistries with cofactor-dependent enzymes.Fig. 4: Chemomimetic carbene- and nitrene-transfer chemistries with engineered haem proteins.Fig. 5: Different strategies for artificial enzyme construction.\\nHönig, M., Sondermann, P., Carreira, E. M. & Turner, N. Enantioselective chemo- and biocatalysis: partners in retrosynthesis. Angew. Chem. Int. Ed. 56, 8942–8973 (2017). This Review covers many examples of designing retrosynthetic routes with enzymes.\\nArticle\\xa0CAS\\nGoogle Scholar\\xa0de Souza, R. O. M. A., Miranda, L. S. M. & Bornscheuer, U. T. A retrosynthetic approach for biocatalysis in organic synthesis. Chem. Eur. J. 23, 12040–12063 (2017).Article\\xa0CAS\\xa0PubMed\\nGoogle Scholar\\xa0Turner, N. J. & Humphrey, L. Biocatalysis in Organic Synthesis: The Retrosynthesis Approach (Royal Society of Chemistry, 2018).Huang, P.-S., Boyken, S. E. & Baker, D. The coming of age of de novo protein design. Nature 537, 320–327 (2016).Article\\xa0CAS\\xa0PubMed\\nGoogle Scholar\\xa0Zanghellini, A. de novo computational ', 'enzyme design. Curr. Opin. Chem. Biol. 29, 132–138 (2014).CAS\\nGoogle Scholar\\xa0Khersonsky, O. & Tawfik, D. S. Enzyme promiscuity: a mechanistic and evolutionary perspective. Annu. Rev. Biochem. 79, 471–505 (2010). An in-depth Review of how to recognize and understand enzyme promiscuity.\\nArticle\\xa0CAS\\xa0PubMed\\nGoogle Scholar\\xa0Tawfik, D. S. Messy biology and the origins of evolutionary innovations. Nat. Chem. Bio. 6, 692–696 (2010).Article\\xa0CAS\\nGoogle Scholar\\xa0Busto, E., Gotor-Fernández, V. & Gotor, V. Hydrolases: catalytically promiscuous enzymes for non-conventional reactions in organic synthesis. Chem. Soc. Rev. 39, 4504–4523 (2010).Article\\xa0CAS\\xa0PubMed\\nGoogle Scholar\\xa0Campos, K. R. et al. The importance of synthetic chemistry in the pharmaceutical industry. Science 363, eaat0805 (2019).Article\\xa0CAS\\xa0PubMed\\nGoogle Scholar\\xa0Bornscheuer, U. T. & Kazlauskas, R. J. Catalytic promiscuity in biocatalysis: using old enzymes to form new bonds and follow new pathways. Angew. Chem. Int. Ed. 43, 6032–6040 (200', 'tics of native enzymes. Science 354, 102–106 (2016).Article\\xa0CAS\\xa0PubMed\\nGoogle Scholar\\xa0Key, H. M. et al. Beyond iron: iridium-containing P450 enzymes for selective cyclopropanations of structurally diverse alkenes. ACS Cent. Sci. 3, 302–308 (2017).Article\\xa0CAS\\xa0PubMed\\xa0PubMed Central\\nGoogle Scholar\\xa0Dydio, P. et al. Chemoselective, enzymatic C‒H bond amination catalyzed by a cytochrome P450 containing an Ir(Me)–PIX cofactor. J. Am. Chem. Soc. 139, 1750–1753 (2017).Article\\xa0CAS\\xa0PubMed\\nGoogle Scholar\\xa0Agostini, F. et al. Biocatalysis with unnatural amino acids: enzymology meets xenobiology. Angew. Chem. Int. Ed. 56, 9680–9703 (2017).Article\\xa0CAS\\nGoogle Scholar\\xa0Drienovská, I., Mayer, C., Dulson, C. & Roelfes, G. A designer enzyme for hydrazone and oxime formation featuring an unnatural catalytic aniline residue. Nat. Chem. 10, 946–952 (2018).Article\\xa0CAS\\xa0PubMed\\nGoogle Scholar\\xa0Mayer, C. et al. Directed evolution of a designer enzyme featuring an unnatural catalytic amino acid. Angew. Chem. Int. Ed.']\n",
      "\n",
      "The output should be of the following JSON format\n",
      "{\n",
      "    \"\"predicted_usefulness: <insert predicted usefulness rating>,\n",
      "   etc.\n",
      "}\n",
      "\n",
      "\n",
      "Respond only with the output, with no explanation or conversation.\n",
      "\n",
      "Completion info:  {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"{\\n    \\\"predicted_usefulness\\\": 3\\n}\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1691418989,\n",
      "  \"id\": \"chatcmpl-7kvdNnP0YrsjZ0fojOAmdEkKqNDLE\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 11,\n",
      "    \"prompt_tokens\": 1705,\n",
      "    \"total_tokens\": 1716\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "folder_path = f'autoscious_logs/{search_query_file_safe}/sources/full_text'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Skimming through each highly relevant paper from skimming\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'r') as f:\n",
    "    key_question_initial_search_queries = json.load(f)\n",
    "\n",
    "for query_idx, query in key_question_initial_search_queries.items():\n",
    "    # open filtered search results\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/rated_web_results_query_{int(query_idx)}.json', 'r') as f:\n",
    "        ratings_url_dict = json.loads(f.read())\n",
    "\n",
    "    # open web search info to extract metadata\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_results_query_{int(query_idx)}.json', 'r') as f:\n",
    "        web_search_info = json.load(f)\n",
    "    \n",
    "    for rating, urls in ratings_url_dict.items():\n",
    "        if rating == '5' or rating == '4' or rating == '3': # Scraping all useful websites to skim through\n",
    "            # Start with iterating through 4s and 5s of ratings_url_dict\n",
    "            folder_path = f'autoscious_logs/{search_query_file_safe}/sources/predicted_usefulness_{rating}'\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "\n",
    "            for rating_source_idx, url in enumerate(urls):\n",
    "                print(\"query \", query_idx, \"rating_source_idx\", rating_source_idx, \"Skimming url:\", url)\n",
    "\n",
    "                # Ensure the url hasn't already been visited\n",
    "                title = find_title(url, web_search_info)\n",
    "                if title and not os.path.exists(f'autoscious_logs/{sanitize_filename(search_query)}/sources/full_text/{sanitize_filename(title)}.txt') and not os.path.exists(f'{folder_path}/query_{query_idx}_url_index_{rating_source_idx}.json'):\n",
    "\n",
    "                    # Check if it's a pdf or not\n",
    "                    if try_getting_pdf(url):\n",
    "                        print(\"PDF found!\")\n",
    "                        text = try_getting_pdf_content(url)\n",
    "                    else:\n",
    "                        text = scrape_text_with_selenium_no_agent(url, None, search_engine='firefox')\n",
    "\n",
    "                    # Only evaluate websites you're able to scrape\n",
    "                    if text and text != \"No information found\":\n",
    "                        total_chunks = len(text) / CHUNK_SIZE\n",
    "                        num_chunk_samples = min(int(total_chunks * SAMPLING_FACTOR), MAX_CHUNKS)\n",
    "                        # sample_chunks = get_sample_chunks(text, CHUNK_SIZE, num_chunk_samples)\n",
    "                        sample_chunks = get_most_relevant_chunks_with_bm25(keywords_query, text, CHUNK_SIZE, num_chunk_samples) # Using BM25 to search for keywords instead of general query\n",
    "                        print(\"len(sample_chunks)\", len(sample_chunks))\n",
    "\n",
    "                        # Get predicted usefulness based on sample chunks\n",
    "                        predicted_usefulness_results = json.loads(chat_openai(get_predicted_usefulness_of_text_prompt(search_query, sample_chunks), model=\"gpt-3.5-turbo\")[0])\n",
    "\n",
    "                        # save filtered search results\n",
    "                        with open(f'{folder_path}/query_{query_idx}_url_index_{rating_source_idx}.json', 'w') as f:\n",
    "                            predicted_usefulness_results['title'] = title\n",
    "                            predicted_usefulness_results['url'] = url\n",
    "                            json.dump(predicted_usefulness_results, f, indent=2)\n",
    "                        \n",
    "                        # Check if any scores were (4 or) 5, because then we should save the full text\n",
    "                        pred_usefulness = predicted_usefulness_results.values()\n",
    "\n",
    "                        # TODO: perhaps make this more dynamic\n",
    "                        if 5 in pred_usefulness or '5' in pred_usefulness or 4 in pred_usefulness or '4' in pred_usefulness:\n",
    "                        # DEBUG: Just looking at the scraping results\n",
    "                            with open(f'autoscious_logs/{sanitize_filename(search_query)}/sources/full_text/{sanitize_filename(title)}.txt', 'w', encoding='utf-8') as f:\n",
    "                                f.write(title + '\\n')\n",
    "                                f.write(url + '\\n')\n",
    "                                f.write(text)\n",
    "                else:\n",
    "                    print(\"URL or text already visited!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
