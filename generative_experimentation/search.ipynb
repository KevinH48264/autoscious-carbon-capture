{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key question\n",
    "search_query = '''Objective: \n",
    "Carbon capture 1 Gt of CO2 per year ASAP\n",
    "\n",
    "Existing approach and problems:\n",
    "{'approach': 'Direct Air Capture (DAC) with Carbon Storage', 'proposition': 'DAC with carbon storage is an ideal approach as it directly captures CO2 from the atmosphere and stores it in a secure manner. It has the potential to capture large amounts of CO2 and can be deployed in various locations, making it a scalable solution.', 'opposition': 'The biggest weakness of DAC with carbon storage is its high cost per metric ton of CO2 captured. The energy consumption is also significant, requiring a substantial amount of electricity. Additionally, the storage capacity of certain methods may be limited, requiring frequent transportation and storage infrastructure.', 'limitations': {'1': {'limitation': 'High cost per metric ton of CO2 captured', 'metric': 'Cost per metric ton of CO2 captured'}, '2': {'limitation': 'Significant energy consumption', 'metric': 'Energy consumption'}, '3': {'limitation': 'Limited storage capacity', 'metric': 'Storage capacity'}}}\n",
    "\n",
    "First principles:\n",
    "Efficiency: The ideal approach should focus on improving the efficiency of the carbon capture process to reduce the cost per metric ton of CO2 captured. This can be achieved by optimizing the design and operation of the capture technology, minimizing energy requirements, and maximizing the utilization of captured CO2.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from util import sanitize_filename, get_predicted_usefulness_of_text_prompt, web_search_ddg\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from llm import chat_openai\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_file_safe = sanitize_filename(search_query[:50])\n",
    "search_engine = \"academic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f'autoscious_logs/{search_query_file_safe}'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f'autoscious_logs/{search_query_file_safe}/sources'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_search_queries_prompt(key_question, search_engine):\n",
    "  return f'''\n",
    "Key question:\n",
    "{key_question}\n",
    "\n",
    "Task:\n",
    "For the key question, write a clear and comprehensive but short (1 query) list of search queries optimized for best search engine results, so that you can confidently and quickly surface the most relevant information to determine the best answer to the question. Extract a string of search keywords query from the key question.\n",
    "\n",
    "The output should be in JSON format: \n",
    "```json\n",
    "{{\n",
    "  \"1\": \"<insert query>\",\n",
    "  \"keywords_query\": \"<insert keywords>\"\n",
    "}}\n",
    "\n",
    "Respond only with the output, with no explanation or conversation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for decomposition_idx, key_question_decomposition in enumerate(key_question_decomposition_list):\n",
    "key_question_initial_search_queries = json.loads(chat_openai(get_initial_search_queries_prompt(search_query, search_engine), model=\"gpt-3.5-turbo\")[0])\n",
    "\n",
    "keywords_query = key_question_initial_search_queries.pop('keywords_query')\n",
    "\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'w') as f:\n",
    "    json.dump(key_question_initial_search_queries, f, indent=2)\n",
    "\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/keywords_query.txt', 'w') as f:\n",
    "    json.dump(keywords_query, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'r') as f:\n",
    "    key_question_initial_search_queries = json.loads(f.read())\n",
    "\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/keywords_query.txt', 'r') as f:\n",
    "    keywords_query = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web search given search keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "from scholarly import scholarly\n",
    "from scholarly import ProxyGenerator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up a ProxyGenerator object to use free proxies\n",
    "# This needs to be done only once per session\n",
    "pg = ProxyGenerator()\n",
    "pg.FreeProxies()\n",
    "scholarly.use_proxy(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader \n",
    "from io import BytesIO\n",
    "\n",
    "def try_getting_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    f = BytesIO(response.content)\n",
    "    try:\n",
    "        pdf = PdfReader(f)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Could not get pdf\")\n",
    "        return False\n",
    "\n",
    "# Get the PDF content\n",
    "def try_getting_pdf_content(url):\n",
    "    response = requests.get(url)\n",
    "    f = BytesIO(response.content)\n",
    "    try:\n",
    "        pdf = PdfReader(f)\n",
    "        content = \"\"\n",
    "\n",
    "        for i in range(len(pdf.pages)):\n",
    "            page = pdf.pages[i]\n",
    "            text = page.extract_text()\n",
    "            content += text\n",
    "        return content\n",
    "    except:\n",
    "        print(\"Error getting PDF content\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def google_search_raw(search_term, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=os.getenv('DEV_KEY'))\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "\n",
    "    search_results = res.get(\"items\", [])\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Create a list of only the URLs from the search results\n",
    "    search_results_links = [item[\"link\"] for item in search_results]\n",
    "    return search_results\n",
    "\n",
    "def search_google(search_query):\n",
    "    num_google_searches = 8\n",
    "    results = google_search_raw(search_query, os.getenv('MY_CSE_ID'), num=num_google_searches, lr=\"lang_en\", cr=\"countryUS\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  Carbon capture technologies\n",
      "trying academic search\n",
      "Exception, trying normal search\n",
      "trying normal search\n"
     ]
    }
   ],
   "source": [
    "MAX_RETRIES = 3\n",
    "\n",
    "# for decomposition_idx, key_question_decomposition in enumerate(key_question_decomposition_list):\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'r') as f:\n",
    "    key_question_initial_search_queries = json.load(f)\n",
    "\n",
    "for idx, query in key_question_initial_search_queries.items():\n",
    "    print(\"query: \", query)\n",
    "    # query = \"ECR enzyme efficiency in k setae\" # Hard coded to get the results I want\n",
    "\n",
    "    web_search_res = []\n",
    "    if search_engine == \"academic\":\n",
    "        print(\"trying academic search\")\n",
    "        try:\n",
    "            scholar_res_gen = scholarly.search_pubs(query)\n",
    "\n",
    "            for res in scholar_res_gen:\n",
    "                item = {}\n",
    "                item['title'] = res['bib']['title']\n",
    "                if try_getting_pdf(res['eprint_url']):\n",
    "                    item['href'] = res['eprint_url']\n",
    "                    item['pdf'] = True\n",
    "                else:\n",
    "                    item['href'] = res['pub_url']\n",
    "                    item['pdf'] = False\n",
    "                item['body'] = res['bib']['abstract']\n",
    "                web_search_res += [item]\n",
    "        except: \n",
    "            print(\"Exception, trying normal search\")\n",
    "    if web_search_res == []:\n",
    "        # DDG\n",
    "        print(\"trying normal search\")\n",
    "        web_search_res = json.loads(web_search_ddg(query))\n",
    "        if len(web_search_res) == 0:\n",
    "            print(\"trying google search!\")\n",
    "            # Google\n",
    "            web_search_res_raw = search_google(query) # google uses 'link' instead of 'href'\n",
    "            web_search_res = [{\n",
    "                'title': web_search_res_raw[i]['title'], \n",
    "                'href': web_search_res_raw[i]['link'], \n",
    "                'body': web_search_res_raw[i]['snippet'],\n",
    "                'pdf': False\n",
    "                } for i in range(len(web_search_res_raw))\n",
    "            ]\n",
    "\n",
    "    # save web search results\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_results_query_{idx}.json', 'w') as f:\n",
    "        json.dump(web_search_res, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading type 1: filtering unlikely relevant sources based on title and body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtering_web_results_ratings(key_question, web_search_res):\n",
    "    return f'''\n",
    "Key question:\n",
    "{key_question}\n",
    "\n",
    "Task:\n",
    "Based on the key question and each search result's title and body content, reason and assign a predicted usefulness score of the search result's content and potential useful references to answering the key question using a 5-point Likert scale, with 1 being very not useful, 2 being not useful, 3 being somewhat useful, 4 being useful, 5 being very useful.\n",
    "\n",
    "Search results:\n",
    "{web_search_res}\n",
    "\n",
    "The output should be in JSON format: \n",
    "```json\n",
    "{{\n",
    "  'href': 'relevance score',\n",
    "  etc.\n",
    "}}\n",
    "```\n",
    "\n",
    "Respond only with the output, with no explanation or conversation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'r') as f:\n",
    "    key_question_initial_search_queries = json.load(f)\n",
    "\n",
    "for query_idx, query in key_question_initial_search_queries.items():\n",
    "    # load web search results\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_results_query_{query_idx}.json', 'r') as f:\n",
    "        web_search_res = json.loads(f.read())\n",
    "    \n",
    "    filtered_web_results = {}\n",
    "    if web_search_res != []:\n",
    "        # filter web results based on title and body\n",
    "        filtered_web_results = json.loads(chat_openai(get_filtering_web_results_ratings(search_query, web_search_res), model=\"gpt-3.5-turbo\")[0])\n",
    "\n",
    "    ratings_url_dict = defaultdict(list)\n",
    "    for url, rating in filtered_web_results.items():\n",
    "        ratings_url_dict[str(rating)].append(url)\n",
    "\n",
    "    # save filtered search results\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/rated_web_results_query_{int(query_idx)}.json', 'w') as f:\n",
    "        json.dump(ratings_url_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading type 2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE code for predicting usefulness of very relevant (5) and relevant (4) results.\n",
    "from util import scrape_text_with_selenium_no_agent\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "SAMPLING_FACTOR = 0.1 # Also cap it so it falls under the max token limit\n",
    "MAX_TOKENS = 2500 * 4 # 1 token = 4 chars, 2500 + 500 (prompt) tokens is high for GPT3.5\n",
    "MAX_CHUNKS = int(MAX_TOKENS / CHUNK_SIZE)\n",
    "# context = \"Enoyl-CoA carboxylase/reductase enzymes (ECRs)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_chunks(text, CHUNK_SIZE, num_chunk_samples):\n",
    "    step_size = len(text) // num_chunk_samples\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), step_size):\n",
    "        chunk = text[i:i+CHUNK_SIZE]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Break after getting the required number of chunks\n",
    "        if len(chunks) >= num_chunk_samples:\n",
    "            break\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to determine how useful the text is likely to be for answering the key questions\n",
    "def get_predicted_usefulness_of_text_prompt(key_question, sample_text_chunks):\n",
    "    return f'''\n",
    "Key question:\n",
    "{key_question}\n",
    "\n",
    "Task: \n",
    "Based on the key question and the sample text chunks of the source text, the goal is to identify how useful reading the full source text would be to extract direct quoted facts or references to determine the best answer to the key question. \n",
    "\n",
    "Deliverable:\n",
    "Assign a predicted usefulness score of the full source text using a 5-point Likert scale, with 1 being very unlikely to be usefulness, 2 being unlikely to be useful, 3 being somewhat likely to be useful, 4 being likely to be useful, and 5 being very likely useful and containing facts or references that answer the key question.\n",
    "\n",
    "Sample text chunks from the source text:\n",
    "{sample_text_chunks}\n",
    "\n",
    "The output should be of the following JSON format\n",
    "{{\n",
    "    \"\"predicted_usefulness: <insert predicted usefulness rating>,\n",
    "   etc.\n",
    "}}\n",
    "\n",
    "\n",
    "Respond only with the output, with no explanation or conversation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "def get_most_relevant_chunks_with_bm25(key_question, text, CHUNK_SIZE, num_chunk_samples):\n",
    "    # 1. Split text into chunks\n",
    "    chunks = [text[i:i+CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n",
    "\n",
    "    # 2. Tokenize the chunks\n",
    "    tokenized_chunks = [re.findall(r\"\\w+\", chunk) for chunk in chunks]\n",
    "\n",
    "    # 3. Initialize BM25\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "    # 4. Query BM25 with the key question\n",
    "    tokenized_question = re.findall(r\"\\w+\", key_question)\n",
    "    scores = bm25.get_scores(tokenized_question)\n",
    "\n",
    "    # 5. Sort chunks by BM25 scores\n",
    "    sorted_chunks = [chunk for _, chunk in sorted(zip(scores, chunks), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    # 6. Return top num_chunk_samples chunks\n",
    "    return sorted_chunks[:num_chunk_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_title(url, web_search_info):\n",
    "    for item in web_search_info:\n",
    "        if item[\"href\"] == url:\n",
    "            return item[\"title\"]\n",
    "    return None\n",
    "\n",
    "def check_pdf(url, web_search_info):\n",
    "    for item in web_search_info:\n",
    "        if \"pdf\" in item.keys() and item[\"pdf\"]:\n",
    "            return True\n",
    "    print(\"Not pdf\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  1 rating_source_idx 0 Skimming url: https://climate.mit.edu/explainers/carbon-capture\n",
      "URL or text already visited!\n",
      "query  1 rating_source_idx 1 Skimming url: https://www.npr.org/2021/08/08/1025845745/what-is-carbon-capture-technology-it-has-a-complicated-place-in-the-infrastructu\n",
      "URL or text already visited!\n",
      "query  1 rating_source_idx 2 Skimming url: https://www.honeywell.com/us/en/news/2022/09/how-carbon-capture-works\n",
      "URL or text already visited!\n",
      "query  1 rating_source_idx 3 Skimming url: https://www.nationalgrid.com/stories/energy-explained/carbon-capture-technology-and-how-it-works\n",
      "URL or text already visited!\n",
      "query  1 rating_source_idx 0 Skimming url: https://www.nytimes.com/interactive/2023/03/19/us/carbon-capture.html\n",
      "Could not get pdf\n",
      "Going through url:  https://www.nytimes.com/interactive/2023/03/19/us/carbon-capture.html\n",
      "select firefox options!\n",
      "Driver is getting url\n",
      "set timeout!\n",
      "Page loaded within 15 seconds\n",
      "Driver got url\n",
      "Driver has found page source\n",
      "Handing off to Beautiful Soup!\n",
      "done extractin\n",
      "Text:  How Does Carbon Capture Work? The idea of removing carbon dioxide from the atmosphere to turn back the clock on climate change is an appealing one. Can these technologies deliver on their promise? By Eden Weingart March 19, 2023\n",
      "Share full article\n",
      "The world has a carbon problem. To solve it will require moving away from burning carbon-emitting fuels and relying instead on cleaner energy sources like wind turbines and solar cells. But is there anything we can do about all the carbon dioxide that \n"
     ]
    }
   ],
   "source": [
    "folder_path = f'autoscious_logs/{search_query_file_safe}/sources/full_text'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Skimming through each highly relevant paper from skimming\n",
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_queries.json', 'r') as f:\n",
    "    key_question_initial_search_queries = json.load(f)\n",
    "\n",
    "for query_idx, query in key_question_initial_search_queries.items():\n",
    "    # open filtered search results\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/rated_web_results_query_{int(query_idx)}.json', 'r') as f:\n",
    "        ratings_url_dict = json.loads(f.read())\n",
    "\n",
    "    # open web search info to extract metadata\n",
    "    with open(f'autoscious_logs/{search_query_file_safe}/sources/initial_search_results_query_{int(query_idx)}.json', 'r') as f:\n",
    "        web_search_info = json.load(f)\n",
    "    \n",
    "    for rating, urls in ratings_url_dict.items():\n",
    "        if rating == '5' or rating == '4' or rating == '3': # Scraping all useful websites to skim through\n",
    "            # Start with iterating through 4s and 5s of ratings_url_dict\n",
    "            folder_path = f'autoscious_logs/{search_query_file_safe}/sources/predicted_usefulness_{rating}'\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "\n",
    "            for rating_source_idx, url in enumerate(urls):\n",
    "                print(\"query \", query_idx, \"rating_source_idx\", rating_source_idx, \"Skimming url:\", url)\n",
    "\n",
    "                # Ensure the url hasn't already been visited\n",
    "                title = find_title(url, web_search_info)\n",
    "                if title and not os.path.exists(f'autoscious_logs/{sanitize_filename(search_query)}/sources/full_text/{sanitize_filename(title)}.txt') and not os.path.exists(f'{folder_path}/query_{query_idx}_url_index_{rating_source_idx}.json'):\n",
    "\n",
    "                    # Check if it's a pdf or not\n",
    "                    if try_getting_pdf(url):\n",
    "                        print(\"PDF found!\")\n",
    "                        text = try_getting_pdf_content(url)\n",
    "                    else:\n",
    "                        text = scrape_text_with_selenium_no_agent(url, None, search_engine='firefox')\n",
    "\n",
    "                    # Only evaluate websites you're able to scrape\n",
    "                    if text and text != \"No information found\":\n",
    "                        total_chunks = len(text) / CHUNK_SIZE\n",
    "                        num_chunk_samples = min(int(total_chunks * SAMPLING_FACTOR), MAX_CHUNKS)\n",
    "                        # sample_chunks = get_sample_chunks(text, CHUNK_SIZE, num_chunk_samples)\n",
    "                        sample_chunks = get_most_relevant_chunks_with_bm25(keywords_query, text, CHUNK_SIZE, num_chunk_samples) # Using BM25 to search for keywords instead of general query\n",
    "                        print(\"len(sample_chunks)\", len(sample_chunks))\n",
    "\n",
    "                        # Get predicted usefulness based on sample chunks\n",
    "                        predicted_usefulness_results = json.loads(chat_openai(get_predicted_usefulness_of_text_prompt(search_query, sample_chunks), model=\"gpt-3.5-turbo\")[0])\n",
    "\n",
    "                        # save filtered search results\n",
    "                        with open(f'{folder_path}/query_{query_idx}_url_index_{rating_source_idx}.json', 'w') as f:\n",
    "                            predicted_usefulness_results['title'] = title\n",
    "                            predicted_usefulness_results['url'] = url\n",
    "                            json.dump(predicted_usefulness_results, f, indent=2)\n",
    "                        \n",
    "                        # Check if any scores were (4 or) 5, because then we should save the full text\n",
    "                        pred_usefulness = predicted_usefulness_results.values()\n",
    "\n",
    "                        # TODO: perhaps make this more dynamic\n",
    "                        if 5 in pred_usefulness or '5' in pred_usefulness or 4 in pred_usefulness or '4' in pred_usefulness:\n",
    "                        # DEBUG: Just looking at the scraping results\n",
    "                            with open(f'autoscious_logs/{sanitize_filename(search_query)}/sources/full_text/{sanitize_filename(title)}.txt', 'w', encoding='utf-8') as f:\n",
    "                                f.write(title + '\\n')\n",
    "                                f.write(url + '\\n')\n",
    "                                f.write(text)\n",
    "                else:\n",
    "                    print(\"URL or text already visited!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) create facts folder and subfolders\n",
    "facts_folder_path = f'autoscious_logs/{sanitize_filename(search_query)}/facts'\n",
    "if not os.path.exists(facts_folder_path):\n",
    "    os.makedirs(facts_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be broken to prompts.py and util.py\n",
    "import os\n",
    "\n",
    "def extract_facts_from_website_text(search_query_file_safe, key_question, website_title, website_text, website_url):\n",
    "    seed_initial_question_decomposition_prompt = f'''\n",
    "Key question: \n",
    "{key_question}\n",
    "\n",
    "Context:\n",
    "Ideally, I'm looking for a numerical answer.\n",
    "\n",
    "Task:\n",
    "What's the best answer based on the source text? Give me as specific and correct of an answer as possible. Then, quote the section of the source text that supports your answer. \n",
    "\n",
    "The output should be in JSON format: \n",
    "```json\n",
    "{{\n",
    "  \"<insert best answer>\": \"<insert quote>\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Source text: {website_text}\n",
    "\n",
    "Respond only with the output, with no explanation or conversation.\n",
    "'''\n",
    "    # Ask GPT the prompt\n",
    "    print(\"seed_initial_question_decomposition_prompt\", seed_initial_question_decomposition_prompt)\n",
    "    res = chat_openai(seed_initial_question_decomposition_prompt, model=\"gpt-3.5-turbo\")\n",
    "    print(\"Extracted quotes: \", res[0])\n",
    "\n",
    "    # Save the quote to the corresponding key question index file\n",
    "    res_json = json.loads(res[0])\n",
    "    for key, value in res_json.items():\n",
    "        answer = key\n",
    "        quote = value\n",
    "\n",
    "        # Only log if there is a quote\n",
    "        if quote:\n",
    "          file_name = f'autoscious_logs/{search_query_file_safe}/facts/facts.txt'\n",
    "\n",
    "          with open(file_name, 'a', encoding='utf-8') as f:\n",
    "              f.write(answer + os.linesep)\n",
    "\n",
    "          # Save the best answer and quote into a JSON for reference retrieval later\n",
    "          json_file_name = f'autoscious_logs/{search_query_file_safe}/facts/facts.json'\n",
    "          if os.path.exists(json_file_name):\n",
    "            with open(json_file_name, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "          else:\n",
    "              data = {}\n",
    "\n",
    "          # Update the dictionary and save it back\n",
    "          data[answer] = quote.replace('/\"', '\"') + f\"[{website_url}]\"\n",
    "          \n",
    "          with open(json_file_name, 'w', encoding='utf-8') as json_file:\n",
    "              json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "def chunk_text(text: str, key_question: str, chunk_size: int, overlap: int = 0) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits a text into overlapping chunks and ranks them using BM25 based on relevance to a key question.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The source text.\n",
    "    - key_question (str): The key question to rank the chunks by.\n",
    "    - chunk_size (int): The size of each chunk.\n",
    "    - num_chunk_samples (int): The number of top-ranked chunks to return.\n",
    "    - overlap (int): The size of the overlap between chunks. Default is 0.\n",
    "    \n",
    "    Returns:\n",
    "    - list[str]: The top-ranked chunks based on BM25 scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Split text into overlapping chunks\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)]\n",
    "\n",
    "    # 2. Tokenize the chunks\n",
    "    tokenized_chunks = [re.findall(r\"\\w+\", chunk) for chunk in chunks]\n",
    "\n",
    "    # 3. Initialize BM25\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "    # 4. Query BM25 with the key question\n",
    "    tokenized_question = re.findall(r\"\\w+\", key_question)\n",
    "    scores = bm25.get_scores(tokenized_question)\n",
    "\n",
    "    # 5. Sort chunks by BM25 scores\n",
    "    sorted_chunks = [chunk for _, chunk in sorted(zip(scores, chunks), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "    # 6. Return top num_chunk_samples chunks\n",
    "    return sorted_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'autoscious_logs/{search_query_file_safe}/sources/keywords_query.txt', 'r') as f:\n",
    "    keywords_query = f.read().strip('\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method can partially extract the answer, but not the exact table passage which is critical NOR does it prioritize the list of facts\n",
    "# Go through each full text rated highly to extract facts from\n",
    "full_text_folder_path = f'autoscious_logs/{sanitize_filename(search_query)}/sources/full_text'\n",
    "\n",
    "# Loop through every file in the directory, just goes in order\n",
    "for filename in os.listdir(full_text_folder_path):\n",
    "    curr_tokens = 0\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(full_text_folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            title = file.readline().strip()\n",
    "            url = file.readline().strip()\n",
    "            text = file.read()\n",
    "        \n",
    "        # Break the text into chunk to extract information from\n",
    "        chunks = chunk_text(text, keywords_query, chunk_size, overlap)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Chunk: {i} / {len(chunks)}\")\n",
    "            extract_facts_from_website_text(search_query_file_safe, search_query, title, chunk, url)\n",
    "\n",
    "            curr_tokens += len(chunk)\n",
    "            if curr_tokens > MAX_TOKENS:\n",
    "                print(\"Max tokens reached in chunks!\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerank facts.txt file based on relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'autoscious_logs/{search_query_file_safe}/facts/facts.txt'\n",
    "\n",
    "with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    facts_list = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rerank_facts_list_prompt(facts_list):\n",
    "  return f'''\n",
    "Key question: \n",
    "How efficiently do the ECR enzymes work in Kitsatospor setae bacteria?\n",
    "\n",
    "Context:\n",
    "Ideally, I'm looking for a numerical answer.\n",
    "\n",
    "Task:\n",
    "Rerank the following facts based on how well they answer the key question. The more specific and correct, the better. The best answer should be at the top, and the worst answer should be at the bottom of the list. Use direct quotes and do not change the wording of the facts. Leave out facts that are not relevant to the key question.\n",
    "\n",
    "Current facts list:\n",
    "{facts_list}\n",
    "\n",
    "The output should be a JSON list of facts:\n",
    "```json\n",
    "['<insert fact>', etc.]\n",
    "```\n",
    "\n",
    "Respond only with the output, with no explanation or conversation.\n",
    "'''\n",
    "reranked_facts_list = json.loads(chat_openai(get_rerank_facts_list_prompt(facts_list), model=\"gpt-3.5-turbo\")[0])\n",
    "print(reranked_facts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'autoscious_logs/{search_query_file_safe}/facts/facts_reranked.txt'\n",
    "\n",
    "with open(file_name, 'w', encoding='utf-8') as f:\n",
    "    for answer in reranked_facts_list:\n",
    "        f.write(answer + os.linesep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
