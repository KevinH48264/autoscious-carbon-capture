{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Pre-processing - 1] Add embeddings from each paperId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# NOTE: Only need to edit this with latest papers JSON for pre-processing steps 1 and 2\n",
    "# Load in latest JSON files\n",
    "papers_json_path = r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\knowledge_base\\papers\\23-07-25_11935_database_update.json'\n",
    "\n",
    "# load the data from your JSON file\n",
    "with open(papers_json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print out the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# text not None: \", len(df[df['text'].notna()]), \"# None: \", len(df[df['text'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the conditions for the 'text' column\n",
    "condition = df['text'].isna()\n",
    "\n",
    "# Create a subset dataframe where 'text' is NaN\n",
    "subset_df = df[condition]\n",
    "\n",
    "# Define what the 'text' column should be if the condition is met\n",
    "value_when_true = 'Title: ' + subset_df['title'].astype(str) + '. Abstract: ' + subset_df['abstract'].astype(str)\n",
    "\n",
    "# Define what the 'text' column should be if the abstract is na\n",
    "value_when_abstract_na = 'Title: ' + subset_df['title'].astype(str) + '.'\n",
    "\n",
    "# Apply the conditions to the DataFrame\n",
    "df.loc[condition, 'text'] = np.where(subset_df['abstract'].isna(), value_when_abstract_na, value_when_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# text not None: \", len(df[df['text'].notna()]), \"# None: \", len(df[df['text'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['embedding'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# embedding not None: \", len(df[df['embedding'].notna()]), \"# None: \", len(df[df['embedding'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings based on text column for new rows\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter\")\n",
    "model = AutoModel.from_pretrained(\"allenai/specter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get SPECTER embedding\n",
    "def get_specter_embedding(text):\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs)[0].mean(dim=1).squeeze()\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    embedding_np = embedding.numpy()\n",
    "\n",
    "    return str(embedding_np.tolist())\n",
    "\n",
    "# Get indices where 'embedding' is None\n",
    "embedding_isna_indices = df[df['embedding'].isna()].index\n",
    "max_index = embedding_isna_indices.max()\n",
    "\n",
    "# Compute SPECTER embeddings for these rows and store in 'embedding' column\n",
    "for i, idx in enumerate(embedding_isna_indices):\n",
    "    print(f\"idx {idx} / {max_index}\")\n",
    "    df.loc[idx, 'embedding'] = get_specter_embedding(df.loc[idx, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# embedding not None: \", len(df[df['embedding'].notna()]), \"# None: \", len(df[df['embedding'].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Pre-processing - 2] Generating a T-SNE x, y coordinates from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# x not None: \", len(df[df['x'].notna()]), \"# None: \", len(df[df['x'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# Create a temporary DataFrame with only rows where 'embedding' is not NaN\n",
    "df_temp = df[df['embedding'].notna()]\n",
    "\n",
    "# Convert string of list to numpy array\n",
    "# TODO: this is probably a very slow step to convert strings to lists\n",
    "print(\"converting all embeddings to lists\")\n",
    "df_temp['embedding'] = df_temp['embedding'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "# Stack all embeddings into a numpy array\n",
    "embeddings = np.vstack(df_temp['embedding'])\n",
    "\n",
    "# Compute t-SNE\n",
    "print(\"computing tsne\")\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Update 'x' and 'y' in the original DataFrame for rows that have embeddings\n",
    "print(\"updating df\")\n",
    "df.loc[df_temp.index, 'x'] = embeddings_2d[:, 0]\n",
    "df.loc[df_temp.index, 'y'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Create a scatter plot of all the points with node sizes based on normalized citationCount\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(df['x'], df['y'], alpha=0.5, label='All papers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get today's date\n",
    "now = datetime.now()\n",
    "date_str = now.strftime('%y-%m-%d')\n",
    "time_str = now.strftime('%H-%M-%S')\n",
    "folder_path = f'papers/{date_str}'\n",
    "n = len(papers.keys())\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "df.to_json(f'{folder_path}/{time_str}_tsne_output_{df.shape[0]}.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# x not None: \", len(df[df['x'].notna()]), \"# None: \", len(df[df['x'].isna()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Skip for now) Generating edges from embedding similarity & direct citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load the data from your JSON file\n",
    "with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\data_collection\\openalex\\extracted_results_with_embeddings.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df['paperId'] = df['id']\n",
    "df = df[df['embedding'].apply(lambda x: len(x) != 0)] # drop empty embeddings\n",
    "\n",
    "# print out the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding distances\n",
    "\n",
    "# Extract 'vector' from 'embedding' and convert to DataFrame\n",
    "embedding_df = pd.DataFrame(df['embedding'].tolist())\n",
    "embedding_df = embedding_df.dropna()\n",
    "\n",
    "# Compute the cosine distances\n",
    "distances = cosine_distances(embedding_df)\n",
    "\n",
    "# Initialize a MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "\n",
    "# Reshape distances to a 1D array and scale\n",
    "distances_scaled = scaler.fit_transform(distances.reshape(-1, 1))\n",
    "\n",
    "# Reshape back to original shape\n",
    "distances_scaled = distances_scaled.reshape(distances.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the edges data\n",
    "source = []\n",
    "target = []\n",
    "weights = []\n",
    "\n",
    "# Iterate over the distances matrix\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(i+1, distances.shape[1]):  # j starts from i+1 to avoid duplicate edges and self-edges\n",
    "        source.append(df.iloc[i]['paperId'])\n",
    "        target.append(df.iloc[j]['paperId'])\n",
    "        weights.append(distances_scaled[i, j])\n",
    "\n",
    "# Create the edges DataFrame\n",
    "edges_df = pd.DataFrame({'id': range(len(source)), 'weight': weights, 'source': source, 'target': target})\n",
    "\n",
    "# 14m for 2000 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a JSON string\n",
    "edges_json = edges_df.to_json(orient='records', indent=4)\n",
    "\n",
    "# Dump in JSON\n",
    "with open('edges/edges_2000.json', 'w') as f:\n",
    "    f.write(edges_json)\n",
    "\n",
    "# Convert the JSON string to a dictionary\n",
    "edges_dict = json.loads(edges_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for creating edges based on direct citations\n",
    "df[\"citations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LLMs for topic-based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 3]\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load in latest papers file\n",
    "papers_json_path = r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\knowledge_base\\papers\\23-07-25_11935_tsne_output.json'\n",
    "\n",
    "# load the data from your JSON file\n",
    "with open(papers_json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print out the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 2] Using GPT3.5 to generate topics\n",
    "# Improvement: add rate limiting error handling so you don't hardcode the wait time\n",
    "\n",
    "# imports\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "import openai\n",
    "# openai.api_key = os.getenv(\"OPENAI_GPT4_API_KEY\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-003\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "# GPT_MODEL = \"gpt-4\"\n",
    "\n",
    "# for bulk openai message, no stream\n",
    "def chat_openai(prompt=\"Tell me to ask you a prompt\", model=GPT_MODEL, chat_history=[]):\n",
    "    # define message conversation for model\n",
    "    if chat_history:\n",
    "        messages = chat_history\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an educated carbon capture research consultant and a generally educated and helpful researcher and programmer. Answer as correctly, clearly, and concisely as possible.\"},\n",
    "        ]\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # create the chat completion\n",
    "    print(\"Prompt: \", prompt)\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"Completion info: \", completion)\n",
    "    text_answer = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # updated conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": text_answer})\n",
    "\n",
    "    return text_answer, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Seeding – Ask GPT4 to be a domain expert and use “expert knowledge” to seed an initial taxonomy definition of main classes, subclasses, and divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_initial_taxonomy_prompt = '''\n",
    "Create a taxonomy of all carbon capture research areas. Be as mutually exclusive, completely exhaustive (MECE) and concise as possible. Be sure to also include a \"General\" category for information like literature reviews and updates, a \"Miscellaneous\" category for concepts that have yet to be covered by an appropriate category and non-carbon capture related concepts, and use multilevel numbering. Create as many levels breadth-wise and depth-wise as appropriate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat_openai(seed_initial_taxonomy_prompt)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_taxonomy = res[0]\n",
    "print(initial_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonify_taxonomy_prompt = f'''\n",
    "# {initial_taxonomy}\n",
    "\n",
    "# Put the above hierarchy of categories and sub-categories into a JSON format of 1. unique id (integer), 2. name of the category, 3. layer in the hierarchy (integer), and 4. content (a list of subcategories) \n",
    "\n",
    "# The output should be of this format: [\n",
    "#     {{\n",
    "#         \"cluster_id\": 0,\n",
    "#         \"name\": \"General\"\n",
    "#         \"layer\": 0,\n",
    "#         \"content\": [...],\n",
    "#     }}\n",
    "# ]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Pre-processing - 3] 2. Initial categorization – Use GPT3.5 to look at 1) the seeded taxonomy and 2) paper titles and abstracts, 3) categorize the paper title and abstract and adapt the seeded taxonomy as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing step 1 and df should've already updated the text column and there should already be a classification_ids column\n",
    "# # [GPT Topic Labeling - 3] Create a new text column\n",
    "# df['text'] = df.apply(lambda row: 'Title: ' + row['title'] + '.' if pd.isna(row['abstract']) else 'Title: ' + row['title'] + '. Abstract: ' + row['abstract'], axis=1)\n",
    "\n",
    "# # [GPT Topic Labeling - 4]\n",
    "df['classification_ids'] = pd.Series(dtype='object')\n",
    "# # df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 4.5, see everything for debugging]\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_taxonomy_papers():\n",
    "    # load the df\n",
    "    with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\knowledge_base\\papers\\latest_papers.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\knowledge_base\\clusters\\latest_taxonomy.txt', 'r') as f:\n",
    "        numbered_taxonomy = f.read()\n",
    "\n",
    "    return numbered_taxonomy, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def save_taxonomy_papers_note(numbered_taxonomy, df, note):\n",
    "    print(\"Saving taxonomy and df to papers with note\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%y-%m-%d')\n",
    "    time_str = now.strftime('%H-%M-%S')\n",
    "    if not os.path.exists(f'clusters/{date_str}'):\n",
    "        os.makedirs(f'clusters/{date_str}')\n",
    "    if not os.path.exists(f'papers/{date_str}'):\n",
    "        os.makedirs(f'papers/{date_str}')\n",
    "\n",
    "    # save the taxonomy and df to a txt and csv file\n",
    "    with open(f'clusters/{date_str}/{time_str}_{df.shape[0]}_{note}.txt', 'w') as f:\n",
    "        f.write(numbered_taxonomy)\n",
    "    df.to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_{note}.json', orient='records')\n",
    "    df[['title', 'classification_ids']].to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_{note}_manual_inspection.json', orient='records', indent=2)\n",
    "\n",
    "    # save to main\n",
    "    with open(f'clusters/latest_taxonomy.txt', 'w') as f:\n",
    "        f.write(numbered_taxonomy)\n",
    "    df.to_json(f'papers/latest_papers.json', orient='records')\n",
    "\n",
    "    print(\"Finished saving taxonomy and df to papers with note\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_update_taxonomy_extract_keywords_prompt(taxonomy, papers):\n",
    "    update_taxonomy_extract_keywords_prompt = f'''\n",
    "Task:\n",
    "The task is to 1) update the taxonomy of all carbon capture research by re-arranging or adding new categories and levels as appropriate and 2) correctly extract the most relevant and important paper text keywords to use them to classify each paper below into its top 3 matching categories in the updated taxonomy. \n",
    "\n",
    "Rules and Instructions:\n",
    "1. For the taxonomy, be as mutually exclusive, completely exhaustive (MECE) and concise as possible. Try to avoid repetition and overlap. \n",
    "2. Ensure that the taxonomy is readable and not overwhelming. Try to model the taxonomy to reflect the usabilty and usefulness of great classification systems like Dewey Decimal System and Library of Congress Classification.\n",
    "3. Use a hierarchical structure to manage the breadth and depth of the categories effectively, with the broadest categories at top and these categories becoming more specific as you go down the hierarchy. A general rule of thumb is to have around 10 top-level categories and 10 sub-categories for every parent category.\n",
    "4. For paper keyword extraction and classification, be as accurate and grounded in the extracted paper text keywords as possible. \n",
    "\n",
    "Papers (id : text): \n",
    "{papers}\n",
    "\n",
    "Input Taxonomy (category id : category name):\n",
    "{taxonomy}\n",
    "\n",
    "The output should be in the format of: \n",
    "1. \"UPDATED TAXONOMY: \" -- a readable and updated MECE multilevel taxonomy with any re-arrangement of categories or new categories and levels added.\n",
    "\n",
    "2. \"PAPER CLASSIFICATION: \n",
    "    [\n",
    "        paper id : [[paper text keywords, corresponding category id], [paper text keywords, corresponding category id], etc.], \n",
    "        paper id : [[paper text keywords, corresponding category id], etc.], \n",
    "        etc.\n",
    "    ]\" \n",
    "    -- a JSON of each paper id with a relevance ranked list of paper text keywords and corresponding category id, with everything being strings. Rank by most to least relevant category to so that anyone looking for all papers about a category can find the most relevant papers to the category.\n",
    "'''\n",
    "    return update_taxonomy_extract_keywords_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_taxonomy_mapping_prompt(old_taxonomy, new_taxonomy):\n",
    "    retrieve_taxonomy_mapping_prompt = f'''\n",
    "Task:\n",
    "The task is to match each Input Taxonomy category id to its closest Updated Taxonomy category id based on category names.\n",
    "\n",
    "Rules:\n",
    "1. Be as clear and correct as possible.\n",
    "\n",
    "Input Taxonomy (id : name):\n",
    "{old_taxonomy}\n",
    "\n",
    "Updated Taxonomy (id : name):\n",
    "{new_taxonomy}\n",
    "\n",
    "The output should be in the following JSON format: \n",
    "\"UPDATED CATEGORY IDS: [\n",
    "{{ \"Input Taxonomy category id\" : \"Updated Taxonomy category id\"}},\n",
    "{{ \"Input Taxonomy category id\" : \"Updated Taxonomy category id\"}},\n",
    "etc.]\" \n",
    "-- List every category id in Input Taxonomy and its closest category id in Updated Taxonomy based on category name. Use double quotes around each id.\n",
    "'''\n",
    "    return retrieve_taxonomy_mapping_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying token-optimizing keyword classification instead of paper\n",
    "def retrieve_classify_keywords_prompt(taxonomy, keywords):\n",
    "    classify_keywords_prompt = f'''\n",
    "Task:\n",
    "The task is to 1) update the taxonomy of all carbon capture research by re-arranging or adding new categories and levels as appropriate and 2) use the list of paper keywords to classify each paper id and keywords below into its keywords matching category ids in the updated taxonomy. \n",
    "\n",
    "Rules and Instructions:\n",
    "1. For the taxonomy, be as mutually exclusive, completely exhaustive (MECE) and concise as possible. Try to avoid repetition and overlap. \n",
    "2. Ensure that the taxonomy is readable and not overwhelming. Try to model the taxonomy to reflect the usability and usefulness of great classification systems like Dewey Decimal System and Library of Congress Classification.\n",
    "3. Use a hierarchical structure to manage the breadth and depth of the categories effectively, with the broadest categories at top and these categories becoming more specific as you go down the hierarchy. A general rule of thumb is to have around 10 top-level categories and 10 sub-categories for every parent category. Feel free to use as many depth levels as appropriate.\n",
    "4. For paper keyword classification, be as accurate and grounded in the paper keywords as possible. \n",
    "\n",
    "Papers (id : keywords): \n",
    "{keywords}\n",
    "\n",
    "Input Taxonomy (category id : category name):\n",
    "{taxonomy}\n",
    "\n",
    "The output should be in the format of: \n",
    "1. \"UPDATED TAXONOMY: \" -- a readable and updated MECE multilevel taxonomy with any re-arrangement of categories or new categories and levels added.\n",
    "\n",
    "2. \"PAPER CLASSIFICATION: \n",
    "[\n",
    "    paper id : [[paper keywords, corresponding category id], [paper keywords, corresponding category id], etc.], \n",
    "    paper id : [[paper keywords, corresponding category id], etc.], \n",
    "    etc.\n",
    "]\" \n",
    "-- a JSON of each paper id with a list of its paper keywords and corresponding Updated Taxonomy category id, with everything being strings.\n",
    "\n",
    "List formatting example (content and assignments are arbitrary):\n",
    "Input: 80 : ['carbon capture', 'biology', 'sand']\n",
    "Output: 80 : [['carbon capture, 8], ['biology', 10], ['sand', 9.3]]\n",
    "'''\n",
    "\n",
    "    return classify_keywords_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 6] Putting everything together to iterate through all papers, update and save taxonomy, and add category ids to each paper\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import ast  # The Abstract Syntax Trees module\n",
    "\n",
    "def extract_valid_json_string(json_str):\n",
    "    print(\"EXTRACTING VALID JSONS FROM \", json_str)\n",
    "    closing_brace_indices = [i for i, char in enumerate(json_str) if char == \"}\"]\n",
    "    for index in reversed(closing_brace_indices):\n",
    "        test_str = json_str[:index+1] + \"]\"\n",
    "        try:\n",
    "            json.loads(test_str)\n",
    "            return test_str\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def extract_taxonomy_and_classification(chat_output):\n",
    "    # print(\"THIS IS CHAT OUTPUT IN EXTRACT TAXONOMY AND CLASSIFICATION: \", chat_output)\n",
    "\n",
    "    # Extracting taxonomy\n",
    "    taxonomy_start = chat_output.find('UPDATED TAXONOMY:') + len('UPDATED TAXONOMY:')\n",
    "    taxonomy_end = chat_output.find('PAPER CLASSIFICATION:')\n",
    "    updated_taxonomy = chat_output[taxonomy_start:taxonomy_end].strip()\n",
    "\n",
    "    # Extracting paper classifications\n",
    "    end_index = chat_output.rfind(']')\n",
    "    classification_str = chat_output[taxonomy_end+len('PAPER CLASSIFICATION:'):end_index+1].strip()\n",
    "\n",
    "    # Iterate through each line until one ending with ']]' is found\n",
    "    valid_classification_str = \"\"\n",
    "    classification_dict = {}\n",
    "    for line in classification_str.splitlines():\n",
    "        print(\"Line: \", line)\n",
    "        if line.strip().endswith(']],'):\n",
    "            key, value = line.split(\":\", 1)\n",
    "            end_line_index = value.rfind(',')\n",
    "            classification_dict[key.strip().strip('\"')] = value[:end_line_index].strip()\n",
    "        elif line.strip().endswith(']]'):\n",
    "            key, value = line.split(\":\", 1)\n",
    "            end_line_index = value.rfind(']')\n",
    "            classification_dict[key.strip().strip('\"')] = value[:end_line_index+1].strip()\n",
    "    \n",
    "    print(\"classification_dict\", classification_dict)\n",
    "    return updated_taxonomy, classification_dict\n",
    "\n",
    "def extract_taxonomy_mapping(chat_output):\n",
    "    print(\"THIS IS CHAT OUTPUT EXTRACT TAXONOMY MAPPING: \", chat_output)\n",
    "\n",
    "    # Extracting changed category IDs\n",
    "    changed_category_start = chat_output.find('[')\n",
    "    changed_category_end = chat_output.rfind(']')\n",
    "    print(\"changed_category_start\", changed_category_start, \"changed_category_end\", changed_category_end)\n",
    "    changed_category_ids_str = chat_output[changed_category_start:changed_category_end+1].strip()\n",
    "    print(\"changed_category_ids_str\", changed_category_ids_str)\n",
    "\n",
    "    if changed_category_ids_str and (changed_category_ids_str[0] == '[' and changed_category_ids_str[-1] == ']'):\n",
    "        changed_category_ids = json.loads(changed_category_ids_str)\n",
    "        changed_category_ids_dict = {list(d.keys())[0]: list(d.values())[0] for d in changed_category_ids}\n",
    "    else:\n",
    "        changed_category_ids_dict = {}\n",
    "\n",
    "    print(\"\\nchanged changed_category_ids_dict: \", changed_category_ids_dict)\n",
    "    return changed_category_ids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging the extract taxonomy and classification for reordering\n",
    "\n",
    "chat_output = '''\n",
    "UPDATED TAXONOMY:\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "extract_taxonomy_and_classification(chat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 7] read and update BOTH df and numbered_taxonomy from last checkpoint or initial taxonomy\n",
    "import pandas as pd\n",
    "# df = pd.read_csv('checkpoints/gpt4_papers_100.csv')\n",
    "with open('checkpoints/gpt4_2000/taxonomy_0_64.txt', 'r') as f:\n",
    "    numbered_taxonomy = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "def update_classification_ids(classification_ids, changed_category_ids):\n",
    "    print(\"classification_ids\", classification_ids)\n",
    "\n",
    "    # Parse string into actual list if necessary\n",
    "    if isinstance(classification_ids, str):\n",
    "        classification_ids = ast.literal_eval(classification_ids)\n",
    "\n",
    "    # Check if the classification id exists in changed_category_ids. If it does, replace it\n",
    "    # If classification_ids is NaN, skip over it\n",
    "    if (classification_ids is np.nan) or (not classification_ids):\n",
    "        return classification_ids\n",
    "    \n",
    "    res = []\n",
    "    for item in classification_ids:\n",
    "        if len(item) > 1:\n",
    "            if item[1] in changed_category_ids:\n",
    "                res.append([item[0], changed_category_ids[item[1]]])\n",
    "            else:\n",
    "                res.append(item)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# classification not None: \", len(df[df['classification_ids'].notna()]), \"# None: \", len(df[df['classification_ids'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 8]\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def process_papers(df, numbered_taxonomy):\n",
    "    # Typically 16000 is good for 8K max tokens\n",
    "    TOTAL_PROMPT_TOKENS = 2500\n",
    "    CHARS_PER_TEXT = 250\n",
    "    NUM_BATCHES = TOTAL_PROMPT_TOKENS / CHARS_PER_TEXT # should be more than enough\n",
    "\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%y-%m-%d')\n",
    "    time_str = now.strftime('%H-%M-%S')\n",
    "    if not os.path.exists(f'clusters/{date_str}'):\n",
    "        os.makedirs(f'clusters/{date_str}')\n",
    "    if not os.path.exists(f'papers/{date_str}'):\n",
    "        os.makedirs(f'papers/{date_str}')\n",
    "\n",
    "    for i in range(0, int(NUM_BATCHES)):\n",
    "        print(f\"--- ITERATION {i} ---\")\n",
    "        subset = df.loc[df['classification_ids'].isna(), 'paperId':'text']\n",
    "        min_idx = subset.index.min()\n",
    "        if subset.empty:\n",
    "            print(\"subset was all classified!\")\n",
    "            return\n",
    "        print(\"Checking rows starting from\", subset.index.min(), \" num paper tokens to use: \", TOTAL_PROMPT_TOKENS - len(numbered_taxonomy))\n",
    "        print(\"df\", df['classification_ids'][min_idx-100:min_idx + 100], \"numbered_taxonomy\", numbered_taxonomy)\n",
    "        \n",
    "        # Create dictionary mapping index to paperId and add as many papers up to TOTAL_PROMPT_TOKENS\n",
    "        index_to_paperId = {i: row['paperId'] for i, (_, row) in enumerate(subset.iterrows())}\n",
    "        papers = {}\n",
    "        total_length = 0\n",
    "        for i, (_, row) in enumerate(subset.iterrows()):\n",
    "            text = row['text'][:CHARS_PER_TEXT]\n",
    "            if total_length + len(text) > TOTAL_PROMPT_TOKENS - len(numbered_taxonomy):\n",
    "                break \n",
    "            papers[i] = text\n",
    "            total_length += len(text)\n",
    "        papers_processed = \"\"\n",
    "        for index in papers.keys():\n",
    "            papers_processed += f\"{index} : {papers[index]}\\n\"\n",
    "\n",
    "        # Call OpenAI API to update taxonomy and classify papers\n",
    "        update_taxonomy_classify_papers_prompt = retrieve_update_taxonomy_extract_keywords_prompt(numbered_taxonomy, papers_processed)\n",
    "        res = chat_openai(update_taxonomy_classify_papers_prompt)\n",
    "        updated_taxonomy, paper_classification = extract_taxonomy_and_classification(res[0])\n",
    "        print(\"updated taxonomy: \", updated_taxonomy, \"paper classification: \", paper_classification)\n",
    "\n",
    "        # Ensure that you update all previously classified papers' classification ids with the new taxonomy\n",
    "        taxonomy_mapping_prompt = retrieve_taxonomy_mapping_prompt(numbered_taxonomy, updated_taxonomy)\n",
    "        res = chat_openai(taxonomy_mapping_prompt)  # call to OpenAI API\n",
    "        print(\"Map taxonomies result: \", res[0])\n",
    "        changed_category_ids = extract_taxonomy_mapping(res[0])\n",
    "        print(\"changed category ids: \", changed_category_ids)\n",
    "    \n",
    "        # update classification_ids from paper_classification using index_to_paperId\n",
    "        for idx, class_ids in paper_classification.items():\n",
    "            paper_id = index_to_paperId[int(idx)]\n",
    "            df.loc[df['paperId'] == paper_id, 'classification_ids'] = df.loc[df['paperId'] == paper_id, 'classification_ids'].apply(lambda x: class_ids)\n",
    "            \n",
    "        # check and update for any changed paper classification ids because of updated taxonomy\n",
    "        df['classification_ids'] = df['classification_ids'].apply(update_classification_ids, args=(changed_category_ids,))\n",
    "\n",
    "        # save the taxonomy and df to a txt and csv file\n",
    "        n = len(papers.keys())\n",
    "\n",
    "        with open(f'clusters/{date_str}/{time_str}_{df.shape[0]}_{min_idx}_{n}.txt', 'w') as f:\n",
    "            f.write(updated_taxonomy)\n",
    "        df.to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_{min_idx}_{n}.json', orient='records')\n",
    "        df[['title', 'classification_ids']].to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_{min_idx}_{n}_manual_analysis.json', orient='records', indent=2)\n",
    "        \n",
    "        numbered_taxonomy = updated_taxonomy\n",
    "\n",
    "    return df, numbered_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 9.5] read and update BOTH df and numbered_taxonomy from last checkpoint or initial taxonomy\n",
    "import pandas as pd\n",
    "\n",
    "# load the df\n",
    "with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\knowledge_base\\papers\\latest_papers.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\knowledge_base\\clusters\\latest_taxonomy.txt', 'r') as f:\n",
    "    numbered_taxonomy = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classification_ids'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 9]\n",
    "df, numbered_taxonomy = process_papers(df, numbered_taxonomy)\n",
    "\n",
    "# 159 min for 2000 papers, batch size of 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['classification_ids'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: keyword classification\n",
    "# [GPT Topic Labeling - 8]\n",
    "import time\n",
    "import ast\n",
    "\n",
    "def process_keywords(df, numbered_taxonomy):\n",
    "    try:\n",
    "        # Typically 16000 is good for 8K max tokens\n",
    "        TOTAL_PROMPT_TOKENS = 5000\n",
    "        CHARS_PER_TEXT = 250\n",
    "        NUM_BATCHES = TOTAL_PROMPT_TOKENS / CHARS_PER_TEXT # should be more than enough\n",
    "\n",
    "        now = datetime.now()\n",
    "        date_str = now.strftime('%y-%m-%d')\n",
    "        time_str = now.strftime('%H-%M-%S')\n",
    "        if not os.path.exists(f'clusters/{date_str}'):\n",
    "            os.makedirs(f'clusters/{date_str}')\n",
    "        if not os.path.exists(f'papers/{date_str}'):\n",
    "            os.makedirs(f'papers/{date_str}')\n",
    "\n",
    "        for i in range(0, int(NUM_BATCHES)):\n",
    "            print(f\"--- ITERATION {i} ---\")\n",
    "            # only select rows that don't have keyword classification ids yet but have classification_ids (keywords have been extracted)\n",
    "            subset_cols = df[['paperId', 'classification_ids']]\n",
    "            subset = subset_cols[subset_cols['classification_ids'].apply(lambda x: type(x) == list)]\n",
    "            min_idx = subset.index.min()\n",
    "            if subset.empty:\n",
    "                print(\"subset was all classified!\")\n",
    "                return\n",
    "            \n",
    "            print(\"Checking rows starting from\", subset.index.min(), \" num paper tokens to use: \", TOTAL_PROMPT_TOKENS - len(numbered_taxonomy))\n",
    "            print(\"df\", df['classification_ids'][min_idx-50:min_idx + 50], \"numbered_taxonomy\", numbered_taxonomy)\n",
    "            \n",
    "            # Create dictionary mapping index to paperId and add as many paper keywords up to TOTAL_PROMPT_TOKENS\n",
    "            index_to_paperId = {i: row['paperId'] for i, (_, row) in enumerate(subset.iterrows())}\n",
    "            papers = {}\n",
    "            total_length = 0\n",
    "            for i, (_, row) in enumerate(subset.iterrows()):\n",
    "                classification_ids = row['classification_ids']\n",
    "                keywords = str([item[0] for item in classification_ids])\n",
    "                # 10K for GPT3.5, 15K for GPT4\n",
    "                if total_length + len(keywords) > TOTAL_PROMPT_TOKENS - len(numbered_taxonomy): \n",
    "                    break \n",
    "                papers[i] = keywords\n",
    "                total_length += len(keywords)\n",
    "            papers_processed = \"\"\n",
    "            for index in papers.keys():\n",
    "                papers_processed += f\"{index} : {papers[index]}\\n\"\n",
    "\n",
    "            # Call OpenAI API to update taxonomy and classify papers\n",
    "            update_taxonomy_prompt = retrieve_classify_keywords_prompt(numbered_taxonomy, papers_processed)\n",
    "            res = chat_openai(update_taxonomy_prompt)\n",
    "            updated_taxonomy, paper_classification = extract_taxonomy_and_classification(res[0])\n",
    "            print(\"updated taxonomy: \", updated_taxonomy)\n",
    "            print(\"paper classification: \", paper_classification)\n",
    "\n",
    "            # Ensure that you update all previously classified papers' classification ids with the new taxonomy\n",
    "            taxonomy_mapping_prompt = retrieve_taxonomy_mapping_prompt(numbered_taxonomy, updated_taxonomy)\n",
    "            res = chat_openai(taxonomy_mapping_prompt)\n",
    "            print(\"Map taxonomies result: \", res[0])\n",
    "            changed_category_ids = extract_taxonomy_mapping(res[0])\n",
    "            print(\"changed category ids: \", changed_category_ids)\n",
    "\n",
    "            # update keyword_classification_ids using index_to_paperId\n",
    "            for idx, class_ids in paper_classification.items():\n",
    "                paper_id = index_to_paperId[int(idx)]  # map index back to paperId\n",
    "                df.loc[df['paperId'] == paper_id, 'classification_ids'] = df.loc[df['paperId'] == paper_id, 'classification_ids'].apply(lambda x: class_ids)\n",
    "                \n",
    "            # check and update for any changed paper classification ids because of updated taxonomy\n",
    "            df['classification_ids'] = df['classification_ids'].apply(update_classification_ids, args=(changed_category_ids,))\n",
    "\n",
    "            # save the taxonomy and df to a txt and csv file\n",
    "            n = len(papers.keys())\n",
    "\n",
    "            with open(f'clusters/{date_str}/{time_str}_{df.shape[0]}_{min_idx}_{n}_reclassify_keywords.txt', 'w') as f:\n",
    "                f.write(updated_taxonomy)\n",
    "            df.to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_{min_idx}_{n}_reclassify_keywords.json', orient='records')\n",
    "            df[['title', 'classification_ids']].to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_{min_idx}_{n}_manual_analysis_reclassify_keywords.json', orient='records', indent=2)\n",
    "            \n",
    "            numbered_taxonomy = updated_taxonomy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "\n",
    "    return df, numbered_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: keyword classification\n",
    "process_keywords(df, numbered_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df['classification_ids'].notnull(), 'classification_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize taxonomy - 1\n",
    "def retrieve_organize_taxonomy(taxonomy):\n",
    "    organize_taxonomy_prompt = f'''\n",
    "Initial Taxonomy (id : name)\n",
    "{taxonomy}\n",
    "\n",
    "Task:\n",
    "There are already papers classified under each category, but the taxonomy is potentially all over the place. Imagine that the taxonomy is going to be transformed into a map, and that the top level categories would represent a zoomed out view and the lower level categories would appear as a user zooms in.\n",
    "\n",
    "You are trying to create a useful taxonomy for carbon capture researchers. Re-arrange the categories and their levels so that the more relevant categories are  at the top level, with non-relevant categories categorized as lower levels under Miscellaneous. Feel free to use as many depth levels as necessary. Do not change category names to make them more or less relevant.\n",
    "'''\n",
    "    return organize_taxonomy_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize taxonomy - 2\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def reorganize_taxonomy(df, numbered_taxonomy):\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%y-%m-%d')\n",
    "    time_str = now.strftime('%H-%M-%S')\n",
    "    if not os.path.exists(f'clusters/{date_str}'):\n",
    "        os.makedirs(f'clusters/{date_str}')\n",
    "    if not os.path.exists(f'papers/{date_str}'):\n",
    "        os.makedirs(f'papers/{date_str}')\n",
    "\n",
    "    try:\n",
    "        update_taxonomy_prompt = retrieve_organize_taxonomy(numbered_taxonomy)\n",
    "        print(\"update_taxonomy_prompt\", update_taxonomy_prompt)\n",
    "\n",
    "        res = chat_openai(update_taxonomy_prompt)  # call to OpenAI API\n",
    "        print(\"Reorganized taxonomy result: \", res[0])\n",
    "        \n",
    "        # parse the res[0]\n",
    "        updated_taxonomy = \"\"\n",
    "        for line in res[0].splitlines():\n",
    "            if len(line.strip()) > 2 and line.strip()[1] == \".\":\n",
    "                updated_taxonomy += line.strip() + \"\\n\"\n",
    "        print(\"updated taxonomy: \", updated_taxonomy)\n",
    "\n",
    "        # Ensure that you update all previously classified papers' classification ids with the new taxonomy\n",
    "        print(\"MAPPING TAXONOMIES\")\n",
    "        taxonomy_mapping_prompt = retrieve_taxonomy_mapping_prompt(numbered_taxonomy, updated_taxonomy)\n",
    "        res = chat_openai(taxonomy_mapping_prompt)  # call to OpenAI API\n",
    "        \n",
    "        print(\"Map taxonomies result: \", res[0])\n",
    "        changed_category_ids = extract_taxonomy_mapping(res[0])\n",
    "        print(\"changed category ids: \", changed_category_ids)\n",
    "            \n",
    "        # check and update for any changed paper classification ids\n",
    "        df['keyword_classification_ids'] = df['keyword_classification_ids'].apply(update_classification_ids, args=(changed_category_ids,))\n",
    "\n",
    "        # save the taxonomy and df to a txt and csv file\n",
    "        with open(f'clusters/{date_str}/{time_str}_{df.shape[0]}_reorganize_taxonomy.txt', 'w') as f:\n",
    "            f.write(updated_taxonomy)\n",
    "        df.to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_reorganize_taxonomy.json', orient='records')\n",
    "        df[['title', 'classification_ids']].to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_reorganize_taxonomy_manual_inspection.json', orient='records', indent=2)\n",
    "\n",
    "        # save to main\n",
    "        with open(f'clusters/latest_taxonomy.txt', 'w') as f:\n",
    "            f.write(numbered_taxonomy)\n",
    "        df.to_json(f'papers/latest_papers.json', orient='records')\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "\n",
    "    return df, numbered_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, numbered_taxonomy = reorganize_taxonomy(df, numbered_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keyword classification confidence scores - 1\n",
    "import re\n",
    "\n",
    "# create class_id_to_name dictionary\n",
    "class_id_to_name = {}\n",
    "for line in numbered_taxonomy.split(\"\\n\"):\n",
    "    split_line = line.strip().split(maxsplit=1)\n",
    "    if len(split_line) == 2:\n",
    "        if split_line[0][-1] == \".\": \n",
    "            class_id_to_name[split_line[0][:-1]] = split_line[1]\n",
    "        else:\n",
    "            class_id_to_name[split_line[0]] = split_line[1]\n",
    "\n",
    "print(\"class_id_to_name:\", class_id_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keyword classification confidence scores - 2\n",
    "import ast\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter\")\n",
    "model = AutoModel.from_pretrained(\"allenai/specter\")\n",
    "\n",
    "def get_cosine_similarity(text1, text2):\n",
    "    # Tokenize texts\n",
    "    inputs1 = tokenizer(text1, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    inputs2 = tokenizer(text2, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding1 = model(**inputs1)[0].mean(dim=1).squeeze()\n",
    "        embedding2 = model(**inputs2)[0].mean(dim=1).squeeze()\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    score = 1 - cosine(embedding1.numpy(), embedding2.numpy())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keyword classification confidence scores - 3\n",
    "for i, (_, row) in enumerate(df.iterrows()):\n",
    "    if row['classification_ids'] and row['classification_ids'] != 'nan' and (type(row['classification_ids']) == list or type(row['classification_ids']) == str):\n",
    "        # Using ast.literal_eval to convert the string to a list of lists\n",
    "        if type(row['classification_ids']) != list:\n",
    "            classification_ids = ast.literal_eval(row['classification_ids'])\n",
    "        else:\n",
    "            classification_ids = row['classification_ids']\n",
    "\n",
    "        updated_classification_ids = []\n",
    "        for item in classification_ids:\n",
    "            if item and len(item) == 2:\n",
    "                keywords = item[0]\n",
    "                class_id = item[1]\n",
    "                if class_id in class_id_to_name.keys():\n",
    "                    classification = class_id_to_name[class_id]\n",
    "                    \n",
    "                    # Get cosine similarity score using HuggingFace Semantic Scholar Spectre API embeddings\n",
    "                    score = round(get_cosine_similarity(keywords, classification), 2)\n",
    "                    updated_classification_ids.append([keywords, item[1], str(score)])\n",
    "                else:\n",
    "                    print(\"ROW \", i, \" CLASS ID: \", class_id, \" WAS NOT FOUND IN CLASS_ID_TO_NAME\")\n",
    "            \n",
    "    df.loc[i, 'classification_ids'] = str(updated_classification_ids)\n",
    "\n",
    "now = datetime.now()\n",
    "date_str = now.strftime('%y-%m-%d')\n",
    "time_str = now.strftime('%H-%M-%S')\n",
    "if not os.path.exists(f'clusters/{date_str}'):\n",
    "    os.makedirs(f'clusters/{date_str}')\n",
    "if not os.path.exists(f'papers/{date_str}'):\n",
    "    os.makedirs(f'papers/{date_str}')\n",
    "\n",
    "# save the taxonomy and df to a txt and csv file\n",
    "with open(f'clusters/{date_str}/{time_str}_{df.shape[0]}_add_keyword_class_scores.txt', 'w') as f:\n",
    "    f.write(numbered_taxonomy)\n",
    "df.to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_add_keyword_class_scores.json', orient='records')\n",
    "df[['title', 'classification_ids']].to_json(f'papers/{date_str}/{time_str}_{df.shape[0]}_add_keyword_class_scores_manual_inspection.json', orient='records', indent=2)\n",
    "\n",
    "# save to main\n",
    "with open(f'clusters/latest_taxonomy.txt', 'w') as f:\n",
    "    f.write(numbered_taxonomy)\n",
    "df.to_json(f'papers/latest_papers.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_taxonomy_nested():\n",
    "    taxonomy_str, df = load_latest_taxonomy_papers()\n",
    "\n",
    "    # Create a dictionary where each key is a classification_id and\n",
    "    # each value is a list of dictionaries containing paper data.\n",
    "    class_children = defaultdict(list)\n",
    "\n",
    "    # Iterate over the DataFrame once\n",
    "    for i, row in df.iterrows():\n",
    "        # Skip rows with no classification_ids\n",
    "        if not row['classification_ids']:\n",
    "            continue\n",
    "\n",
    "        # Ensure classifications_ids is a list\n",
    "        classification_ids = row['classification_ids']\n",
    "        if type(classification_ids) == str:\n",
    "            classification_ids_list = ast.literal_eval(classification_ids)\n",
    "        else:\n",
    "            classification_ids_list = classification_ids\n",
    "\n",
    "        for keyword_idx, id_info in enumerate(classification_ids_list):\n",
    "            # Skip malformed id_info\n",
    "            if len(id_info) != 3:\n",
    "                continue\n",
    "\n",
    "            # Extract data from id_info\n",
    "            keywords = id_info[0]\n",
    "            paper_classification_id = id_info[1]\n",
    "            confidence_score = id_info[2]\n",
    "\n",
    "            # Add the paper data to the corresponding classification_id in class_children\n",
    "            paper_data = {\n",
    "                \"name\": str(row['paperId']) + \"-\" + str(keyword_idx), \n",
    "                \"value\": [{\n",
    "                    \"paperId\": row['paperId'] if pd.notna(row['paperId']) else None, \n",
    "                    \"title\": row['title'] if pd.notna(row['title']) else None, \n",
    "                    \"abstract\": row['abstract'] if pd.notna(row['abstract']) else None,\n",
    "                    \"authors\": [[item if pd.notna(item) else None for item in sublist] for sublist in row['authors']] if row['authors'] is not None else None,\n",
    "                    \"citationCount\": row['citationCount'] if pd.notna(row['citationCount']) else None,\n",
    "                    \"doi\": row['doi'] if pd.notna(row['doi']) else None,\n",
    "                    \"isOpenAccess\": row['isOpenAccess'] if pd.notna(row['isOpenAccess']) else None,\n",
    "                    \"language\": row['language'] if pd.notna(row['language']) else None,\n",
    "                    \"publicationDate\": row['publication_date'] if pd.notna(row['publication_date']) else None,\n",
    "                    \"relevance_score\": row[\"relevance_score\"] if pd.notna(row[\"relevance_score\"]) else None,\n",
    "                    \"url\": row[\"url\"] if pd.notna(row[\"url\"]) else None,\n",
    "                    \"year\": row[\"year\"] if pd.notna(row[\"year\"]) else None,\n",
    "                    \"tsne_x\": row[\"x\"] if pd.notna(row[\"x\"]) else None,\n",
    "                    \"tsne_y\": row[\"y\"] if pd.notna(row[\"y\"]) else None,\n",
    "                    \"keywords\": keywords if keywords else None, \n",
    "                    \"score\": confidence_score if confidence_score else None\n",
    "                }]\n",
    "            }\n",
    "            class_children[paper_classification_id].append(paper_data)\n",
    "\n",
    "    # Now you can use your existing code to generate the taxonomy tree,\n",
    "    # but replace the paper data generation part with a lookup in class_children.\n",
    "    # Here is a rough example:\n",
    "\n",
    "    print(\"class_children\", class_children.keys())\n",
    "\n",
    "    stack = []\n",
    "    taxonomy_json = []\n",
    "    lines = taxonomy_str.split('\\n')\n",
    "    id_counter = 0  # Keep track of unique id\n",
    "\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            category_id, category_name = line.strip().split(' ', 1)\n",
    "            category_id = category_id.rstrip('.')\n",
    "            layer = category_id.count('.')\n",
    "\n",
    "            category_obj = {\n",
    "                'id': id_counter,\n",
    "                'classification_id': category_id,\n",
    "                'name': category_name,\n",
    "                'layer': layer,\n",
    "                'children': class_children[category_id]\n",
    "            }\n",
    "            id_counter += 1\n",
    "\n",
    "            if not stack:\n",
    "                taxonomy_json.append(category_obj)\n",
    "            else:\n",
    "                while stack and stack[-1]['layer'] >= layer:\n",
    "                    stack.pop()\n",
    "                if not stack:\n",
    "                    taxonomy_json.append(category_obj)\n",
    "                else:\n",
    "                    stack[-1]['children'].append(category_obj)\n",
    "            stack.append(category_obj)\n",
    "\n",
    "    # Write the taxonomy JSON to a file\n",
    "    preprocessed_taxonomy_json = [{\n",
    "        \"name\": \"Carbon capture\",\n",
    "        \"children\": taxonomy_json\n",
    "    }]\n",
    "\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%y-%m-%d')\n",
    "    time_str = now.strftime('%H-%M-%S')\n",
    "\n",
    "    with open(f'clusters/{date_str}/{time_str}_taxonomy_gen_taxonomy_json.json', 'w') as f:\n",
    "        json.dump(preprocessed_taxonomy_json, f, indent=4)\n",
    "\n",
    "    # save to main\n",
    "    with open(f'clusters/latest_taxonomy.json', 'w') as f:\n",
    "        json.dump(preprocessed_taxonomy_json, f, indent=4)\n",
    "\n",
    "    # save checkpoint\n",
    "    save_taxonomy_papers_note(taxonomy_str, df, \"gen_taxonomy_json\")\n",
    "\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 11]\n",
    "generate_taxonomy_nested()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in parsed taxonomy from file\n",
    "import json\n",
    "\n",
    "# Open the file and load the JSON\n",
    "with open('clusters/latest_taxonomy.json', 'r') as f:\n",
    "    parsed_taxonomy = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On classification id:  1.1\n",
      "On classification id:  1.2\n",
      "On classification id:  1.3\n",
      "On classification id:  1\n",
      "On classification id:  2.1.1\n",
      "On classification id:  2.1.2\n",
      "On classification id:  2.1.3\n",
      "On classification id:  2.1.4\n",
      "On classification id:  2.1\n",
      "On classification id:  2.2.1\n",
      "On classification id:  2.2.2\n",
      "On classification id:  2.2.3\n",
      "On classification id:  2.2\n",
      "On classification id:  2.3.1\n",
      "On classification id:  2.3.2\n",
      "On classification id:  2.3\n",
      "On classification id:  2.4.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On classification id:  2.4.2\n",
      "On classification id:  2.4.3\n",
      "On classification id:  2.4\n",
      "On classification id:  2.5.1\n",
      "On classification id:  2.5.2\n",
      "On classification id:  2.5\n",
      "On classification id:  2.6.1\n",
      "On classification id:  2.6.2\n",
      "On classification id:  2.6\n",
      "On classification id:  2.7\n",
      "On classification id:  2.8\n",
      "On classification id:  2.9.1\n",
      "On classification id:  2.9.2\n",
      "On classification id:  2.9\n",
      "On classification id:  2\n",
      "On classification id:  3.1.1\n",
      "On classification id:  3.1.2\n",
      "On classification id:  3.1.3\n",
      "On classification id:  3.1\n",
      "On classification id:  3.2\n",
      "On classification id:  3.3\n",
      "On classification id:  3.4.1\n",
      "On classification id:  3.4.2\n",
      "On classification id:  3.4.3\n",
      "On classification id:  3.4\n",
      "On classification id:  3.5\n",
      "On classification id:  3\n",
      "On classification id:  4.1\n",
      "On classification id:  4.2\n",
      "On classification id:  4.3\n",
      "On classification id:  4.4\n",
      "On classification id:  4.5\n",
      "On classification id:  4\n",
      "On classification id:  5.1\n",
      "On classification id:  5.2\n",
      "On classification id:  5.3\n",
      "On classification id:  5.4\n",
      "On classification id:  5\n",
      "On classification id:  6.1\n",
      "On classification id:  6.2\n",
      "On classification id:  6.3\n",
      "On classification id:  6\n",
      "On classification id:  7.1\n",
      "On classification id:  7.2\n",
      "On classification id:  7.3\n",
      "On classification id:  7\n"
     ]
    }
   ],
   "source": [
    "# [GPT Topic Labeling - 11]\n",
    "# EDGES\n",
    "# Selecting edges within each classification id group: keeping the paperId's most similar paper in their cluster, and also constructing the MST for each cluster to ensure it's a connected graph\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# 1. Create a dictionary that maps each classification_id to a list of papers in that cluster.\n",
    "def traverse_clusters(cluster, classification_to_papers):\n",
    "    values = []\n",
    "\n",
    "    # Recursively traverse this cluster's child clusters\n",
    "    for child in cluster['children']:\n",
    "        if 'value' in child:\n",
    "            values.append([child['name'], child['value'][0]])\n",
    "        elif 'children' in child:\n",
    "            traverse_clusters(child, classification_to_papers)\n",
    "\n",
    "    # Add this cluster's classification_id and papers to the dictionary\n",
    "    if 'classification_id' in cluster:\n",
    "        classification_to_papers[cluster['classification_id']] = values\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "classification_to_papers = {}\n",
    "\n",
    "# Traverse each top-level category in the parsed taxonomy\n",
    "for category in parsed_taxonomy:\n",
    "    traverse_clusters(category, classification_to_papers)\n",
    "\n",
    "\n",
    "# 2. Iterate through this dictionary, and for each category, compute the cosine similarity between every pair of papers in that category.\n",
    "# 3. Store the results in a list of dictionaries, as you've described.\n",
    "\n",
    "# A dictionary mapping each paper_id to its list of highest weight edge\n",
    "max_edges = defaultdict(list)\n",
    "\n",
    "# A dictionary mapping each classification_id to its graph\n",
    "classification_to_graph = defaultdict(nx.Graph)\n",
    "\n",
    "for classification_id, paper_ids in classification_to_papers.items():\n",
    "    print(\"On classification id: \", classification_id)\n",
    "    paper_pairs = list(combinations(paper_ids, 2))\n",
    "\n",
    "    for paper_id1_obj, paper_id2_obj in paper_pairs:\n",
    "        # print(\"paper_id1\", paper_id1_obj[0], \"paper_id2\", paper_id2_obj[0])\n",
    "\n",
    "        # Only use paperIds\n",
    "        paper_id1, paper_id2 = paper_id1_obj[0], paper_id2_obj[0]\n",
    "\n",
    "        if (paper_id1_obj[1][\"tsne_x\"] == None or paper_id1_obj[1][\"tsne_y\"] == None or paper_id2_obj[1][\"tsne_x\"] == None or paper_id2_obj[1][\"tsne_y\"] == None):\n",
    "            # print(\"skipping because of None tsne_x or tsne_y\")\n",
    "            continue\n",
    "\n",
    "        x1, y1 = paper_id1_obj[1][\"tsne_x\"], paper_id1_obj[1][\"tsne_y\"]\n",
    "        x2, y2 = paper_id2_obj[1][\"tsne_x\"], paper_id2_obj[1][\"tsne_y\"]\n",
    "        \n",
    "        distance = euclidean([x1, y1], [x2, y2])\n",
    "        weight = 1 / (1 + distance)\n",
    "\n",
    "        # Add the edge to the graph for this classification_id\n",
    "        classification_to_graph[classification_id].add_edge(paper_id1, paper_id2, weight=weight)\n",
    "\n",
    "        # Create a new edge\n",
    "        new_edge = {\"source\": paper_id1, \"target\": paper_id2, \"weight\": weight}\n",
    "        \n",
    "        # Add the new edge to the list for paper_id1 if it's one of the top num_most_similar\n",
    "        num_most_similar = 1\n",
    "        if len(max_edges[paper_id1]) < num_most_similar or weight > min(edge['weight'] for edge in max_edges[paper_id1]):\n",
    "            if len(max_edges[paper_id1]) == num_most_similar:\n",
    "                # Remove the edge with the lowest weight\n",
    "                max_edges[paper_id1].remove(min(max_edges[paper_id1], key=lambda edge: edge['weight']))\n",
    "            max_edges[paper_id1].append(new_edge)\n",
    "        \n",
    "        # Do the same for paper_id2, but reverse the source and target\n",
    "        new_edge = {\"source\": paper_id2, \"target\": paper_id1, \"weight\": weight}\n",
    "        if len(max_edges[paper_id2]) < num_most_similar or weight > min(edge['weight'] for edge in max_edges[paper_id2]):\n",
    "            if len(max_edges[paper_id2]) == num_most_similar:\n",
    "                max_edges[paper_id2].remove(min(max_edges[paper_id2], key=lambda edge: edge['weight']))\n",
    "            max_edges[paper_id2].append(new_edge)\n",
    "\n",
    "# Combine the max weight edges for each node with the MSTs for each classification_id\n",
    "final_edges = []\n",
    "\n",
    "# Create a set to keep track of edges that have been added\n",
    "added_edges = set()\n",
    "\n",
    "# Flatten max_edges.values()\n",
    "flattened_max_edges = [edge for edges in max_edges.values() for edge in edges]\n",
    "\n",
    "# Add an index as the id for each edge in final_edges\n",
    "for idx, edge in enumerate(flattened_max_edges):\n",
    "    edge_tuple = (edge[\"source\"], edge[\"target\"])\n",
    "    if edge_tuple not in added_edges:\n",
    "        # Add the edge id first so it appears first\n",
    "        edge_with_id = {\"id\": idx}\n",
    "        edge_with_id.update(edge)\n",
    "        final_edges.append(edge_with_id)\n",
    "        added_edges.add(edge_tuple)\n",
    "\n",
    "# Now add the MST edges, continuing the ids from where they left off\n",
    "# for graph in classification_to_graph.values():\n",
    "#     mst_edges = nx.algorithms.tree.maximum_spanning_edges(graph, data=False)\n",
    "#     for source, target in mst_edges:\n",
    "#         edge_tuple = (source, target)\n",
    "#         if edge_tuple not in added_edges:\n",
    "#             weight = graph[source][target]['weight']\n",
    "#             final_edges.append({\n",
    "#                 \"id\": len(final_edges),  # The next id is the current length of final_edges\n",
    "#                 \"weight\": weight,\n",
    "#                 \"source\": source,\n",
    "#                 \"target\": target\n",
    "#             })\n",
    "#             added_edges.add(edge_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT Topic Labeling - 11.1]\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "now = datetime.now()\n",
    "date_str = now.strftime('%y-%m-%d')\n",
    "time_str = now.strftime('%H-%M-%S')\n",
    "if not os.path.exists(f'edges/{date_str}'):\n",
    "    os.makedirs(f'edges/{date_str}')\n",
    "\n",
    "with open(f'edges/{date_str}/{time_str}_edges_gen_edges.json', 'w') as f:\n",
    "    json.dump(final_edges, f, indent=4)\n",
    "with open('edges/latest_edges.json', 'w') as f:\n",
    "    json.dump(final_edges, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Iteration – it’s likely that the fields will get too big, so if there are sections with too many papers, have GPT3.5 break them down even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_themes = 'Renewable integration, carbon capture, Clean energy transition, Operational flexibility, Cost-effective CO2 reduction, carbon storage, lost decade'\n",
    "\n",
    "# paper = \"titled '\" + df.iloc[0][\"title\"] + \"'\"\n",
    "# if df.iloc[0][\"abstract\"]:\n",
    "#     paper += \"with the following abstract: \" + df.iloc[0][\"abstract\"]\n",
    "\n",
    "# prompt = f'''\n",
    "# Paper: {paper} \\n Task: Given the paper title and abstract above, determine at most 5 themes for a researcher whose goal is to eventually make impactful discoveries and experiments. \\n Rules: Do not output any theme that is beyond what is given in the paper. Be as concise (less than 5 words), clear, and correct as possible. Do not make up anything not apparent from the paper. \\n Use themes from other papers only if the paper mentions them: {existing_themes}. \n",
    "\n",
    "# Your output should be of the following format: Theme1, Theme2, Theme3, Theme4, Theme5\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2 following some guy's topic modeling article: https://medium.com/@stephensonebinezer/transform-your-topic-modeling-with-chatgpt-cutting-edge-nlp-f4654b4eac99\n",
    "\n",
    "existing_themes = set()\n",
    "\n",
    "df['title_abstract'] = df.apply(lambda row: \"Title: \" + row[\"title\"] + (\"; Abstract: \" + row[\"abstract\"] if pd.notnull(row[\"abstract\"]) else \"\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by='citationCount', ascending=False)\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for paper in df:\n",
    "prompt = f'''\n",
    "I am giving you the title and abstract (if provided) of a paper in the format [Title ; Abstract]. Give me at most 5 broader categories or themes like carbon capture, membranes, algae sinking, enhanced rock weathering, etc. in the format [Theme1, Theme2, Theme3, Theme4, Theme5] for the paper '{df.iloc[0]['title_abstract']}'. Be as concise, clear, and correct as possible. Do not make up anything not apparent in the paper. Use as minimal number of categories and themes as possible, and rank the most relevant and specific ones to the paper first'\n",
    "'''\n",
    "\n",
    "res = chat_openai(prompt)\n",
    "print(prompt + \" -- \" + df.iloc[0]['title_abstract'] + \" -- \" + res[0])\n",
    "\n",
    "# Tried to label topics in a way dependent on previous topics, but I'll just sample topics independently instead\n",
    "# if not existing_themes:\n",
    "#     existing_themes = res[0][1:-1]\n",
    "# else:\n",
    "#     existing_themes += \", \" + res[0][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to get up to 5 topics from GPT, or Unknown as a list for each paper\n",
    "import time\n",
    "\n",
    "def get_topics(title_abstract):\n",
    "    prompt = f'''\n",
    "    I am giving you the title and abstract (if provided) of a paper in the format [Title ; Abstract]. Give me at most 5 broader categories or themes like carbon capture, membranes, algae sinking, enhanced rock weathering, etc. in the format [Theme1, Theme2, Theme3, Theme4, Theme5] for the paper '{title_abstract}'. Be as concise, clear, and correct as possible. Do not make up anything not apparent in the paper.'\n",
    "    '''\n",
    "    \n",
    "    time.sleep(20)\n",
    "    res = chat_openai(prompt)\n",
    "    print(title_abstract + \" -- \" + res[0])\n",
    "\n",
    "    if (res[0][0] != '[' and res[0][-1] != ']'):\n",
    "        return \"[Unknown]\"\n",
    "\n",
    "    return res[0]\n",
    "\n",
    "# Add the GPT_topics column if it doesn't exist\n",
    "if 'GPT_topics' not in df.columns:\n",
    "    df['GPT_topics'] = None\n",
    "\n",
    "for i in df.index:\n",
    "    print(i)\n",
    "\n",
    "    # Check if topics already has a value or if it's not in the right format\n",
    "    if pd.isnull(df.at[i, 'GPT_topics']) or (df.at[i, 'GPT_topics'][0] != '[' or df.at[i, 'GPT_topics'][-1] != ']'):\n",
    "        df.at[i, 'GPT_topics'] = get_topics(df.at[i, 'title_abstract'])\n",
    "\n",
    "    # Save every 100 iterations\n",
    "    df.to_json('df_with_topics.json', orient='records', indent=4)\n",
    "\n",
    "# Save the final DataFrame\n",
    "df.to_json('df_with_topics.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pruned_trees already exists\n",
    "\n",
    "# Loading trees back in\n",
    "import json\n",
    "\n",
    "# Open the file in read mode\n",
    "with open('pruned_tree.json', 'r') as f:\n",
    "    pruned_trees = json.load(f)  # Load the JSON data from the file\n",
    "\n",
    "# Now, 'data' is a list of dictionaries that you can manipulate\n",
    "# print(trees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing main topic helper function\n",
    "def get_main_topic(topics_list):\n",
    "    # Converts the list of topics to a comma-separated string enclosed in brackets\n",
    "    topics_str = ', '.join(topics_list)\n",
    "\n",
    "    # hardcoded to get the prompt right\n",
    "    # topics_str = \"Carbon capture, Membrane design, Gas separation, Mechanical strength, Plant root mimicry, Carbon capture, Metal-organic frameworks, CO2 capture, Adsorption, Porous materials, Carbon capture, Mixed matrix membranes, Post-combustion carbon capture, Cost prediction, High-throughput computational, Carbon capture, Separation\"\n",
    "\n",
    "    # topics_str = \"Public opinion, Carbon capture and storage, Framing effects, Support, Energy transition, Policy scenarios, Bioenergy, Carbon capture and storage, Perceptions, Climate engineering perception, Carbon capture and storage, Public acceptance, Germany, Policy implications, Climate change mitigation, Carbon capture and storage, Decade-long challenges, Policy and regulatory framework, Technological advancements, Climate change mitigation, Carbon sequestration, Hydrological processes, Geologic storage, Environmental impact, Carbon capture and storage (CCS), Negative emissions technologies (NET), Climate mitigation, Bioenergy with CCS (BECCS), CO2 storage assessment, Carbon capture, utilization, and storage (CCUS), Pipeline network for carbon dioxide transport, Financial incentives for CCUS, Ethanol biorefineries, Enhanced oil recovery (EOR), Climate change mitigation, Negative emission technologies, Carbon accounting, Policy frameworks, Sustainable biomass production\"\n",
    "\n",
    "    prompt = f'''\n",
    "    I am giving you a list of topics in the format [Topic1, Topic2, ...]. Give me the main topic that most differentiates this list from other carbon capture topic lists: '{topics_str}'. Be as concise, clear, and accurate as possible. Only use carbon capture when all the topics are essentially the entire field of carbon capture, otherwise, try to output a broader theme that is differentiated from the other topics. The output should be of this format: '[<insert main topic>]'\n",
    "\n",
    "    Example:\n",
    "    Topics: [Public opinion, carbon capture, policy, regulatory framework, technological advancements, CCUS, CCS, storage, enhanced rock weathering]\n",
    "    Output: [Public policy and governance]\n",
    "    '''\n",
    "    time.sleep(25)\n",
    "    res = chat_openai(prompt)\n",
    "    print(topics_str + \" -- \" + res[0])\n",
    "\n",
    "    if (res[0][0] != '[' and res[0][-1] != ']'):\n",
    "        return \"[Unknown]\"\n",
    "\n",
    "    # Assuming that res[0] is a string of main topic enclosed in brackets, we strip the brackets\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_main_topic([\n",
    "                    \"Carbon capture\",\n",
    "                    \"Membrane design\",\n",
    "                    \"Gas separation\",\n",
    "                    \"Mechanical strength\",\n",
    "                    \"Plant root mimicry\",\n",
    "                    \"Carbon capture\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a list of GPT topics and main topics to each cluster\n",
    "def update_cluster_topics(cluster, df):\n",
    "    if isinstance(cluster['content'][0], dict):  # it's a parent cluster\n",
    "        all_topics = []  # will store all topics from all children clusters\n",
    "        for sub_cluster in cluster['content']:\n",
    "            update_cluster_topics(sub_cluster, df)\n",
    "            all_topics.extend(sub_cluster['GPT_topics'])\n",
    "        cluster['GPT_topics'] = all_topics\n",
    "    else:  # it's a leaf cluster\n",
    "        paperIds = cluster['content']\n",
    "        all_topics = []  # will store all topics from all papers in this cluster\n",
    "        for paperId in paperIds:\n",
    "            paper_topics = df[df['paperId'] == paperId]['GPT_topics'].values\n",
    "            if len(paper_topics) > 0:\n",
    "                # paper_topics[0] is a string of topics comma separated in brackets, \n",
    "                # we convert it to a list of topics and extend all_topics with it\n",
    "                topics_list = paper_topics[0].strip('[]').split(', ')\n",
    "                all_topics.extend(topics_list)\n",
    "        cluster['GPT_topics'] = all_topics\n",
    "\n",
    "    # After assigning GPT_topics, we get the main topic\n",
    "    print(cluster['cluster_id'])\n",
    "    # make sure cluster main topic hasn't already been assigned to save token usage\n",
    "    if cluster['main_topic'] and cluster['main_topic'][0] != '[' and cluster['main_topic'][-1] != ']':\n",
    "        cluster['main_topic'] = get_main_topic(cluster['GPT_topics'])\n",
    "\n",
    "        with open('pruned_tree_w_main_topic.json', 'w') as f:\n",
    "            json.dump(pruned_trees, f, indent=4)\n",
    "\n",
    "# Then call the function for each tree in the list like this\n",
    "for tree in pruned_trees:\n",
    "    update_cluster_topics(tree, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(pruned_trees, indent=4))\n",
    "\n",
    "with open('pruned_tree_w_topics.json', 'w') as f:\n",
    "    json.dump(pruned_trees, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating clusters from 1. x, y coordinates, 2. edges, and 3. topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using HDBSCAN for hierarchical clustering on TSNE data. Worst case just use K-means or DBSCAN\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load the data from your JSON file\n",
    "with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\knowledge_base\\t-sne\\output_2000.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print out the DataFrame to verify\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.any(np.isnan(df[['x', 'y']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract 'x' and 'y' columns from df and convert them into a 2D array\n",
    "data = df[['x', 'y']].values\n",
    "\n",
    "# Create an HDBSCAN instance\n",
    "clusterer = HDBSCAN(min_cluster_size=100, min_samples=1, gen_min_span_tree=True, cluster_selection_epsilon=0.5, cluster_selection_method='eom')\n",
    "\n",
    "# Fit the model to your data\n",
    "clusterer.fit(data)\n",
    "\n",
    "# Add the labels to the DataFrame\n",
    "df['hdbscan_labels'] = clusterer.labels_\n",
    "\n",
    "# Get the condensed tree \n",
    "tree = clusterer.condensed_tree_.to_pandas()\n",
    "\n",
    "# Create a dictionary to map every cluster to its paper IDs\n",
    "cluster_to_paper = df.groupby('hdbscan_labels')['paperId'].apply(list).to_dict()\n",
    "\n",
    "# the parent-child relationship of the clusters\n",
    "relationships = tree[['parent', 'child']].values\n",
    "\n",
    "# create a dictionary to store cluster hierarchy\n",
    "cluster_tree = defaultdict(list)\n",
    "\n",
    "# iterate through hierarchy df and fill children\n",
    "for parent, child in relationships:\n",
    "    cluster_tree[parent].append(child)\n",
    "\n",
    "# Use cache to store results of function calls (memoization)\n",
    "descendants_cache = {}\n",
    "def get_all_descendants(cluster_tree, cluster_id):\n",
    "    if cluster_id in descendants_cache:\n",
    "        return descendants_cache[cluster_id]\n",
    "\n",
    "    descendants = []\n",
    "    child_clusters = cluster_tree[cluster_id]\n",
    "    if child_clusters:\n",
    "        descendants.extend(child_clusters)\n",
    "        for child_cluster in child_clusters:\n",
    "            descendants.extend(get_all_descendants(cluster_tree, child_cluster))\n",
    "    \n",
    "    descendants_cache[cluster_id] = descendants\n",
    "    return descendants\n",
    "\n",
    "# Use cache to store results of centroid calculations (memoization)\n",
    "centroid_cache = {}\n",
    "def calculate_cluster_centroid(cluster_ids):\n",
    "    # Get all the descendants of the cluster\n",
    "    all_cluster_ids = cluster_ids\n",
    "    for cluster_id in cluster_ids:\n",
    "        child_clusters = get_all_descendants(cluster_tree, cluster_id)\n",
    "        if child_clusters:\n",
    "            all_cluster_ids += child_clusters\n",
    "\n",
    "    centroid_x = []\n",
    "    centroid_y = []\n",
    "    for cluster_id in all_cluster_ids:\n",
    "        if cluster_id in cluster_to_paper:\n",
    "            paper_ids = cluster_to_paper[cluster_id]\n",
    "            cluster_points = df[df['paperId'].isin(paper_ids)]\n",
    "            x_values = cluster_points['x'].values\n",
    "            y_values = cluster_points['y'].values\n",
    "\n",
    "            valid_values = np.logical_and(~np.isnan(x_values), ~np.isnan(y_values))\n",
    "            if np.any(valid_values):\n",
    "                centroid_x.append(np.mean(x_values[valid_values]))\n",
    "                centroid_y.append(np.mean(y_values[valid_values]))\n",
    "    \n",
    "    if centroid_x and centroid_y:\n",
    "        centroid_cache[cluster_id] = (np.mean(centroid_x), np.mean(centroid_y))\n",
    "        return centroid_cache[cluster_id]\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "\n",
    "def traverse_tree(cluster_tree, cluster_id, layer):\n",
    "    if not cluster_tree[cluster_id]:\n",
    "        paper_points = df[df['hdbscan_labels'] == cluster_id]['paperId'].tolist()\n",
    "        centroid_x, centroid_y = centroid_cache.get(cluster_id) if cluster_id in centroid_cache else calculate_cluster_centroid([cluster_id])\n",
    "        return {\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"layer\": layer,\n",
    "            \"content\": paper_points,\n",
    "            \"centroid_x\": centroid_x,\n",
    "            \"centroid_y\": centroid_y\n",
    "        } if paper_points else None\n",
    "\n",
    "    result = {\"cluster_id\": cluster_id, \"layer\": layer, \"content\": [], \"centroid_x\": None, \"centroid_y\": None}\n",
    "    child_cluster_ids = cluster_tree[cluster_id]\n",
    "    for child_cluster_id in child_cluster_ids:\n",
    "        child_tree = traverse_tree(cluster_tree, child_cluster_id, layer + 1)\n",
    "        if child_tree is not None:  # only add child_tree if it's not None\n",
    "            result[\"content\"].append(child_tree)\n",
    "    \n",
    "    centroid_x, centroid_y = centroid_cache.get(cluster_id) if cluster_id in centroid_cache else calculate_cluster_centroid(child_cluster_ids)\n",
    "    result[\"centroid_x\"] = centroid_x\n",
    "    result[\"centroid_y\"] = centroid_y\n",
    "\n",
    "    return result if result[\"content\"] else None\n",
    "\n",
    "# Let's find all the roots and traverse the tree from each root\n",
    "roots = set(cluster_tree.keys()) - set(child for children in cluster_tree.values() for child in children)\n",
    "\n",
    "trees = []\n",
    "for root in roots:\n",
    "    tree = traverse_tree(cluster_tree, root, 0)\n",
    "    if tree is not None:\n",
    "        trees.append(tree)\n",
    "\n",
    "\n",
    "# took 2 min to complete for around 60 samples.\n",
    "print(\"COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Custom encoder class to convert int64 to int\n",
    "class Int64Encoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "# Convert trees to JSON format with indentation for readability\n",
    "tree_json = json.dumps(trees, indent=4, cls=Int64Encoder)\n",
    "\n",
    "# Print the formatted tree\n",
    "print(tree_json)\n",
    "\n",
    "# Specify the file path to save the JSON data\n",
    "output_file = \"clusters/tree_2000.json\"\n",
    "\n",
    "# Write the JSON data to the file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(trees, file, indent=4, cls=Int64Encoder)\n",
    "\n",
    "print(\"JSON file saved successfully.\")\n",
    "\n",
    "# JUMP TO PRUNING BELOW IF NECESSARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, it seems like d3 for computing infinite voronoi diagrams is probably the best bet because python doesn't have a library for that for some reason\n",
    "# Pre-processing cluster trees to form voronoi diagram\n",
    "from shapely.geometry import Polygon\n",
    "from scipy.spatial import Voronoi, convex_hull_plot_2d, Delaunay\n",
    "import numpy as np\n",
    "\n",
    "# This function creates Voronoi polygons\n",
    "def create_voronoi_polygons(cluster):\n",
    "    leaf_clusters = []\n",
    "    all_centroids = []\n",
    "\n",
    "    # print(\"cluster\", cluster)\n",
    "    # print(\"cluster['content']\", cluster['content'])\n",
    "    # print(\"cluster['content'][0]\", cluster['content'][0])\n",
    "    def recurse(cluster):\n",
    "        if isinstance(cluster['content'][0], dict):\n",
    "            for subcluster in cluster['content']:\n",
    "                recurse(subcluster)\n",
    "        else:\n",
    "            leaf_clusters.append(cluster)\n",
    "            all_centroids.append([cluster['centroid_x'], cluster['centroid_y']])\n",
    "\n",
    "    recurse(cluster)\n",
    "\n",
    "\n",
    "    all_centroids = np.array(all_centroids)\n",
    "    vor = Voronoi(all_centroids)\n",
    "    hull = Delaunay(all_centroids)\n",
    "\n",
    "    # testing if voronoi works properly\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = voronoi_plot_2d(vor)\n",
    "    plt.show()\n",
    "    print(\"leaf_clusters\", len(leaf_clusters))\n",
    "    print(\"len(vor.point_region)\", len(vor.point_region))\n",
    "    print(\"vor.point_region\", vor.point_region)\n",
    "\n",
    "    polygons = []\n",
    "    for indices in vor.point_region:\n",
    "        print(\"indices\", indices)\n",
    "        region = vor.regions[indices]\n",
    "        print(\"vor.vertices[region].tolist()\", vor.vertices[region].tolist())\n",
    "\n",
    "        # TODO: risk, just not handling this and adding the vertices\n",
    "        # if not region: \n",
    "        #     continue\n",
    "        # if -1 in region:  # infite region that goes on forever\n",
    "        #     print(\"region\", region)\n",
    "        #     polygon = hull.points[hull.convex_hull].tolist()  \n",
    "        # else:\n",
    "        \n",
    "        polygon = vor.vertices[region].tolist()\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    for i, leaf_cluster in enumerate(leaf_clusters):\n",
    "        # print(polygons[i])\n",
    "        leaf_cluster['polygonPoints'] = polygons[i]\n",
    "\n",
    "    # visualie leaf_cluster polygons\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = voronoi_plot_2d(vor)\n",
    "    plt.show()\n",
    "\n",
    "# Main function to add polygon points to clusters\n",
    "def add_polygon_points_to_clusters(cluster):\n",
    "    # create Voronoi polygons for the leaf clusters\n",
    "    create_voronoi_polygons(cluster)\n",
    "\n",
    "    # This function populates polygon points up to the parent clusters\n",
    "    def recurse(cluster):\n",
    "        if isinstance(cluster['content'][0], dict):\n",
    "            cluster['polygonPoints'] = []\n",
    "            for subcluster in cluster['content']:\n",
    "                recurse(subcluster)\n",
    "                cluster['polygonPoints'].extend(subcluster.get('polygonPoints', []))\n",
    "\n",
    "    recurse(cluster)\n",
    "\n",
    "# Let's use the functions\n",
    "add_polygon_points_to_clusters(trees[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert trees to JSON format with indentation for readability\n",
    "tree_json = json.dumps(trees, indent=4, cls=Int64Encoder)\n",
    "\n",
    "# Print the formatted tree\n",
    "print(tree_json)\n",
    "\n",
    "# Specify the file path to save the JSON data\n",
    "output_file = \"tree.json\"\n",
    "\n",
    "# Write the JSON data to the file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(trees, file, indent=4, cls=Int64Encoder)\n",
    "\n",
    "print(\"JSON file saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUNING STARTS HERE\n",
    "# Loading trees back in\n",
    "import json\n",
    "\n",
    "# Open the file in read mode\n",
    "with open('clusters/tree_2000.json', 'r') as f:\n",
    "    trees = json.load(f)  # Load the JSON data from the file\n",
    "\n",
    "# Now, 'data' is a list of dictionaries that you can manipulate\n",
    "# print(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning the trees\n",
    "def prune_single_child_clusters(cluster, depth=0):\n",
    "    cluster['layer'] = depth  # Set the layer to the current depth\n",
    "\n",
    "    # If this cluster contains sub-clusters\n",
    "    if 'content' in cluster and isinstance(cluster['content'][0], dict):\n",
    "        new_content = []\n",
    "        for sub_cluster in cluster['content']:\n",
    "            pruned = prune_single_child_clusters(sub_cluster, depth+1)\n",
    "            # if the pruned sub_cluster has only one sub_cluster, replace it with its sub_cluster\n",
    "            if len(pruned.get('content', [])) == 1 and isinstance(pruned['content'][0], dict):\n",
    "                new_content.append(pruned['content'][0])\n",
    "            else:\n",
    "                new_content.append(pruned)\n",
    "        cluster['content'] = new_content\n",
    "    return cluster\n",
    "\n",
    "pruned_trees = [prune_single_child_clusters(cluster) for cluster in trees]\n",
    "\n",
    "print(json.dumps(pruned_trees, indent=4))\n",
    "\n",
    "with open('clusters/pruned_tree_2000.json', 'w') as f:\n",
    "    json.dump(pruned_trees, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually inspect simple tree\n",
    "def simplify_cluster(cluster):\n",
    "    # Create a new dictionary with just the cluster_id\n",
    "    simple_cluster = {'cluster_id': cluster['cluster_id']}\n",
    "\n",
    "    # If this cluster contains sub-clusters, simplify those as well\n",
    "    if 'content' in cluster and isinstance(cluster['content'][0], dict):\n",
    "        simple_cluster['content'] = [simplify_cluster(sub_cluster) for sub_cluster in cluster['content']]\n",
    "\n",
    "    return simple_cluster\n",
    "\n",
    "# Simplify each cluster in the data\n",
    "simple_data = [simplify_cluster(cluster) for cluster in pruned_trees]\n",
    "\n",
    "# Print the simplified data as a formatted JSON string\n",
    "print(json.dumps(simple_data, indent=4))\n",
    "\n",
    "with open('simple_tree.json', 'w') as f:\n",
    "    json.dump(simple_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually inspect both papers\n",
    "def get_papers_details(paper_ids, cluster_id, df):\n",
    "    papers_df = df[df['paperId'].isin(paper_ids)][['paperId', 'title', 'abstract', 's2FieldsOfStudy']]\n",
    "    papers_df.to_json(f'leaf_cluster_paper_inspection/papers_details_{cluster_id}.json', orient='records', indent=4)\n",
    "\n",
    "def print_cluster_details(cluster):\n",
    "    if 'content' in cluster:\n",
    "        if isinstance(cluster['content'][0], dict):  # if 'content' contains clusters\n",
    "            for item in cluster['content']:  # iterate over the list of clusters\n",
    "                print_cluster_details(item)  # recursion call\n",
    "        else:  # it's a leaf cluster\n",
    "            print(cluster['cluster_id'])  # print leaf cluster id\n",
    "            cluster_id = cluster['cluster_id']\n",
    "            paper_ids = cluster['content']\n",
    "            get_papers_details(paper_ids, cluster_id, df)\n",
    "            \n",
    "# Use the function like this\n",
    "for cluster in trees:\n",
    "    print(f\"Cluster Id: {cluster['cluster_id']}\")\n",
    "    print_cluster_details(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a literature review using concepts (TLDRs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use knowledge base from Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load the data from your JSON file\n",
    "with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\data_collection\\output_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# remove rows with null abstracts\n",
    "df = df[df['abstract'].notna()]\n",
    "\n",
    "# print out the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load the data from your JSON file\n",
    "with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\data_collection\\output_50.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# remove rows with null abstracts\n",
    "# df = df[df['abstract'].notna()]\n",
    "# df = df[df['tldr'].notna()]\n",
    "\n",
    "# print out the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row of the DataFrame\n",
    "first_row = df.iloc[0]\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the DataFrame\n",
    "num_rows, num_cols = df.shape\n",
    "print(f\"The dataframe has {num_rows} rows and {num_cols} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have GPT3.5 generate a literature review of carbon capture using the TLDRs (significance)\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bulk openai message, no stream\n",
    "def chat_openai(prompt=\"Tell me to ask you a prompt\", chat_history=[]):\n",
    "    # define message conversation for model\n",
    "    if chat_history:\n",
    "        messages = chat_history\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        ]\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # create the chat completion\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    text_answer = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # updated conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": text_answer})\n",
    "\n",
    "    return text_answer, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Topic Metadata from S2FieldsOfStudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 words for each topic\n",
    "for i in range(lda.num_topics):\n",
    "    print(f\"Topic {i}:\")\n",
    "    for word, prob in lda.show_topic(i, topn=10):\n",
    "        print(f\"  {word}: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect list of all s2FieldsOfStudy topics, corresponding PaperIds, and citation counts, and the centroid x and y values\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "topic_data = defaultdict(lambda: {\"paperIds\": [], \"citationCount\": 0, \"xSum\": 0, \"ySum\": 0, \"count\": 0})\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for category_info in row['s2FieldsOfStudy']:\n",
    "        # Get category\n",
    "        category = category_info['category']\n",
    "        \n",
    "        # Prevent duplicates\n",
    "        if category_info['source'] == 's2-fos-model':\n",
    "            # Check if paperId already counted for this category\n",
    "            if row['paperId'] not in topic_data[category][\"paperIds\"]:\n",
    "                # Add paper id to category\n",
    "                topic_data[category][\"paperIds\"].append(row['paperId'])\n",
    "            \n",
    "                # Cumulative citation count\n",
    "                topic_data[category][\"citationCount\"] += row['citationCount']\n",
    "                \n",
    "                # Sum x and y for the average\n",
    "                topic_data[category][\"xSum\"] += row['x']\n",
    "                topic_data[category][\"ySum\"] += row['y']\n",
    "                topic_data[category][\"count\"] += 1\n",
    "\n",
    "# Compute the average x and y for each category\n",
    "for category in topic_data:\n",
    "    topic_data[category][\"x\"] = topic_data[category][\"xSum\"] / topic_data[category][\"count\"]\n",
    "    topic_data[category][\"y\"] = topic_data[category][\"ySum\"] / topic_data[category][\"count\"]\n",
    "    \n",
    "    # Remove unnecessary keys\n",
    "    del topic_data[category][\"xSum\"]\n",
    "    del topic_data[category][\"ySum\"]\n",
    "    del topic_data[category][\"count\"]\n",
    "\n",
    "# Convert topic_data to DataFrame\n",
    "topic_df = pd.DataFrame.from_dict(topic_data, orient='index').reset_index()\n",
    "\n",
    "# Rename 'index' column to 'Topic'\n",
    "topic_df.rename(columns={'index': 'topic'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.head()\n",
    "\n",
    "# Exporting dataframe with T-SNE coordinates\n",
    "topic_df.to_json('topic_100_tsne.json', orient='records')\n",
    "topic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for SPECTRE API\n",
    "SAMPLE_TOPICS = []\n",
    "\n",
    "for i, topic in enumerate(topic_data.keys(), start=1):\n",
    "    topic_dict = {\n",
    "        \"paper_id\": topic,\n",
    "        \"title\": topic,\n",
    "        \"abstract\": topic\n",
    "    }\n",
    "    SAMPLE_TOPICS.append(topic_dict)\n",
    "\n",
    "SAMPLE_TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create SPECTRE em beddings for all the s2FieldsOfStudy\n",
    "from typing import Dict, List\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "URL = \"https://model-apis.semanticscholar.org/specter/v1/invoke\"\n",
    "MAX_BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def chunks(lst, chunk_size=MAX_BATCH_SIZE):\n",
    "    \"\"\"Splits a longer list to respect batch size\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i : i + chunk_size]\n",
    "\n",
    "\n",
    "SAMPLE_PAPERS = [\n",
    "    {\n",
    "        \"paper_id\": \"A\",\n",
    "        \"title\": \"Angiotensin-converting enzyme 2 is a functional receptor for the SARS coronavirus\",\n",
    "        \"abstract\": \"Spike (S) proteins of coronaviruses ...\",\n",
    "    },\n",
    "    {\n",
    "        \"paper_id\": \"B\",\n",
    "        \"title\": \"Hospital outbreak of Middle East respiratory syndrome coronavirus\",\n",
    "        \"abstract\": \"Between April 1 and May 23, 2013, a total of 23 cases of MERS-CoV ...\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def embed(papers):\n",
    "    embeddings_by_paper_id: Dict[str, List[float]] = {}\n",
    "\n",
    "    for chunk in chunks(papers):\n",
    "        # Allow Python requests to convert the data above to JSON\n",
    "        response = requests.post(URL, json=chunk)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\"Sorry, something went wrong, please try later!\")\n",
    "\n",
    "        for paper in response.json()[\"preds\"]:\n",
    "            embeddings_by_paper_id[paper[\"paper_id\"]] = paper[\"embedding\"]\n",
    "\n",
    "    return embeddings_by_paper_id\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "all_embeddings = embed(SAMPLE_TOPICS)\n",
    "\n",
    "# Prints { 'A': [4.089589595794678, ...], 'B': [-0.15814849734306335, ...] }\n",
    "print(len(all_embeddings))\n",
    "print(len(df['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all_embeddings to new df and then run T-SNE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all_embeddings values to a list of lists\n",
    "all_embeddings_list = list(all_embeddings.values())\n",
    "\n",
    "# Convert df['embedding'] to a list of lists\n",
    "df_embeddings = df['embedding'].apply(lambda x: x['vector']).tolist()\n",
    "\n",
    "# Stack them together\n",
    "combined_embeddings = np.vstack([df_embeddings, all_embeddings_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "embeddings_2d = tsne.fit_transform(combined_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 2D t-SNE coordinates to the papers DataFrame and normalize citationCount for node sizes\n",
    "df['x'] = embeddings_2d[:len(df), 0]\n",
    "df['y'] = embeddings_2d[:len(df), 1]\n",
    "df['citationCount_normalized'] = (df['citationCount'] - df['citationCount'].min()) / (df['citationCount'].max() - df['citationCount'].min())\n",
    "df['citationCount_normalized'] = df['citationCount_normalized'] * 100  # Scale to a suitable range for scatter plot node sizes\n",
    "\n",
    "# Sort the DataFrame based on citationCount and select the top 20\n",
    "df_top20 = df.nlargest(20, 'citationCount')\n",
    "\n",
    "# Create a scatter plot of all the points with node sizes based on normalized citationCount\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(df['x'], df['y'], s=df['citationCount_normalized'], alpha=0.5, label='All papers')\n",
    "\n",
    "# Highlight the top 20 papers in the plot with node sizes based on normalized citationCount\n",
    "plt.scatter(df_top20['x'], df_top20['y'], s=df_top20['citationCount_normalized'], color='red', label='Top 20 papers')\n",
    "\n",
    "# Prepare to add titles of the top 20 papers to the plot with text wrapping\n",
    "texts = []\n",
    "for i, row in df_top20.iterrows():\n",
    "    title_wrapped = textwrap.fill(row['title'], width=100)  # Wrap text after 20 characters\n",
    "    plt.scatter(row['x'], row['y'], color='red')  # This will ensure the dot is above the line\n",
    "    texts.append(plt.annotate(title_wrapped, (row['x'], row['y']), textcoords=\"offset points\", xytext=(0,10), ha='center', arrowprops=dict(arrowstyle=\"->\")))\n",
    "\n",
    "# Exporting dataframe with T-SNE coordinates\n",
    "df.to_json('output_100_tsne.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add x and y coordinates for topics\n",
    "\n",
    "topic_df['x'] = embeddings_2d[len(df):, 0]\n",
    "topic_df['y'] = embeddings_2d[len(df):, 1]\n",
    "# topic_df['citationCount_normalized'] = (df['citationCount'] - df['citationCount'].min()) / (df['citationCount'].max() - df['citationCount'].min())\n",
    "# topic_df['citationCount_normalized'] = df['citationCount_normalized'] * 100  # Scale to a suitable range for scatter plot node sizes\n",
    "\n",
    "# Exporting dataframe with T-SNE coordinates\n",
    "topic_df.to_json('topic_100_tsne.json', orient='records')\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tldr_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I am a college student who is a beginner in carbon capture. Write a literature review of carbon capture using the TLDRs (significance) of papers: \" + str(tldr_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat_openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 'tldr' column as a Series\n",
    "tldr_series = df['tldr']\n",
    "\n",
    "# Extract the 'text' from each 'tldr' dictionary in the Series\n",
    "tldr_texts = [item['text'] if item is not None else None for item in tldr_series]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inspection of Output for Manual Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the input data\n",
    "with open('t-sne/output_100_tsne.json', 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "# Extract desired fields\n",
    "new_data = []\n",
    "for entry in data:\n",
    "    paperId = entry.get('paperId', None)\n",
    "    title = entry.get('title', None)\n",
    "    abstract = entry.get('abstract', None)\n",
    "    \n",
    "    # Build new data entry\n",
    "    if paperId is not None and title is not None and abstract is not None:\n",
    "        new_data.append({\n",
    "            'paperId': paperId,\n",
    "            'title': title,\n",
    "            # 'abstract': abstract\n",
    "        })\n",
    "\n",
    "# Write to output file\n",
    "with open('t-sne/output_100_tsne_manual_inspection_id_titles_only.json', 'w') as outfile:\n",
    "    json.dump(new_data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating paper topics from title and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load the data from your JSON file\n",
    "with open(r'C:\\Users\\1kevi\\Desktop\\projects\\Research\\autoscious-carbon-capture\\knowledge_base\\t-sne\\output_100_tsne.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print out the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing top 3 n-grams from title, embedding each n-gram, and then clustering each n-gram\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.corpus import stopwords\n",
    "# import numpy as np\n",
    "\n",
    "# # List of English stop words\n",
    "# stop_words = list(stopwords.words('english'))\n",
    "\n",
    "# # Create the transform\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(2,3), stop_words=stop_words)\n",
    "\n",
    "# # Tokenize, build vocab and calculate TF-IDF\n",
    "# tfidf_matrix = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to extract top 3 words with the highest TF-IDF score in a given document\n",
    "# def top_words(doc_index):\n",
    "#     feature_index = tfidf_matrix[doc_index,:].nonzero()[1]\n",
    "#     tfidf_scores = zip(feature_index, [tfidf_matrix[doc_index, x] for x in feature_index])\n",
    "    \n",
    "#     # Corresponding feature names and scores\n",
    "#     words_scores = [(vectorizer.get_feature_names_out() [i], s) for (i, s) in tfidf_scores]\n",
    "#     words_scores = sorted(words_scores, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     return words_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the function to each row index in the dataframe\n",
    "# df['top_words'] = [top_words(i) for i in range(tfidf_matrix.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['title', 'text', 'top_words']]\n",
    "# df[['title', 'text', 'top_words']].to_json('topics/top_words.json', orient='records', lines=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing reranking of words based on similarity to document embedding via specter & ada\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # models\n",
    "# EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "# model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "# def calculate_embedding(text):\n",
    "#     inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         return model(**inputs)[0].mean(dim=1).numpy()\n",
    "\n",
    "# # Calculate document embeddings\n",
    "# # df['document_embedding'] = df['text'].apply(calculate_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exploring LDA2Vec\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from lda2vec import LDA2Vec\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# # Load your data\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# # Preprocess your text data\n",
    "# # This step will depend on the specifics of your data\n",
    "# # For example, you might need to remove stop words, perform lemmatization, etc.\n",
    "\n",
    "# # Set up SPECTER model for embeddings\n",
    "# tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "# model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "# def calculate_embedding(text):\n",
    "#     inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         return model(**inputs)[0].mean(dim=1).numpy()\n",
    "\n",
    "# # Generate word embeddings using SPECTER\n",
    "# df['embeddings'] = df['text'].apply(calculate_embedding)\n",
    "\n",
    "# # Prepare data for LDA2Vec\n",
    "# # This involves creating a count matrix of your text data\n",
    "# vectorizer = CountVectorizer()\n",
    "# counts = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# # Create an LDA2Vec model\n",
    "# lda2vec = LDA2Vec(n_topics=20, n_words=10000)\n",
    "\n",
    "# # Fit the LDA2Vec model\n",
    "# lda2vec.fit(counts, df['embeddings'].tolist())\n",
    "\n",
    "# # Now you can use the lda2vec model to explore topics in your text\n",
    "# # For example, you can look at the most common words in each topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying Guided LDA\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.cluster import KMeans\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from nltk.util import ngrams\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Initialize SPECTER model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "# model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "# # Function to calculate embeddings\n",
    "# def calculate_embedding(text):\n",
    "#     inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         return model(**inputs)[0].mean(dim=1).numpy()\n",
    "\n",
    "# # Calculate document embeddings\n",
    "# # df['document_embedding'] = df['text'].apply(calculate_embedding)\n",
    "\n",
    "# # Tokenize text and create bigrams\n",
    "\n",
    "# # Load the set of English stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# df['tokens'] = df['text'].apply(lambda x: [word for word in x.split() if word not in stop_words])\n",
    "# df['bigrams'] = df['tokens'].apply(lambda x: [' '.join(gram) for gram in ngrams(x, 2) if all(word not in stop_words for word in gram)])\n",
    "\n",
    "# unique_tokens = pd.Series([item for sublist in df['tokens'].tolist() for item in sublist]).unique()\n",
    "# unique_bigrams = pd.Series([item for sublist in df['bigrams'].tolist() for item in sublist]).unique()\n",
    "\n",
    "# word_embeddings = {word: calculate_embedding(word) for word in unique_tokens}\n",
    "# bigram_embeddings = {bigram: calculate_embedding(bigram) for bigram in unique_bigrams}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embeddings = {word: word_embeddings[word].reshape(1, -1) for word in word_embeddings}\n",
    "# bigram_embeddings = {bigram: bigram_embeddings[bigram].reshape(1, -1) for bigram in bigram_embeddings}\n",
    "\n",
    "# print(word_embeddings['carbon'].shape)\n",
    "# print(bigram_embeddings['carbon capture'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cluster words and bigrams based on embeddings\n",
    "# # Cluster words and bigrams based on embeddings\n",
    "# word_clusters = KMeans(n_clusters=20).fit(np.array(list(word_embeddings.values())).reshape(-1, 768))\n",
    "# bigram_clusters = KMeans(n_clusters=20).fit(np.array(list(bigram_embeddings.values())).reshape(-1, 768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create seed topics based on clusters\n",
    "# seed_topics = {word: word_clusters.labels_[i] for i, word in enumerate(word_embeddings.keys())}\n",
    "# seed_topics.update({bigram: bigram_clusters.labels_[i] for i, bigram in enumerate(bigram_embeddings.keys())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(seed_topics)\n",
    "# # Need to verify if the topics and words under them are even good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_topics_grouped = {}\n",
    "\n",
    "# for word, topic in seed_topics.items():\n",
    "#     if topic in seed_topics_grouped:\n",
    "#         seed_topics_grouped[topic].append(word)\n",
    "#     else:\n",
    "#         seed_topics_grouped[topic] = [word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_topics_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import dot\n",
    "# from numpy.linalg import norm\n",
    "\n",
    "# def cosine_similarity(a, b):\n",
    "#     a = a.flatten()  # Add this line\n",
    "#     b = b.flatten()  # Add this line\n",
    "#     return dot(a, b) / (norm(a) * norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Running GuidedLDA\n",
    "# from guidedlda import GuidedLDA\n",
    "\n",
    "# # Vectorize your text data\n",
    "# vectorizer = CountVectorizer(vocabulary=seed_topics.keys())\n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# # Create a seed topics matrix\n",
    "# seed_topics_matrix = np.zeros((40, len(seed_topics)))\n",
    "\n",
    "# for word, topic in seed_topics.items():\n",
    "#     seed_topics_matrix[topic, vectorizer.vocabulary_[word]] = 1\n",
    "\n",
    "# # Run Guided LDA\n",
    "# model = GuidedLDA(n_topics=40, n_iter=100, random_state=7, refresh=20)\n",
    "# model.fit(X, seed_topics=seed_topics_matrix)\n",
    "\n",
    "# # Get the topic-word and document-topic distributions\n",
    "# topic_word_distributions = model.topic_word_\n",
    "# document_topic_distributions = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_top_words_spectre(row):\n",
    "#     # Get the document embedding and flatten it\n",
    "#     document_embedding = row['document_embedding_ada'].flatten()\n",
    "    \n",
    "#     top_words_spectre = []\n",
    "#     for word, _ in row['top_words']:\n",
    "#         word_embedding = calculate_embedding(word)\n",
    "#         word_embedding = word_embedding.flatten()  # Flatten the word embedding\n",
    "#         similarity = cosine_similarity(word_embedding, document_embedding)\n",
    "#         top_words_spectre.append((word, similarity))\n",
    "    \n",
    "#     # Sort the words by their similarity to the document embedding\n",
    "#     top_words_spectre.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     return top_words_spectre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 21 min on 100 papers and all their n grams\n",
    "# df['top_words_spectre'] = df.apply(get_top_words_spectre, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df[['title', 'text', 'top_words_spectre']].to_json('topics/top_words_spectre.json', orient='records', lines=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use an embedding to transform each of these words? \n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import numpy as np\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/miniLM-L6-H384-uncased')\n",
    "\n",
    "# # Assume 'top_words' is a list of your top words for each paper\n",
    "# top_words = ['carbon', 'capture', 'storage', 'renewable', 'energy']\n",
    "\n",
    "# word_embeddings = {}\n",
    "# for word in top_words:\n",
    "#     word_embeddings[word] = model.encode([word])[0]\n",
    "\n",
    "# # Now 'word_embeddings' is a dictionary that maps words to their corresponding embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT3.5 to generate topics\n",
    "\n",
    "# imports\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_GPT4_API_KEY')\n",
    "import openai\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "# GPT_MODEL = \"gpt-4\"\n",
    "\n",
    "# for bulk openai message, no stream\n",
    "def chat_openai(prompt=\"Tell me to ask you a prompt\", chat_history=[]):\n",
    "    # define message conversation for model\n",
    "    if chat_history:\n",
    "        messages = chat_history\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\"},\n",
    "        ]\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # create the chat completion\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    text_answer = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # updated conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": text_answer})\n",
    "\n",
    "    return text_answer, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_themes = 'Renewable integration, carbon capture, Clean energy transition, Operational flexibility, Cost-effective CO2 reduction, carbon storage, lost decade'\n",
    "\n",
    "# paper = \"titled '\" + df.iloc[0][\"title\"] + \"'\"\n",
    "# if df.iloc[0][\"abstract\"]:\n",
    "#     paper += \"with the following abstract: \" + df.iloc[0][\"abstract\"]\n",
    "\n",
    "# prompt = f'''\n",
    "# Paper: {paper} \\n Task: Given the paper title and abstract above, determine at most 5 themes for a researcher whose goal is to eventually make impactful discoveries and experiments. \\n Rules: Do not output any theme that is beyond what is given in the paper. Be as concise (less than 5 words), clear, and correct as possible. Do not make up anything not apparent from the paper. \\n Use themes from other papers only if the paper mentions them: {existing_themes}. \n",
    "\n",
    "# Your output should be of the following format: Theme1, Theme2, Theme3, Theme4, Theme5\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive t-sne & LDA testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use adjust_text to automatically adjust the positions of the text labels\n",
    "# adjust_text(texts)\n",
    "\n",
    "# plt.title('t-SNE visualization of embedding vectors')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Working implementation of t-SNE?\n",
    "\n",
    "# # Add the 2D t-SNE coordinates to the DataFrame and normalize citationCount for node sizes\n",
    "# df['x'] = embeddings_2d[:, 0]\n",
    "# df['y'] = embeddings_2d[:, 1]\n",
    "# df['citationCount_normalized'] = (df['citationCount'] - df['citationCount'].min()) / (df['citationCount'].max() - df['citationCount'].min())\n",
    "# df['citationCount_normalized'] = df['citationCount_normalized'] * len(df)  # Scale to a suitable range for scatter plot node sizes\n",
    "\n",
    "# # Sort the DataFrame based on citationCount and select the top 20\n",
    "# df_top20 = df.nlargest(10, 'citationCount')\n",
    "\n",
    "# # Create a scatter plot of all the points with node sizes based on normalized citationCount\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(df['x'], df['y'], s=df['citationCount_normalized'], alpha=0.5, label='All papers')\n",
    "\n",
    "# # Highlight the top 20 papers in the plot with node sizes based on normalized citationCount\n",
    "# top20_scatter = plt.scatter(df_top20['x'], df_top20['y'], s=df_top20['citationCount_normalized'], color='red')\n",
    "\n",
    "# # Prepare to add titles of the top 20 papers to the plot with text wrapping\n",
    "# texts = []\n",
    "# for i, row in df_top20.iterrows():\n",
    "#     title_wrapped = textwrap.fill(row['title'], width=20)  # Wrap text after 20 characters\n",
    "#     plt.scatter(row['x'], row['y'], color='red')  # This will ensure the dot is above the line\n",
    "#     texts.append(plt.annotate(title_wrapped, (row['x'], row['y']), textcoords=\"offset points\", xytext=(0,10), ha='center', arrowprops=dict(arrowstyle=\"->\")))\n",
    "\n",
    "# # Use adjust_text to automatically adjust the positions of the text labels\n",
    "# adjust_text(texts)\n",
    "\n",
    "# plt.title('t-SNE visualization of embedding vectors')\n",
    "# plt.legend(handles=[top20_scatter], labels=['Top 20 papers'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attempting hierarchical clustering with LDA\n",
    "# import pandas as pd\n",
    "# from gensim.corpora import Dictionary\n",
    "# from gensim.models import LdaModel\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Download the set of stop words the first time\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Load the set of English stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Assume that 'documents' is your list of documents. \n",
    "# # Each document is a string of words.\n",
    "\n",
    "# # Preprocess your documents\n",
    "# documents = df['abstract'].apply(simple_preprocess)\n",
    "\n",
    "# # Tokenize the documents into words, convert to lower case, \n",
    "# # and remove stop words\n",
    "# filtered_documents = []\n",
    "# for doc in documents:\n",
    "#     word_tokens = doc\n",
    "#     filtered_document = [word for word in word_tokens if word not in stop_words]\n",
    "#     filtered_documents.append(filtered_document)\n",
    "\n",
    "# # Create a gensim dictionary from the documents\n",
    "# dictionary = Dictionary(filtered_documents)\n",
    "\n",
    "# # Create a corpus for LDA\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# # Fit an LDA model\n",
    "# lda = LdaModel(corpus, num_topics=20, id2word=dictionary, alpha='auto', eta='auto')\n",
    "\n",
    "# # Print the top words for each topic\n",
    "# for i in range(10):\n",
    "#     print(f\"Topic {i+1}:\")\n",
    "#     print([dictionary[word_id] for word_id, prob in lda.get_topic_terms(i, topn=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the topic-document distribution from the LDA model\n",
    "# topic_dist = [lda.get_document_topics(bow, minimum_probability=0) for bow in corpus]\n",
    "\n",
    "# # Convert the topic distributions to a 2D array\n",
    "# topic_dist_array = np.zeros((len(corpus), lda.num_topics))\n",
    "# for i in range(len(corpus)):\n",
    "#     for topic, prob in topic_dist[i]:  # topic_dist[i] is a list of tuples\n",
    "#         topic_dist_array[i, topic] = prob  # topic is an integer (the topic ID)\n",
    "\n",
    "# # Perform hierarchical clustering on the topic-document distribution\n",
    "# cluster = AgglomerativeClustering(n_clusters=5)\n",
    "# cluster_labels = cluster.fit_predict(topic_dist_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
