{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import chat_openai\n",
    "import numpy as np \n",
    "import json\n",
    "import logging \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time \n",
    "\n",
    "from utils.misc import * \n",
    "# from utils.file_utils import find_files_with_substring\n",
    "# from utils.create_task import create_task\n",
    "from utils.extract_task_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chat_openai(\"hi\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM: gpt-3.5-turbo\n",
      "Task: Humanoid\n",
      "Task description: to make the humanoid run as fast as possible\n"
     ]
    }
   ],
   "source": [
    "# Starting based on eureka.py main\n",
    "EUREKA_ROOT_DIR = os.getcwd()\n",
    "ISAAC_ROOT_DIR = f\"{EUREKA_ROOT_DIR}/../isaacgymenvs/isaacgymenvs\"\n",
    "\n",
    "task = \"Humanoid\"\n",
    "task_description = \"to make the humanoid run as fast as possible\"\n",
    "suffix = \"GPT\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "print(f\"Using LLM: {model}\")\n",
    "print(\"Task: \" + task)\n",
    "print(\"Task description: \" + task_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"humanoid\"\n",
    "env_parent = 'isaac' if f'{env_name}.py' in os.listdir(f'{EUREKA_ROOT_DIR}/envs/isaac') else 'dexterity'\n",
    "task_file = f'{EUREKA_ROOT_DIR}/envs/{env_parent}/{env_name}.py'\n",
    "task_obs_file = f'{EUREKA_ROOT_DIR}/envs/{env_parent}/{env_name}_obs.py'\n",
    "shutil.copy(task_obs_file, f\"env_init_obs.py\")\n",
    "task_code_string  = file_to_string(task_file)\n",
    "task_obs_code_string  = file_to_string(task_obs_file)\n",
    "output_file = f\"{ISAAC_ROOT_DIR}/tasks/{env_name}{suffix.lower()}.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all text prompts\n",
    "prompt_dir = f'{EUREKA_ROOT_DIR}/utils/prompts'\n",
    "initial_system = file_to_string(f'{prompt_dir}/initial_system.txt')\n",
    "code_output_tip = file_to_string(f'{prompt_dir}/code_output_tip.txt')\n",
    "code_feedback = file_to_string(f'{prompt_dir}/code_feedback.txt')\n",
    "initial_user = file_to_string(f'{prompt_dir}/initial_user.txt')\n",
    "reward_signature = file_to_string(f'{prompt_dir}/reward_signature.txt')\n",
    "policy_feedback = file_to_string(f'{prompt_dir}/policy_feedback.txt')\n",
    "execution_error_feedback = file_to_string(f'{prompt_dir}/execution_error_feedback.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_system = initial_system.format(task_reward_signature_string=reward_signature) + code_output_tip\n",
    "initial_user = initial_user.format(task_obs_code_string=task_obs_code_string, task_description=task_description)\n",
    "messages = [{\"role\": \"system\", \"content\": initial_system}, {\"role\": \"user\", \"content\": initial_user}]\n",
    "\n",
    "task_code_string = task_code_string.replace(task, task+suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_FAILURE = -10000.\n",
    "max_successes = []\n",
    "max_successes_reward_correlation = []\n",
    "execute_rates = []\n",
    "best_code_paths = []\n",
    "max_success_overall = DUMMY_FAILURE\n",
    "max_success_reward_correlation_overall = DUMMY_FAILURE\n",
    "max_reward_code_path = None \n",
    "num_iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Generating 1 samples with gpt-3.5-turbo\n",
      "Messages:  [{'role': 'system', 'content': 'You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \\nYour reward function should use useful variables from the environment as inputs. As an example,\\nthe reward function signature can be: @torch.jit.script\\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    ...\\n    return reward, {}\\n\\nSince the reward function will be decorated with @torch.jit.script,\\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \\nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\\n    (1) the total reward,\\n    (2) a dictionary of each individual reward component.\\nThe code output should be formatted as a python code string: \"```python ... ```\".\\n\\nSome helpful tips for writing the reward function code:\\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\\n    (4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\\n'}, {'role': 'user', 'content': 'The Python environment is class Humanoid(VecTask):\\n    \"\"\"Rest of the environment definition omitted.\"\"\"\\n    def compute_observations(self):\\n        self.gym.refresh_dof_state_tensor(self.sim)\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\n\\n        self.gym.refresh_force_sensor_tensor(self.sim)\\n        self.gym.refresh_dof_force_tensor(self.sim)\\n        self.obs_buf[:], self.potentials[:], self.prev_potentials[:], self.up_vec[:], self.heading_vec[:] = compute_humanoid_observations(\\n            self.obs_buf, self.root_states, self.targets, self.potentials,\\n            self.inv_start_rot, self.dof_pos, self.dof_vel, self.dof_force_tensor,\\n            self.dof_limits_lower, self.dof_limits_upper, self.dof_vel_scale,\\n            self.vec_sensor_tensor, self.actions, self.dt, self.contact_force_scale, self.angular_velocity_scale,\\n            self.basis_vec0, self.basis_vec1)\\n\\ndef compute_humanoid_observations(obs_buf, root_states, targets, potentials, inv_start_rot, dof_pos, dof_vel,\\n                                  dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,\\n                                  sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,\\n                                  basis_vec0, basis_vec1):\\n    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\\n\\n    torso_position = root_states[:, 0:3]\\n    torso_rotation = root_states[:, 3:7]\\n    velocity = root_states[:, 7:10]\\n    ang_velocity = root_states[:, 10:13]\\n\\n    to_target = targets - torso_position\\n    to_target[:, 2] = 0\\n\\n    prev_potentials_new = potentials.clone()\\n    potentials = -torch.norm(to_target, p=2, dim=-1) / dt\\n\\n    torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(\\n        torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2)\\n\\n    vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(\\n        torso_quat, velocity, ang_velocity, targets, torso_position)\\n\\n    roll = normalize_angle(roll).unsqueeze(-1)\\n    yaw = normalize_angle(yaw).unsqueeze(-1)\\n    angle_to_target = normalize_angle(angle_to_target).unsqueeze(-1)\\n    dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)\\n\\n    obs = torch.cat((torso_position[:, 2].view(-1, 1), vel_loc, angvel_loc * angular_velocity_scale,\\n                     yaw, roll, angle_to_target, up_proj.unsqueeze(-1), heading_proj.unsqueeze(-1),\\n                     dof_pos_scaled, dof_vel * dof_vel_scale, dof_force * contact_force_scale,\\n                     sensor_force_torques.view(-1, 12) * contact_force_scale, actions), dim=-1)\\n\\n    return obs, potentials, prev_potentials_new, up_vec, heading_vec\\n\\n\\n\\n. Write a reward function for the following task: to make the humanoid run as fast as possible.'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: GPT Output:\n",
      " Based on the task description, a possible reward function for making the humanoid run as fast as possible could be to provide a positive reward based on the change in velocity of the humanoid. The reward can be calculated by comparing the current velocity with the previous velocity.\n",
      "\n",
      "Here is the reward function implementation:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from typing import Tuple, Dict\n",
      "\n",
      "@torch.jit.script\n",
      "def compute_reward(velocity: torch.Tensor, prev_velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    velocity_diff = velocity - prev_velocity\n",
      "    reward = torch.norm(velocity_diff, p=2)\n",
      "\n",
      "    reward_components = {\n",
      "        \"velocity_reward\": reward\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "- The function takes in the current velocity of the humanoid (`velocity`) and the previous velocity (`prev_velocity`) as input.\n",
      "- `velocity_diff` calculates the difference between the current velocity and the previous velocity of the humanoid.\n",
      "- `reward` calculates the reward based on the Euclidean norm of the velocity difference. This represents the speed of the humanoid.\n",
      "- The function returns the total reward (`reward`) and a dictionary (`reward_components`) containing the individual reward component (`velocity_reward`).\n",
      "\n",
      "Please note that this reward function only considers the velocity as the reward component and does not take into account other factors that may affect the running behavior of the humanoid.\n",
      "\n",
      "Iteration 0: Prompt Tokens: 1085, Completion Tokens: 281, Total Tokens: 1366\n",
      "RESPONSES:  [<OpenAIObject at 0x20de2d1daf0> JSON: {\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Based on the task description, a possible reward function for making the humanoid run as fast as possible could be to provide a positive reward based on the change in velocity of the humanoid. The reward can be calculated by comparing the current velocity with the previous velocity.\\n\\nHere is the reward function implementation:\\n\\n```python\\nimport torch\\nfrom typing import Tuple, Dict\\n\\n@torch.jit.script\\ndef compute_reward(velocity: torch.Tensor, prev_velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    velocity_diff = velocity - prev_velocity\\n    reward = torch.norm(velocity_diff, p=2)\\n\\n    reward_components = {\\n        \\\"velocity_reward\\\": reward\\n    }\\n\\n    return reward, reward_components\\n```\\n\\nExplanation:\\n- The function takes in the current velocity of the humanoid (`velocity`) and the previous velocity (`prev_velocity`) as input.\\n- `velocity_diff` calculates the difference between the current velocity and the previous velocity of the humanoid.\\n- `reward` calculates the reward based on the Euclidean norm of the velocity difference. This represents the speed of the humanoid.\\n- The function returns the total reward (`reward`) and a dictionary (`reward_components`) containing the individual reward component (`velocity_reward`).\\n\\nPlease note that this reward function only considers the velocity as the reward component and does not take into account other factors that may affect the running behavior of the humanoid.\"\n",
      "  },\n",
      "  \"finish_reason\": \"stop\"\n",
      "}]\n",
      "Iteration 0: Processing Code Run 0\n",
      "before simulation messages:  [{'role': 'system', 'content': 'You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \\nYour reward function should use useful variables from the environment as inputs. As an example,\\nthe reward function signature can be: @torch.jit.script\\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    ...\\n    return reward, {}\\n\\nSince the reward function will be decorated with @torch.jit.script,\\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \\nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\\n    (1) the total reward,\\n    (2) a dictionary of each individual reward component.\\nThe code output should be formatted as a python code string: \"```python ... ```\".\\n\\nSome helpful tips for writing the reward function code:\\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\\n    (4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\\n'}, {'role': 'user', 'content': 'The Python environment is class Humanoid(VecTask):\\n    \"\"\"Rest of the environment definition omitted.\"\"\"\\n    def compute_observations(self):\\n        self.gym.refresh_dof_state_tensor(self.sim)\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\n\\n        self.gym.refresh_force_sensor_tensor(self.sim)\\n        self.gym.refresh_dof_force_tensor(self.sim)\\n        self.obs_buf[:], self.potentials[:], self.prev_potentials[:], self.up_vec[:], self.heading_vec[:] = compute_humanoid_observations(\\n            self.obs_buf, self.root_states, self.targets, self.potentials,\\n            self.inv_start_rot, self.dof_pos, self.dof_vel, self.dof_force_tensor,\\n            self.dof_limits_lower, self.dof_limits_upper, self.dof_vel_scale,\\n            self.vec_sensor_tensor, self.actions, self.dt, self.contact_force_scale, self.angular_velocity_scale,\\n            self.basis_vec0, self.basis_vec1)\\n\\ndef compute_humanoid_observations(obs_buf, root_states, targets, potentials, inv_start_rot, dof_pos, dof_vel,\\n                                  dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,\\n                                  sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,\\n                                  basis_vec0, basis_vec1):\\n    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\\n\\n    torso_position = root_states[:, 0:3]\\n    torso_rotation = root_states[:, 3:7]\\n    velocity = root_states[:, 7:10]\\n    ang_velocity = root_states[:, 10:13]\\n\\n    to_target = targets - torso_position\\n    to_target[:, 2] = 0\\n\\n    prev_potentials_new = potentials.clone()\\n    potentials = -torch.norm(to_target, p=2, dim=-1) / dt\\n\\n    torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(\\n        torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2)\\n\\n    vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(\\n        torso_quat, velocity, ang_velocity, targets, torso_position)\\n\\n    roll = normalize_angle(roll).unsqueeze(-1)\\n    yaw = normalize_angle(yaw).unsqueeze(-1)\\n    angle_to_target = normalize_angle(angle_to_target).unsqueeze(-1)\\n    dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)\\n\\n    obs = torch.cat((torso_position[:, 2].view(-1, 1), vel_loc, angvel_loc * angular_velocity_scale,\\n                     yaw, roll, angle_to_target, up_proj.unsqueeze(-1), heading_proj.unsqueeze(-1),\\n                     dof_pos_scaled, dof_vel * dof_vel_scale, dof_force * contact_force_scale,\\n                     sensor_force_torques.view(-1, 12) * contact_force_scale, actions), dim=-1)\\n\\n    return obs, potentials, prev_potentials_new, up_vec, heading_vec\\n\\n\\n\\n. Write a reward function for the following task: to make the humanoid run as fast as possible.'}]\n",
      "simulation messages:  [{'role': 'system', 'content': 'You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \\nYour reward function should use useful variables from the environment as inputs. As an example,\\nthe reward function signature can be: @torch.jit.script\\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    ...\\n    return reward, {}\\n\\nSince the reward function will be decorated with @torch.jit.script,\\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \\nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\\n    (1) the total reward,\\n    (2) a dictionary of each individual reward component.\\nThe code output should be formatted as a python code string: \"```python ... ```\".\\n\\nSome helpful tips for writing the reward function code:\\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\\n    (4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\\n'}, {'role': 'user', 'content': 'The Python environment is class Humanoid(VecTask):\\n    \"\"\"Rest of the environment definition omitted.\"\"\"\\n    def compute_observations(self):\\n        self.gym.refresh_dof_state_tensor(self.sim)\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\n\\n        self.gym.refresh_force_sensor_tensor(self.sim)\\n        self.gym.refresh_dof_force_tensor(self.sim)\\n        self.obs_buf[:], self.potentials[:], self.prev_potentials[:], self.up_vec[:], self.heading_vec[:] = compute_humanoid_observations(\\n            self.obs_buf, self.root_states, self.targets, self.potentials,\\n            self.inv_start_rot, self.dof_pos, self.dof_vel, self.dof_force_tensor,\\n            self.dof_limits_lower, self.dof_limits_upper, self.dof_vel_scale,\\n            self.vec_sensor_tensor, self.actions, self.dt, self.contact_force_scale, self.angular_velocity_scale,\\n            self.basis_vec0, self.basis_vec1)\\n\\ndef compute_humanoid_observations(obs_buf, root_states, targets, potentials, inv_start_rot, dof_pos, dof_vel,\\n                                  dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,\\n                                  sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,\\n                                  basis_vec0, basis_vec1):\\n    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\\n\\n    torso_position = root_states[:, 0:3]\\n    torso_rotation = root_states[:, 3:7]\\n    velocity = root_states[:, 7:10]\\n    ang_velocity = root_states[:, 10:13]\\n\\n    to_target = targets - torso_position\\n    to_target[:, 2] = 0\\n\\n    prev_potentials_new = potentials.clone()\\n    potentials = -torch.norm(to_target, p=2, dim=-1) / dt\\n\\n    torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(\\n        torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2)\\n\\n    vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(\\n        torso_quat, velocity, ang_velocity, targets, torso_position)\\n\\n    roll = normalize_angle(roll).unsqueeze(-1)\\n    yaw = normalize_angle(yaw).unsqueeze(-1)\\n    angle_to_target = normalize_angle(angle_to_target).unsqueeze(-1)\\n    dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)\\n\\n    obs = torch.cat((torso_position[:, 2].view(-1, 1), vel_loc, angvel_loc * angular_velocity_scale,\\n                     yaw, roll, angle_to_target, up_proj.unsqueeze(-1), heading_proj.unsqueeze(-1),\\n                     dof_pos_scaled, dof_vel * dof_vel_scale, dof_force * contact_force_scale,\\n                     sensor_force_torques.view(-1, 12) * contact_force_scale, actions), dim=-1)\\n\\n    return obs, potentials, prev_potentials_new, up_vec, heading_vec\\n\\n\\n\\n. Write a reward function for the following task: to make the humanoid run as fast as possible.'}, {'role': 'assistant', 'content': 'Based on the task description, a possible reward function for making the humanoid run as fast as possible could be to provide a positive reward based on the change in velocity of the humanoid. The reward can be calculated by comparing the current velocity with the previous velocity.\\n\\nHere is the reward function implementation:\\n\\n```python\\nimport torch\\nfrom typing import Tuple, Dict\\n\\n@torch.jit.script\\ndef compute_reward(velocity: torch.Tensor, prev_velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    velocity_diff = velocity - prev_velocity\\n    reward = torch.norm(velocity_diff, p=2)\\n\\n    reward_components = {\\n        \"velocity_reward\": reward\\n    }\\n\\n    return reward, reward_components\\n```\\n\\nExplanation:\\n- The function takes in the current velocity of the humanoid (`velocity`) and the previous velocity (`prev_velocity`) as input.\\n- `velocity_diff` calculates the difference between the current velocity and the previous velocity of the humanoid.\\n- `reward` calculates the reward based on the Euclidean norm of the velocity difference. This represents the speed of the humanoid.\\n- The function returns the total reward (`reward`) and a dictionary (`reward_components`) containing the individual reward component (`velocity_reward`).\\n\\nPlease note that this reward function only considers the velocity as the reward component and does not take into account other factors that may affect the running behavior of the humanoid.'}, {'role': 'user', 'content': 'Assume that you are about to start testing. Theoretically simulate and predict the most likely outcome. Think step by step at the deepest and most informative level. What will happen? You are to predict the result.'}]\n",
      "...processed code run... {\n",
      "  \"id\": \"chatcmpl-8Fc8R13XEdnpVMpzmbWmn2OeNDud3\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1698732203,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"To predict the most likely outcome of testing the reward function, let's analyze the steps involved and the information available:\\n\\n1. The environment provides observations, including the current velocity and previous velocity of the humanoid.\\n2. The reward function takes the current velocity and previous velocity as inputs.\\n3. The reward function calculates the difference between the current velocity and the previous velocity.\\n4. The reward function calculates the reward based on the Euclidean norm of the velocity difference.\\n5. The reward function returns the total reward and a dictionary containing the individual reward component.\\n\\nBased on this analysis, here is the predicted outcome:\\n\\n1. Initially, the humanoid will have a low velocity, and the previous velocity will be zero.\\n2. The reward function will calculate the velocity difference, which will be equal to the current velocity since the previous velocity is zero.\\n3. The reward function will calculate the reward based on the Euclidean norm of the velocity difference.\\n4. As the humanoid starts running and its velocity increases, the velocity difference will also increase, resulting in a higher reward.\\n5. The reward will continue to increase as the humanoid runs faster.\\n6. The reward function will return the total reward and the individual reward component.\\n\\nIn summary, the predicted outcome is that the reward function will provide a positive reward that increases as the humanoid's velocity increases. This will incentivize the humanoid to run faster in order to maximize the reward.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1418,\n",
      "    \"completion_tokens\": 283,\n",
      "    \"total_tokens\": 1701\n",
      "  }\n",
      "}\n",
      "---Gather RL training results and construct reward reflection as prompt for next LLM---\n",
      "\n",
      "KEYS, VALUES:  0 ('Based on the task description, a possible reward function for making the humanoid run as fast as possible could be to provide a positive reward based on the change in velocity of the humanoid. The reward can be calculated by comparing the current velocity with the previous velocity.\\n\\nHere is the reward function implementation:\\n\\n```python\\nimport torch\\nfrom typing import Tuple, Dict\\n\\n@torch.jit.script\\ndef compute_reward(velocity: torch.Tensor, prev_velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    velocity_diff = velocity - prev_velocity\\n    reward = torch.norm(velocity_diff, p=2)\\n\\n    reward_components = {\\n        \"velocity_reward\": reward\\n    }\\n\\n    return reward, reward_components\\n```\\n\\nExplanation:\\n- The function takes in the current velocity of the humanoid (`velocity`) and the previous velocity (`prev_velocity`) as input.\\n- `velocity_diff` calculates the difference between the current velocity and the previous velocity of the humanoid.\\n- `reward` calculates the reward based on the Euclidean norm of the velocity difference. This represents the speed of the humanoid.\\n- The function returns the total reward (`reward`) and a dictionary (`reward_components`) containing the individual reward component (`velocity_reward`).\\n\\nPlease note that this reward function only considers the velocity as the reward component and does not take into account other factors that may affect the running behavior of the humanoid.', <OpenAIObject chat.completion id=chatcmpl-8Fc8R13XEdnpVMpzmbWmn2OeNDud3 at 0x20de2d1d490> JSON: {\n",
      "  \"id\": \"chatcmpl-8Fc8R13XEdnpVMpzmbWmn2OeNDud3\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1698732203,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"To predict the most likely outcome of testing the reward function, let's analyze the steps involved and the information available:\\n\\n1. The environment provides observations, including the current velocity and previous velocity of the humanoid.\\n2. The reward function takes the current velocity and previous velocity as inputs.\\n3. The reward function calculates the difference between the current velocity and the previous velocity.\\n4. The reward function calculates the reward based on the Euclidean norm of the velocity difference.\\n5. The reward function returns the total reward and a dictionary containing the individual reward component.\\n\\nBased on this analysis, here is the predicted outcome:\\n\\n1. Initially, the humanoid will have a low velocity, and the previous velocity will be zero.\\n2. The reward function will calculate the velocity difference, which will be equal to the current velocity since the previous velocity is zero.\\n3. The reward function will calculate the reward based on the Euclidean norm of the velocity difference.\\n4. As the humanoid starts running and its velocity increases, the velocity difference will also increase, resulting in a higher reward.\\n5. The reward will continue to increase as the humanoid runs faster.\\n6. The reward function will return the total reward and the individual reward component.\\n\\nIn summary, the predicted outcome is that the reward function will provide a positive reward that increases as the humanoid's velocity increases. This will incentivize the humanoid to run faster in order to maximize the reward.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1418,\n",
      "    \"completion_tokens\": 283,\n",
      "    \"total_tokens\": 1701\n",
      "  }\n",
      "})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "split",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kevihuang\\Desktop\\projects\\autoscious-carbon-capture\\eureka_exp\\venv2\\Lib\\site-packages\\openai\\openai_object.py:59\u001b[0m, in \u001b[0;36mOpenAIObject.__getattr__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[k]\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'split'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kevihuang\\Desktop\\projects\\autoscious-carbon-capture\\eureka_exp\\eureka.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevihuang/Desktop/projects/autoscious-carbon-capture/eureka_exp/eureka.ipynb#X15sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevihuang/Desktop/projects/autoscious-carbon-capture/eureka_exp/eureka.ipynb#X15sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m content \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/kevihuang/Desktop/projects/autoscious-carbon-capture/eureka_exp/eureka.ipynb#X15sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m traceback_msg \u001b[39m=\u001b[39m filter_traceback(stdout_str)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevihuang/Desktop/projects/autoscious-carbon-capture/eureka_exp/eureka.ipynb#X15sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39mif\u001b[39;00m traceback_msg \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevihuang/Desktop/projects/autoscious-carbon-capture/eureka_exp/eureka.ipynb#X15sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39m# TODO: FIX THIS\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevihuang/Desktop/projects/autoscious-carbon-capture/eureka_exp/eureka.ipynb#X15sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevihuang/Desktop/projects/autoscious-carbon-capture/eureka_exp/eureka.ipynb#X15sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     \u001b[39m# If RL execution has no error, provide policy statistics feedback\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevihuang/Desktop/projects/autoscious-carbon-capture/eureka_exp/eureka.ipynb#X15sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     exec_success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevihuang\\Desktop\\projects\\autoscious-carbon-capture\\eureka_exp\\utils\\misc.py:22\u001b[0m, in \u001b[0;36mfilter_traceback\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter_traceback\u001b[39m(s):\n\u001b[1;32m---> 22\u001b[0m     lines \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m     filtered_lines \u001b[39m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m     \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lines):\n",
      "File \u001b[1;32mc:\\Users\\kevihuang\\Desktop\\projects\\autoscious-carbon-capture\\eureka_exp\\venv2\\Lib\\site-packages\\openai\\openai_object.py:61\u001b[0m, in \u001b[0;36mOpenAIObject.__getattr__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[k]\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m---> 61\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m*\u001b[39merr\u001b[39m.\u001b[39margs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: split"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Eureka generation loop\n",
    "for iter in range(num_iterations):\n",
    "    # Get Eureka response\n",
    "    responses = []\n",
    "    response_cur = None\n",
    "    total_samples = 0\n",
    "    total_token = 0\n",
    "    total_completion_token = 0\n",
    "    # chunk_size = 3 if \"gpt-3.5\" in model else 4\n",
    "    chunk_size = 1 if \"gpt-3.5\" in model else 4\n",
    "\n",
    "    print(f\"Iteration {iter}: Generating {chunk_size} samples with {model}\")\n",
    "\n",
    "    while True:\n",
    "        if total_samples >= chunk_size:\n",
    "            break\n",
    "        print(\"Messages: \", messages)\n",
    "# Messages:  Messages:  [{'role': 'system', 'content': 'You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \\nYour reward function should use useful variables from the environment as inputs. As an example,\\nthe reward function signature can be: @torch.jit.script\\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    ...\\n    return reward, {}\\n\\nSince the reward function will be decorated with @torch.jit.script,\\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \\nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\\n    (1) the total reward,\\n    (2) a dictionary of each individual reward component.\\nThe code output should be formatted as a python code string: \"```python ... ```\".\\n\\nSome helpful tips for writing the reward function code:\\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\\n    (4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\\n'}, {'role': 'user', 'content': 'The Python environment is class Humanoid(VecTask):\\n    \"\"\"Rest of the environment definition omitted.\"\"\"\\n    def compute_observations(self):\\n        self.gym.refresh_dof_state_tensor(self.sim)\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\n\\n        self.gym.refresh_force_sensor_tensor(self.sim)\\n        self.gym.refresh_dof_force_tensor(self.sim)\\n        self.obs_buf[:], self.potentials[:], self.prev_potentials[:], self.up_vec[:], self.heading_vec[:] = compute_humanoid_observations(\\n            self.obs_buf, self.root_states, self.targets, self.potentials,\\n            self.inv_start_rot, self.dof_pos, self.dof_vel, self.dof_force_tensor,\\n            self.dof_limits_lower, self.dof_limits_upper, self.dof_vel_scale,\\n            self.vec_sensor_tensor, self.actions, self.dt, self.contact_force_scale, self.angular_velocity_scale,\\n            self.basis_vec0, self.basis_vec1)\\n\\ndef compute_humanoid_observations(obs_buf, root_states, targets, potentials, inv_start_rot, dof_pos, dof_vel,\\n                                  dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,\\n                                  sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,\\n                                  basis_vec0, basis_vec1):\\n    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\\n\\n    torso_position = root_states[:, 0:3]\\n    torso_rotation = root_states[:, 3:7]\\n    velocity = root_states[:, 7:10]\\n    ang_velocity = root_states[:, 10:13]\\n\\n    to_target = targets - torso_position\\n    to_target[:, 2] = 0\\n\\n    prev_potentials_new = potentials.clone()\\n    potentials = -torch.norm(to_target, p=2, dim=-1) / dt\\n\\n    torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(\\n        torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2)\\n\\n    vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(\\n        torso_quat, velocity, ang_velocity, targets, torso_position)\\n\\n    roll = normalize_angle(roll).unsqueeze(-1)\\n    yaw = normalize_angle(yaw).unsqueeze(-1)\\n    angle_to_target = normalize_angle(angle_to_target).unsqueeze(-1)\\n    dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)\\n\\n    obs = torch.cat((torso_position[:, 2].view(-1, 1), vel_loc, angvel_loc * angular_velocity_scale,\\n                     yaw, roll, angle_to_target, up_proj.unsqueeze(-1), heading_proj.unsqueeze(-1),\\n                     dof_pos_scaled, dof_vel * dof_vel_scale, dof_force * contact_force_scale,\\n                     sensor_force_torques.view(-1, 12) * contact_force_scale, actions), dim=-1)\\n\\n    return obs, potentials, prev_potentials_new, up_vec, heading_vec\\n\\n\\n\\n. Write a reward function for the following task: to make the humanoid run as fast as possible.'}]\n",
    "        for attempt in range(1000):\n",
    "            try:\n",
    "                response_cur = openai.ChatCompletion.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    temperature=1.0,\n",
    "                    n=chunk_size\n",
    "                )\n",
    "                total_samples += chunk_size\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt >= 10:\n",
    "                    chunk_size = max(int(chunk_size / 2), 1)\n",
    "                    print(\"Current Chunk Size\", chunk_size)\n",
    "                print(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "                time.sleep(1)\n",
    "        if response_cur is None:\n",
    "            print(\"Code terminated due to too many failed attempts!\")\n",
    "            exit()\n",
    "\n",
    "        responses.extend(response_cur[\"choices\"])\n",
    "        prompt_tokens = response_cur[\"usage\"][\"prompt_tokens\"]\n",
    "        total_completion_token += response_cur[\"usage\"][\"completion_tokens\"]\n",
    "        total_token += response_cur[\"usage\"][\"total_tokens\"]\n",
    "\n",
    "    if chunk_size == 1:\n",
    "        print(f\"Iteration {iter}: GPT Output:\\n \" + responses[0][\"message\"][\"content\"] + \"\\n\")\n",
    "\n",
    "    # Logging Token Information\n",
    "    print(f\"Iteration {iter}: Prompt Tokens: {prompt_tokens}, Completion Tokens: {total_completion_token}, Total Tokens: {total_token}\")\n",
    "    print(\"RESPONSES: \", responses)\n",
    "\n",
    "    code_runs = [] \n",
    "    rl_runs = []\n",
    "    for response_id in range(chunk_size):\n",
    "        response_cur = responses[response_id][\"message\"][\"content\"]\n",
    "        print(f\"Iteration {iter}: Processing Code Run {response_id}\")\n",
    "\n",
    "        # SIMULATION STEP? I think\n",
    "        # TODO: replace with having a LLM evaluate this step and act as a simulator\n",
    "        \n",
    "        # TODO: update messages to include response_cur which is the proposed model and add a query for an eval step.\n",
    "\n",
    "        print(\"before simulation messages: \", messages)\n",
    "\n",
    "        sim_instructions = '''Assume that you are about to start testing. Theoretically simulate and predict the most likely outcome. Think step by step at the deepest and most informative level. What will happen? You are to predict the result.'''\n",
    "        sim_messages = messages + [\n",
    "            {\"role\": \"assistant\", \"content\": response_cur}, \n",
    "            {\"role\": \"user\", \"content\": sim_instructions}\n",
    "        ]\n",
    "        print(\"simulation messages: \", sim_messages)\n",
    "\n",
    "        response_sim = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=sim_messages,\n",
    "                temperature=0.0,\n",
    "                n=chunk_size\n",
    "            )\n",
    "        code_runs.append(response_cur)\n",
    "        rl_runs.append(response_sim)\n",
    "        print(\"...processed code run...\", response_sim)\n",
    "\n",
    "    # Gather RL training results and construct reward reflection as prompt for next LLM\n",
    "    print(\"---Gather RL training results and construct reward reflection as prompt for next LLM---\")\n",
    "    code_feedbacks = []\n",
    "    contents = []\n",
    "    successes = []\n",
    "    reward_correlations = []\n",
    "    code_paths = []\n",
    "\n",
    "    exec_success = False \n",
    "    for response_id, (code_run, rl_run) in enumerate(zip(code_runs, rl_runs)):\n",
    "        print(\"\\nKEYS, VALUES: \", response_id, (code_run, rl_run))\n",
    "        # rl_run.communicate()\n",
    "        # rl_filepath = f\"env_iter{iter}_response{response_id}.txt\"\n",
    "        code_paths.append(f\"env_iter{iter}_response{response_id}.py\")\n",
    "        try:\n",
    "            # with open(rl_filepath, 'r') as f:\n",
    "            #     stdout_str = f.read() \n",
    "            stdout_str = rl_run\n",
    "        except: \n",
    "            content = execution_error_feedback.format(traceback_msg=\"Code Run cannot be executed due to function signature error! Please re-write an entirely new reward function!\")\n",
    "            content += code_output_tip\n",
    "            contents.append(content) \n",
    "            successes.append(DUMMY_FAILURE)\n",
    "            reward_correlations.append(DUMMY_FAILURE)\n",
    "            continue\n",
    "\n",
    "        content = ''\n",
    "        traceback_msg = filter_traceback(stdout_str)\n",
    "\n",
    "        if traceback_msg == '':\n",
    "            # TODO: FIX THIS\n",
    "\n",
    "            # If RL execution has no error, provide policy statistics feedback\n",
    "            exec_success = True\n",
    "            content += policy_feedback\n",
    "            \n",
    "            # # Compute Correlation between Human-Engineered and GPT Rewards\n",
    "            # if \"gt_reward\" in tensorboard_logs and \"gpt_reward\" in tensorboard_logs:\n",
    "            #     gt_reward = np.array(tensorboard_logs[\"gt_reward\"])\n",
    "            #     gpt_reward = np.array(tensorboard_logs[\"gpt_reward\"])\n",
    "            #     reward_correlation = np.corrcoef(gt_reward, gpt_reward)[0, 1]\n",
    "            #     reward_correlations.append(reward_correlation)\n",
    "\n",
    "            # # Add reward components log to the feedback\n",
    "            # for metric in tensorboard_logs:\n",
    "            #     if \"/\" not in metric:\n",
    "            #         metric_cur = ['{:.2f}'.format(x) for x in tensorboard_logs[metric][::epoch_freq]]\n",
    "            #         metric_cur_max = max(tensorboard_logs[metric])\n",
    "            #         metric_cur_mean = sum(tensorboard_logs[metric]) / len(tensorboard_logs[metric])\n",
    "            #         if \"consecutive_successes\" == metric:\n",
    "            #             successes.append(metric_cur_max)\n",
    "            #         metric_cur_min = min(tensorboard_logs[metric])\n",
    "            #         if metric != \"gt_reward\" and metric != \"gpt_reward\":\n",
    "            #             if metric != \"consecutive_successes\":\n",
    "            #                 metric_name = metric \n",
    "            #             else:\n",
    "            #                 metric_name = \"task_score\"\n",
    "            #             content += f\"{metric_name}: {metric_cur}, Max: {metric_cur_max:.2f}, Mean: {metric_cur_mean:.2f}, Min: {metric_cur_min:.2f} \\n\"                    \n",
    "            #         else:\n",
    "            #             # Provide ground-truth score when success rate not applicable\n",
    "            #             if \"consecutive_successes\" not in tensorboard_logs:\n",
    "            #                 content += f\"ground-truth score: {metric_cur}, Max: {metric_cur_max:.2f}, Mean: {metric_cur_mean:.2f}, Min: {metric_cur_min:.2f} \\n\"  \n",
    "            successes.append(10) # TODO: hard coded for now\n",
    "            reward_correlations.append(10) # TODO: hard coded for now\n",
    "            code_feedback = stdout_str # TODO: hard coded for now               \n",
    "            code_feedbacks.append(code_feedback)\n",
    "            content += code_feedback  \n",
    "        else:\n",
    "            # Otherwise, provide execution traceback error feedback\n",
    "            successes.append(DUMMY_FAILURE)\n",
    "            reward_correlations.append(DUMMY_FAILURE)\n",
    "            content += execution_error_feedback.format(traceback_msg=traceback_msg)\n",
    "\n",
    "        content += code_output_tip\n",
    "        contents.append(content) \n",
    "\n",
    "    print(\"contents: \", contents)\n",
    "\n",
    "    # Repeat the iteration if all code generation failed\n",
    "    if not exec_success and chunk_size != 1:\n",
    "        execute_rates.append(0.)\n",
    "        max_successes.append(DUMMY_FAILURE)\n",
    "        max_successes_reward_correlation.append(DUMMY_FAILURE)\n",
    "        best_code_paths.append(None)\n",
    "        print(\"All code generation failed! Repeat this iteration from the current message checkpoint!\")\n",
    "        continue\n",
    "\n",
    "    print(\"\\nFINISHED ROUND\")\n",
    "    print(\"successes\", successes)\n",
    "    print(\"contents\", contents)\n",
    "\n",
    "\n",
    "    # Select the best code sample based on the success rate\n",
    "    best_sample_idx = np.argmax(np.array(successes))\n",
    "    best_content = contents[best_sample_idx]\n",
    "\n",
    "    max_success = successes[best_sample_idx]\n",
    "    max_success_reward_correlation = reward_correlations[best_sample_idx]\n",
    "    execute_rate = np.sum(np.array(successes) >= 0.) / chunk_size\n",
    "\n",
    "    # Update the best Eureka Output\n",
    "    if max_success > max_success_overall:\n",
    "        max_success_overall = max_success\n",
    "        max_success_reward_correlation_overall = max_success_reward_correlation\n",
    "        max_reward_code_path = code_paths[best_sample_idx]\n",
    "\n",
    "    execute_rates.append(execute_rate)\n",
    "    max_successes.append(max_success)\n",
    "    max_successes_reward_correlation.append(max_success_reward_correlation)\n",
    "    best_code_paths.append(code_paths[best_sample_idx])\n",
    "\n",
    "    print(f\"Iteration {iter}: Max Success: {max_success}, Execute Rate: {execute_rate}, Max Success Reward Correlation: {max_success_reward_correlation}\")\n",
    "    print(f\"Iteration {iter}: Best Generation ID: {best_sample_idx}\")\n",
    "    print(f\"Iteration {iter}: GPT Output Content:\\n\" +  responses[best_sample_idx][\"message\"][\"content\"] + \"\\n\")\n",
    "    print(f\"Iteration {iter}: User Content:\\n\" + best_content + \"\\n\")\n",
    "        \n",
    "    # Plot the success rate\n",
    "    fig, axs = plt.subplots(2, figsize=(6, 6))\n",
    "    fig.suptitle(task)\n",
    "\n",
    "    x_axis = np.arange(len(max_successes))\n",
    "\n",
    "    axs[0].plot(x_axis, np.array(max_successes))\n",
    "    axs[0].set_title(\"Max Success\")\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "\n",
    "    axs[1].plot(x_axis, np.array(execute_rates))\n",
    "    axs[1].set_title(\"Execute Rate\")\n",
    "    axs[1].set_xlabel(\"Iteration\")\n",
    "\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    plt.savefig('summary.png')\n",
    "    np.savez('summary.npz', max_successes=max_successes, execute_rates=execute_rates, best_code_paths=best_code_paths, max_successes_reward_correlation=max_successes_reward_correlation)\n",
    "\n",
    "    if len(messages) == 2:\n",
    "        messages += [{\"role\": \"assistant\", \"content\": responses[best_sample_idx][\"message\"][\"content\"]}]\n",
    "        messages += [{\"role\": \"user\", \"content\": best_content}]\n",
    "    else:\n",
    "        assert len(messages) == 4\n",
    "        messages[-2] = {\"role\": \"assistant\", \"content\": responses[best_sample_idx][\"message\"][\"content\"]}\n",
    "        messages[-1] = {\"role\": \"user\", \"content\": best_content}\n",
    "\n",
    "    # Save dictionary as JSON file\n",
    "    with open('messages.json', 'w') as file:\n",
    "        json.dump(messages, file, indent=4)\n",
    "\n",
    "    print(\"\\n***SAVED AND FINISHED ONE ROUND ***\\n\", \"messages: \", messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the best reward code many times\n",
    "# if max_reward_code_path is None: \n",
    "#     print(\"All iterations of code generation failed, aborting...\")\n",
    "#     print(\"Please double check the output env_iter*_response*.txt files for repeating errors!\")\n",
    "#     exit()\n",
    "# print(f\"Task: {task}, Max Training Success {max_success_overall}, Correlation {max_success_reward_correlation_overall}, Best Reward Code Path: {max_reward_code_path}\")\n",
    "# print(f\"Evaluating best reward code {cfg.num_eval} times\")\n",
    "# shutil.copy(max_reward_code_path, output_file)\n",
    "\n",
    "# eval_runs = []\n",
    "# for i in range(cfg.num_eval):\n",
    "#     set_freest_gpu()\n",
    "    \n",
    "#     # Execute the python file with flags\n",
    "#     rl_filepath = f\"reward_code_eval{i}.txt\"\n",
    "#     with open(rl_filepath, 'w') as f:\n",
    "#         process = subprocess.Popen(['python', '-u', f'{ISAAC_ROOT_DIR}/train.py',  \n",
    "#                                     'hydra/output=subprocess',\n",
    "#                                     f'task={task}{suffix}', f'wandb_activate={cfg.use_wandb}',\n",
    "#                                     f'wandb_entity={cfg.wandb_username}', f'wandb_project={cfg.wandb_project}',\n",
    "#                                     f'headless={not cfg.capture_video}', f'capture_video={cfg.capture_video}', 'force_render=False', f'seed={i}',\n",
    "#                                     ],\n",
    "#                                     stdout=f, stderr=f)\n",
    "\n",
    "#     block_until_training(rl_filepath)\n",
    "#     eval_runs.append(process)\n",
    "\n",
    "# reward_code_final_successes = []\n",
    "# reward_code_correlations_final = []\n",
    "# for i, rl_run in enumerate(eval_runs):\n",
    "#     rl_run.communicate()\n",
    "#     rl_filepath = f\"reward_code_eval{i}.txt\"\n",
    "#     with open(rl_filepath, 'r') as f:\n",
    "#         stdout_str = f.read() \n",
    "#     lines = stdout_str.split('\\n')\n",
    "#     for i, line in enumerate(lines):\n",
    "#         if line.startswith('Tensorboard Directory:'):\n",
    "#             break \n",
    "#     tensorboard_logdir = line.split(':')[-1].strip() \n",
    "#     tensorboard_logs = load_tensorboard_logs(tensorboard_logdir)\n",
    "#     max_success = max(tensorboard_logs['consecutive_successes'])\n",
    "#     reward_code_final_successes.append(max_success)\n",
    "\n",
    "#     if \"gt_reward\" in tensorboard_logs and \"gpt_reward\" in tensorboard_logs:\n",
    "#         gt_reward = np.array(tensorboard_logs[\"gt_reward\"])\n",
    "#         gpt_reward = np.array(tensorboard_logs[\"gpt_reward\"])\n",
    "#         reward_correlation = np.corrcoef(gt_reward, gpt_reward)[0, 1]\n",
    "#         reward_code_correlations_final.append(reward_correlation)\n",
    "\n",
    "# print(f\"Final Success Mean: {np.mean(reward_code_final_successes)}, Std: {np.std(reward_code_final_successes)}, Raw: {reward_code_final_successes}\")\n",
    "# print(f\"Final Correlation Mean: {np.mean(reward_code_correlations_final)}, Std: {np.std(reward_code_correlations_final)}, Raw: {reward_code_correlations_final}\")\n",
    "# np.savez('final_eval.npz', reward_code_final_successes=reward_code_final_successes, reward_code_correlations_final=reward_code_correlations_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      messages:  [{'role': 'system', 'content': 'You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "'}, {'role': 'user', 'content': 'The Python environment is class Humanoid(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self):\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "        self.gym.refresh_actor_root_state_tensor(self.sim)\n",
      "\n",
      "        self.gym.refresh_force_sensor_tensor(self.sim)\n",
      "        self.gym.refresh_dof_force_tensor(self.sim)\n",
      "        self.obs_buf[:], self.potentials[:], self.prev_potentials[:], self.up_vec[:], self.heading_vec[:] = compute_humanoid_observations(\n",
      "            self.obs_buf, self.root_states, self.targets, self.potentials,\n",
      "            self.inv_start_rot, self.dof_pos, self.dof_vel, self.dof_force_tensor,\n",
      "            self.dof_limits_lower, self.dof_limits_upper, self.dof_vel_scale,\n",
      "            self.vec_sensor_tensor, self.actions, self.dt, self.contact_force_scale, self.angular_velocity_scale,\n",
      "            self.basis_vec0, self.basis_vec1)\n",
      "\n",
      "def compute_humanoid_observations(obs_buf, root_states, targets, potentials, inv_start_rot, dof_pos, dof_vel,\n",
      "                                  dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,\n",
      "                                  sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,\n",
      "                                  basis_vec0, basis_vec1):\n",
      "    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\n",
      "\n",
      "    torso_position = root_states[:, 0:3]\n",
      "    torso_rotation = root_states[:, 3:7]\n",
      "    velocity = root_states[:, 7:10]\n",
      "    ang_velocity = root_states[:, 10:13]\n",
      "\n",
      "    to_target = targets - torso_position\n",
      "    to_target[:, 2] = 0\n",
      "\n",
      "    prev_potentials_new = potentials.clone()\n",
      "    potentials = -torch.norm(to_target, p=2, dim=-1) / dt\n",
      "\n",
      "    torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(\n",
      "        torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2)\n",
      "\n",
      "    vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(\n",
      "        torso_quat, velocity, ang_velocity, targets, torso_position)\n",
      "\n",
      "    roll = normalize_angle(roll).unsqueeze(-1)\n",
      "    yaw = normalize_angle(yaw).unsqueeze(-1)\n",
      "    angle_to_target = normalize_angle(angle_to_target).unsqueeze(-1)\n",
      "    dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)\n",
      "\n",
      "    obs = torch.cat((torso_position[:, 2].view(-1, 1), vel_loc, angvel_loc * angular_velocity_scale,\n",
      "                     yaw, roll, angle_to_target, up_proj.unsqueeze(-1), heading_proj.unsqueeze(-1),\n",
      "                     dof_pos_scaled, dof_vel * dof_vel_scale, dof_force * contact_force_scale,\n",
      "                     sensor_force_torques.view(-1, 12) * contact_force_scale, actions), dim=-1)\n",
      "\n",
      "    return obs, potentials, prev_potentials_new, up_vec, heading_vec\n",
      "\n",
      "\n",
      "\n",
      ". Write a reward function for the following task: to make the humanoid run as fast as possible.'}, {'role': 'assistant', 'content': \"Apologies for the confusion. Here's the updated reward function code that follows the specified format:\n",
      "\n",
      "```python\n",
      "@torch.jit.script\n",
      "def compute_reward(velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    temperature = 1.0\n",
      "\n",
      "    velocity_reward = torch.exp(temperature * velocity.norm(dim=-1))  # Apply temperature and exponential transformation\n",
      "\n",
      "    reward_components = {\n",
      "        'velocity_reward': velocity_reward\n",
      "    }\n",
      "\n",
      "    total_reward = torch.sum(velocity_reward)  # Compute the total reward as the sum of individual rewards\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Please note the following updates:\n",
      "- A temperature parameter `temperature` has been introduced to control the transformation of the reward component. You can adjust its value to change the emphasis on higher velocities.\n",
      "- The individual reward component `velocity_reward` is calculated using the temperature and exponential transformation on the norm of the velocity.\n",
      "- The total reward is computed using `torch.sum`.\n",
      "- The output is now a tuple consisting of the total reward and the dictionary of individual reward components.\n",
      "\n",
      "You can use this updated reward function to train the humanoid to run as fast as possible.\n",
      "\"}, {'role': 'user', 'content': 'Great results!The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "'}]\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "      messages:  [{'role': 'system', 'content': 'You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \\nYour reward function should use useful variables from the environment as inputs. As an example,\\nthe reward function signature can be: @torch.jit.script\\ndef compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    ...\\n    return reward, {}\\n\\nSince the reward function will be decorated with @torch.jit.script,\\nplease make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \\nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\\n    (1) the total reward,\\n    (2) a dictionary of each individual reward component.\\nThe code output should be formatted as a python code string: \"```python ... ```\".\\n\\nSome helpful tips for writing the reward function code:\\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\\n    (4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\\n'}, {'role': 'user', 'content': 'The Python environment is class Humanoid(VecTask):\\n    \"\"\"Rest of the environment definition omitted.\"\"\"\\n    def compute_observations(self):\\n        self.gym.refresh_dof_state_tensor(self.sim)\\n        self.gym.refresh_actor_root_state_tensor(self.sim)\\n\\n        self.gym.refresh_force_sensor_tensor(self.sim)\\n        self.gym.refresh_dof_force_tensor(self.sim)\\n        self.obs_buf[:], self.potentials[:], self.prev_potentials[:], self.up_vec[:], self.heading_vec[:] = compute_humanoid_observations(\\n            self.obs_buf, self.root_states, self.targets, self.potentials,\\n            self.inv_start_rot, self.dof_pos, self.dof_vel, self.dof_force_tensor,\\n            self.dof_limits_lower, self.dof_limits_upper, self.dof_vel_scale,\\n            self.vec_sensor_tensor, self.actions, self.dt, self.contact_force_scale, self.angular_velocity_scale,\\n            self.basis_vec0, self.basis_vec1)\\n\\ndef compute_humanoid_observations(obs_buf, root_states, targets, potentials, inv_start_rot, dof_pos, dof_vel,\\n                                  dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,\\n                                  sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,\\n                                  basis_vec0, basis_vec1):\\n    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]\\n\\n    torso_position = root_states[:, 0:3]\\n    torso_rotation = root_states[:, 3:7]\\n    velocity = root_states[:, 7:10]\\n    ang_velocity = root_states[:, 10:13]\\n\\n    to_target = targets - torso_position\\n    to_target[:, 2] = 0\\n\\n    prev_potentials_new = potentials.clone()\\n    potentials = -torch.norm(to_target, p=2, dim=-1) / dt\\n\\n    torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(\\n        torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2)\\n\\n    vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(\\n        torso_quat, velocity, ang_velocity, targets, torso_position)\\n\\n    roll = normalize_angle(roll).unsqueeze(-1)\\n    yaw = normalize_angle(yaw).unsqueeze(-1)\\n    angle_to_target = normalize_angle(angle_to_target).unsqueeze(-1)\\n    dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)\\n\\n    obs = torch.cat((torso_position[:, 2].view(-1, 1), vel_loc, angvel_loc * angular_velocity_scale,\\n                     yaw, roll, angle_to_target, up_proj.unsqueeze(-1), heading_proj.unsqueeze(-1),\\n                     dof_pos_scaled, dof_vel * dof_vel_scale, dof_force * contact_force_scale,\\n                     sensor_force_torques.view(-1, 12) * contact_force_scale, actions), dim=-1)\\n\\n    return obs, potentials, prev_potentials_new, up_vec, heading_vec\\n\\n\\n\\n. Write a reward function for the following task: to make the humanoid run as fast as possible.'}, {'role': 'assistant', 'content': \"Apologies for the confusion. Here's the updated reward function code that follows the specified format:\\n\\n```python\\n@torch.jit.script\\ndef compute_reward(velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\n    temperature = 1.0\\n\\n    velocity_reward = torch.exp(temperature * velocity.norm(dim=-1))  # Apply temperature and exponential transformation\\n\\n    reward_components = {\\n        'velocity_reward': velocity_reward\\n    }\\n\\n    total_reward = torch.sum(velocity_reward)  # Compute the total reward as the sum of individual rewards\\n\\n    return total_reward, reward_components\\n```\\n\\nPlease note the following updates:\\n- A temperature parameter `temperature` has been introduced to control the transformation of the reward component. You can adjust its value to change the emphasis on higher velocities.\\n- The individual reward component `velocity_reward` is calculated using the temperature and exponential transformation on the norm of the velocity.\\n- The total reward is computed using `torch.sum`.\\n- The output is now a tuple consisting of the total reward and the dictionary of individual reward components.\\n\\nYou can use this updated reward function to train the humanoid to run as fast as possible.\\n\"}, {'role': 'user', 'content': 'Great results!The output of the reward function should consist of two items:\\n    (1) the total reward,\\n    (2) a dictionary of each individual reward component.\\nThe code output should be formatted as a python code string: \"```python ... ```\".\\n\\nSome helpful tips for writing the reward function code:\\n    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\\n    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\\n    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\\n    (4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\\n'}]\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
